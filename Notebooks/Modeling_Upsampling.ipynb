{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 300)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.utils import resample\n",
    "# Import metrics used\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the California Wildfires dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/california_wildfires.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dummy Variables for Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two features that are categorical. The counties and the month of the year column that we engineered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the county column\n",
    "counties = pd.get_dummies(df.county, drop_first = True)\n",
    "# Drop county column along with unnecessary columns (Unnamed columns, year, and acres burned)\n",
    "df2 = df.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1', 'county', 'year', 'acres_burned'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineer month column from the date column\n",
    "df2['month'] = pd.DatetimeIndex(df2['date']).month\n",
    "# Drop the date column\n",
    "df2.drop(columns = ['date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the months\n",
    "month = pd.get_dummies(df2.month, drop_first = True)\n",
    "# Drop the month column\n",
    "df2.drop(columns = 'month', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the original dataframe with the dummy variables\n",
    "df2 = pd.concat([df2, counties, month], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fire_started</th>\n",
       "      <th>Alfalfa &amp; Hay_acres</th>\n",
       "      <th>Alfalfa &amp; Hay_percentage</th>\n",
       "      <th>Almonds_acres</th>\n",
       "      <th>Almonds_percentage</th>\n",
       "      <th>Barren_acres</th>\n",
       "      <th>Barren_percentage</th>\n",
       "      <th>Corn_acres</th>\n",
       "      <th>Corn_percentage</th>\n",
       "      <th>Cotton_acres</th>\n",
       "      <th>Cotton_percentage</th>\n",
       "      <th>Deciduous Forest_acres</th>\n",
       "      <th>Deciduous Forest_percentage</th>\n",
       "      <th>Evergreen Forest_acres</th>\n",
       "      <th>Evergreen Forest_percentage</th>\n",
       "      <th>Fallow_acres</th>\n",
       "      <th>Fallow_percentage</th>\n",
       "      <th>Fruit Trees_acres</th>\n",
       "      <th>Fruit Trees_percentage</th>\n",
       "      <th>Grain Crops_acres</th>\n",
       "      <th>Grain Crops_percentage</th>\n",
       "      <th>Grapes_acres</th>\n",
       "      <th>Grapes_percentage</th>\n",
       "      <th>Grassland_acres</th>\n",
       "      <th>Grassland_percentage</th>\n",
       "      <th>High Intensity Developed_acres</th>\n",
       "      <th>High Intensity Developed_percentage</th>\n",
       "      <th>Low Intensity Developed_acres</th>\n",
       "      <th>Low Intensity Developed_percentage</th>\n",
       "      <th>Mixed Forest_acres</th>\n",
       "      <th>Mixed Forest_percentage</th>\n",
       "      <th>Other Ocean/Mexico_acres</th>\n",
       "      <th>Other Ocean/Mexico_percentage</th>\n",
       "      <th>Other Tree Crops_acres</th>\n",
       "      <th>Other Tree Crops_percentage</th>\n",
       "      <th>Other_acres</th>\n",
       "      <th>Other_percentage</th>\n",
       "      <th>Rice_acres</th>\n",
       "      <th>Rice_percentage</th>\n",
       "      <th>Shrubland_acres</th>\n",
       "      <th>Shrubland_percentage</th>\n",
       "      <th>Tomatoes_acres</th>\n",
       "      <th>Tomatoes_percentage</th>\n",
       "      <th>Vegs &amp; Fruits_acres</th>\n",
       "      <th>Vegs &amp; Fruits_percentage</th>\n",
       "      <th>Walnuts_acres</th>\n",
       "      <th>Walnuts_percentage</th>\n",
       "      <th>Water_acres</th>\n",
       "      <th>Water_percentage</th>\n",
       "      <th>Wetlands_acres</th>\n",
       "      <th>Wetlands_percentage</th>\n",
       "      <th>Winter Wheat_acres</th>\n",
       "      <th>Winter Wheat_percentage</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>min_elevation</th>\n",
       "      <th>Avg Air Temp (F)_Weekly</th>\n",
       "      <th>Avg Rel Hum (%)_Weekly</th>\n",
       "      <th>Avg Wind Speed (mph)_Weekly</th>\n",
       "      <th>Dew Point (F)_Weekly</th>\n",
       "      <th>Max Air Temp (F)_Weekly</th>\n",
       "      <th>Max Rel Hum (%)_Weekly</th>\n",
       "      <th>Min Air Temp (F)_Weekly</th>\n",
       "      <th>Min Rel Hum (%)_Weekly</th>\n",
       "      <th>Precip (in)_Weekly</th>\n",
       "      <th>Avg Air Temp (F)_month</th>\n",
       "      <th>Avg Rel Hum (%)_month</th>\n",
       "      <th>Avg Wind Speed (mph)_month</th>\n",
       "      <th>Dew Point (F)_month</th>\n",
       "      <th>Max Air Temp (F)_month</th>\n",
       "      <th>Max Rel Hum (%)_month</th>\n",
       "      <th>Min Air Temp (F)_month</th>\n",
       "      <th>Min Rel Hum (%)_month</th>\n",
       "      <th>Precip (in)_month</th>\n",
       "      <th>Alpine</th>\n",
       "      <th>Amador</th>\n",
       "      <th>Butte</th>\n",
       "      <th>Calaveras</th>\n",
       "      <th>Colusa</th>\n",
       "      <th>Contra Costa</th>\n",
       "      <th>Del Norte</th>\n",
       "      <th>El Dorado</th>\n",
       "      <th>Fresno</th>\n",
       "      <th>Glenn</th>\n",
       "      <th>Humboldt</th>\n",
       "      <th>Imperial</th>\n",
       "      <th>Inyo</th>\n",
       "      <th>Kern</th>\n",
       "      <th>Kings</th>\n",
       "      <th>Lake</th>\n",
       "      <th>Lassen</th>\n",
       "      <th>Los Angeles</th>\n",
       "      <th>Madera</th>\n",
       "      <th>Marin</th>\n",
       "      <th>Mariposa</th>\n",
       "      <th>Mendocino</th>\n",
       "      <th>Merced</th>\n",
       "      <th>Modoc</th>\n",
       "      <th>Mono</th>\n",
       "      <th>Monterey</th>\n",
       "      <th>Napa</th>\n",
       "      <th>Nevada</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Placer</th>\n",
       "      <th>Plumas</th>\n",
       "      <th>Riverside</th>\n",
       "      <th>Sacramento</th>\n",
       "      <th>San Benito</th>\n",
       "      <th>San Bernardino</th>\n",
       "      <th>San Diego</th>\n",
       "      <th>San Francisco</th>\n",
       "      <th>San Joaquin</th>\n",
       "      <th>San Luis Obispo</th>\n",
       "      <th>San Mateo</th>\n",
       "      <th>Santa Barbara</th>\n",
       "      <th>Santa Clara</th>\n",
       "      <th>Santa Cruz</th>\n",
       "      <th>Shasta</th>\n",
       "      <th>Sierra</th>\n",
       "      <th>Siskiyou</th>\n",
       "      <th>Solano</th>\n",
       "      <th>Sonoma</th>\n",
       "      <th>Stanislaus</th>\n",
       "      <th>Sutter</th>\n",
       "      <th>Tehama</th>\n",
       "      <th>Trinity</th>\n",
       "      <th>Tulare</th>\n",
       "      <th>Tuolumne</th>\n",
       "      <th>Ventura</th>\n",
       "      <th>Yolo</th>\n",
       "      <th>Yuba</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1102.856805</td>\n",
       "      <td>0.300074</td>\n",
       "      <td>4.225505</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>194.595625</td>\n",
       "      <td>0.052947</td>\n",
       "      <td>4.670295</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.337480</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>7838.756565</td>\n",
       "      <td>2.132827</td>\n",
       "      <td>1536.749450</td>\n",
       "      <td>0.418130</td>\n",
       "      <td>1.779160</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>991.214515</td>\n",
       "      <td>0.269697</td>\n",
       "      <td>3722.447510</td>\n",
       "      <td>1.012831</td>\n",
       "      <td>153671.386680</td>\n",
       "      <td>41.812059</td>\n",
       "      <td>28431.421590</td>\n",
       "      <td>7.735834</td>\n",
       "      <td>39470.886995</td>\n",
       "      <td>10.739534</td>\n",
       "      <td>74885.956375</td>\n",
       "      <td>20.375531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.673405</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>30958.051185</td>\n",
       "      <td>8.423298</td>\n",
       "      <td>4.670295</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>164.127510</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>4.670295</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>19403.518960</td>\n",
       "      <td>5.279454</td>\n",
       "      <td>4497.494085</td>\n",
       "      <td>1.223712</td>\n",
       "      <td>624.485160</td>\n",
       "      <td>0.169915</td>\n",
       "      <td>1242</td>\n",
       "      <td>-42</td>\n",
       "      <td>44.214286</td>\n",
       "      <td>82.785714</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>39.321429</td>\n",
       "      <td>54.157143</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>35.771429</td>\n",
       "      <td>60.785714</td>\n",
       "      <td>0.095714</td>\n",
       "      <td>45.506897</td>\n",
       "      <td>78.189655</td>\n",
       "      <td>2.915517</td>\n",
       "      <td>38.932759</td>\n",
       "      <td>55.896552</td>\n",
       "      <td>95.448276</td>\n",
       "      <td>35.725862</td>\n",
       "      <td>55.810345</td>\n",
       "      <td>0.130172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>189.035750</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15482.472715</td>\n",
       "      <td>3.282650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>194.595625</td>\n",
       "      <td>0.041259</td>\n",
       "      <td>195088.007530</td>\n",
       "      <td>41.363269</td>\n",
       "      <td>0.444790</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.222395</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5644.829890</td>\n",
       "      <td>1.196837</td>\n",
       "      <td>121.427670</td>\n",
       "      <td>0.025746</td>\n",
       "      <td>3192.480225</td>\n",
       "      <td>0.676881</td>\n",
       "      <td>0.667185</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>247783.390805</td>\n",
       "      <td>52.535935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2650.503610</td>\n",
       "      <td>0.561969</td>\n",
       "      <td>1297.452430</td>\n",
       "      <td>0.275091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3556</td>\n",
       "      <td>1442</td>\n",
       "      <td>29.657143</td>\n",
       "      <td>76.514286</td>\n",
       "      <td>3.228571</td>\n",
       "      <td>21.328571</td>\n",
       "      <td>34.428571</td>\n",
       "      <td>91.857143</td>\n",
       "      <td>22.857143</td>\n",
       "      <td>55.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.789655</td>\n",
       "      <td>68.162069</td>\n",
       "      <td>4.968966</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>39.344828</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>22.758621</td>\n",
       "      <td>46.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1326.808570</td>\n",
       "      <td>0.414290</td>\n",
       "      <td>16.679625</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>1873.010690</td>\n",
       "      <td>0.584840</td>\n",
       "      <td>242.632945</td>\n",
       "      <td>0.075761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17190.911105</td>\n",
       "      <td>5.367789</td>\n",
       "      <td>114386.866695</td>\n",
       "      <td>35.716810</td>\n",
       "      <td>168.130620</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>12.009330</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>120.093300</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>2587.343430</td>\n",
       "      <td>0.807887</td>\n",
       "      <td>112912.610240</td>\n",
       "      <td>35.256480</td>\n",
       "      <td>440.119705</td>\n",
       "      <td>0.137425</td>\n",
       "      <td>8263.975805</td>\n",
       "      <td>2.580391</td>\n",
       "      <td>1727.119570</td>\n",
       "      <td>0.539286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.334370</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.111975</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>52457.865415</td>\n",
       "      <td>16.379744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.779160</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>122.094855</td>\n",
       "      <td>0.038124</td>\n",
       "      <td>5822.745890</td>\n",
       "      <td>1.818128</td>\n",
       "      <td>105.860020</td>\n",
       "      <td>0.033054</td>\n",
       "      <td>479.483620</td>\n",
       "      <td>0.149717</td>\n",
       "      <td>3121</td>\n",
       "      <td>43</td>\n",
       "      <td>34.114286</td>\n",
       "      <td>83.571429</td>\n",
       "      <td>3.157143</td>\n",
       "      <td>29.585714</td>\n",
       "      <td>40.071429</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>27.757143</td>\n",
       "      <td>66.571429</td>\n",
       "      <td>0.141429</td>\n",
       "      <td>34.289655</td>\n",
       "      <td>76.724138</td>\n",
       "      <td>3.606897</td>\n",
       "      <td>27.410345</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>93.172414</td>\n",
       "      <td>27.768966</td>\n",
       "      <td>58.310345</td>\n",
       "      <td>0.155517</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3777.156680</td>\n",
       "      <td>0.374865</td>\n",
       "      <td>46196.556585</td>\n",
       "      <td>4.584787</td>\n",
       "      <td>1869.452370</td>\n",
       "      <td>0.185534</td>\n",
       "      <td>2023.349710</td>\n",
       "      <td>0.200808</td>\n",
       "      <td>9.118195</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>33181.556395</td>\n",
       "      <td>3.293111</td>\n",
       "      <td>408193.790775</td>\n",
       "      <td>40.511281</td>\n",
       "      <td>56434.510410</td>\n",
       "      <td>5.600855</td>\n",
       "      <td>10563.317710</td>\n",
       "      <td>1.048359</td>\n",
       "      <td>2628.041715</td>\n",
       "      <td>0.260821</td>\n",
       "      <td>247.525635</td>\n",
       "      <td>0.024566</td>\n",
       "      <td>170758.216925</td>\n",
       "      <td>16.946936</td>\n",
       "      <td>4421.657390</td>\n",
       "      <td>0.438828</td>\n",
       "      <td>25520.048645</td>\n",
       "      <td>2.532743</td>\n",
       "      <td>165.684275</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>790.391830</td>\n",
       "      <td>0.078443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105624.281300</td>\n",
       "      <td>10.482705</td>\n",
       "      <td>55372.129495</td>\n",
       "      <td>5.495419</td>\n",
       "      <td>94.295480</td>\n",
       "      <td>0.009358</td>\n",
       "      <td>469.475845</td>\n",
       "      <td>0.046593</td>\n",
       "      <td>42057.340845</td>\n",
       "      <td>4.173990</td>\n",
       "      <td>21360.372565</td>\n",
       "      <td>2.119915</td>\n",
       "      <td>11589.893030</td>\n",
       "      <td>1.150241</td>\n",
       "      <td>4257.085090</td>\n",
       "      <td>0.422495</td>\n",
       "      <td>2192</td>\n",
       "      <td>-1</td>\n",
       "      <td>40.985714</td>\n",
       "      <td>81.285714</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>35.557143</td>\n",
       "      <td>50.114286</td>\n",
       "      <td>91.285714</td>\n",
       "      <td>32.171429</td>\n",
       "      <td>62.857143</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>42.389655</td>\n",
       "      <td>77.448276</td>\n",
       "      <td>3.848276</td>\n",
       "      <td>35.586207</td>\n",
       "      <td>52.455172</td>\n",
       "      <td>88.965517</td>\n",
       "      <td>33.365517</td>\n",
       "      <td>58.862069</td>\n",
       "      <td>0.175517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.802485</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>28.466560</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>218.391890</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34882.878145</td>\n",
       "      <td>5.495994</td>\n",
       "      <td>255438.004310</td>\n",
       "      <td>40.245698</td>\n",
       "      <td>28.688955</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>12.231725</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>2.223950</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>522.183460</td>\n",
       "      <td>0.082273</td>\n",
       "      <td>207502.763615</td>\n",
       "      <td>32.693230</td>\n",
       "      <td>465.027945</td>\n",
       "      <td>0.073268</td>\n",
       "      <td>12257.745215</td>\n",
       "      <td>1.931277</td>\n",
       "      <td>3351.270255</td>\n",
       "      <td>0.528012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106091.088405</td>\n",
       "      <td>16.715249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667185</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>425.664030</td>\n",
       "      <td>0.067066</td>\n",
       "      <td>13178.682910</td>\n",
       "      <td>2.076376</td>\n",
       "      <td>245.079290</td>\n",
       "      <td>0.038614</td>\n",
       "      <td>11.786935</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>3522</td>\n",
       "      <td>787</td>\n",
       "      <td>41.928571</td>\n",
       "      <td>93.014286</td>\n",
       "      <td>5.657143</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>50.142857</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>35.571429</td>\n",
       "      <td>74.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.931034</td>\n",
       "      <td>87.017241</td>\n",
       "      <td>6.268966</td>\n",
       "      <td>37.196552</td>\n",
       "      <td>52.827586</td>\n",
       "      <td>97.551724</td>\n",
       "      <td>34.344828</td>\n",
       "      <td>61.275862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fire_started  Alfalfa & Hay_acres  Alfalfa & Hay_percentage  Almonds_acres  \\\n",
       "0           0.0          1102.856805                  0.300074       4.225505   \n",
       "1           0.0           189.035750                  0.040080       0.000000   \n",
       "2           0.0          1326.808570                  0.414290      16.679625   \n",
       "3           0.0          3777.156680                  0.374865   46196.556585   \n",
       "4           0.0            31.802485                  0.005011      28.466560   \n",
       "\n",
       "   Almonds_percentage  Barren_acres  Barren_percentage   Corn_acres  \\\n",
       "0            0.001150    194.595625           0.052947     4.670295   \n",
       "1            0.000000  15482.472715           3.282650     0.000000   \n",
       "2            0.005208   1873.010690           0.584840   242.632945   \n",
       "3            4.584787   1869.452370           0.185534  2023.349710   \n",
       "4            0.004485    218.391890           0.034409     0.889580   \n",
       "\n",
       "   Corn_percentage  Cotton_acres  Cotton_percentage  Deciduous Forest_acres  \\\n",
       "0         0.001271      0.000000           0.000000                5.337480   \n",
       "1         0.000000      0.000000           0.000000              194.595625   \n",
       "2         0.075761      0.000000           0.000000            17190.911105   \n",
       "3         0.200808      9.118195           0.000905            33181.556395   \n",
       "4         0.000140      0.000000           0.000000            34882.878145   \n",
       "\n",
       "   Deciduous Forest_percentage  Evergreen Forest_acres  \\\n",
       "0                     0.001452             7838.756565   \n",
       "1                     0.041259           195088.007530   \n",
       "2                     5.367789           114386.866695   \n",
       "3                     3.293111           408193.790775   \n",
       "4                     5.495994           255438.004310   \n",
       "\n",
       "   Evergreen Forest_percentage  Fallow_acres  Fallow_percentage  \\\n",
       "0                     2.132827   1536.749450           0.418130   \n",
       "1                    41.363269      0.444790           0.000094   \n",
       "2                    35.716810    168.130620           0.052498   \n",
       "3                    40.511281  56434.510410           5.600855   \n",
       "4                    40.245698     28.688955           0.004520   \n",
       "\n",
       "   Fruit Trees_acres  Fruit Trees_percentage  Grain Crops_acres  \\\n",
       "0           1.779160                0.000484         991.214515   \n",
       "1           0.222395                0.000047           0.000000   \n",
       "2          12.009330                0.003750         120.093300   \n",
       "3       10563.317710                1.048359        2628.041715   \n",
       "4          12.231725                0.001927           2.223950   \n",
       "\n",
       "   Grain Crops_percentage  Grapes_acres  Grapes_percentage  Grassland_acres  \\\n",
       "0                0.269697   3722.447510           1.012831    153671.386680   \n",
       "1                0.000000      0.000000           0.000000      5644.829890   \n",
       "2                0.037499   2587.343430           0.807887    112912.610240   \n",
       "3                0.260821    247.525635           0.024566    170758.216925   \n",
       "4                0.000350    522.183460           0.082273    207502.763615   \n",
       "\n",
       "   Grassland_percentage  High Intensity Developed_acres  \\\n",
       "0             41.812059                    28431.421590   \n",
       "1              1.196837                      121.427670   \n",
       "2             35.256480                      440.119705   \n",
       "3             16.946936                     4421.657390   \n",
       "4             32.693230                      465.027945   \n",
       "\n",
       "   High Intensity Developed_percentage  Low Intensity Developed_acres  \\\n",
       "0                             7.735834                   39470.886995   \n",
       "1                             0.025746                    3192.480225   \n",
       "2                             0.137425                    8263.975805   \n",
       "3                             0.438828                   25520.048645   \n",
       "4                             0.073268                   12257.745215   \n",
       "\n",
       "   Low Intensity Developed_percentage  Mixed Forest_acres  \\\n",
       "0                           10.739534        74885.956375   \n",
       "1                            0.676881            0.667185   \n",
       "2                            2.580391         1727.119570   \n",
       "3                            2.532743          165.684275   \n",
       "4                            1.931277         3351.270255   \n",
       "\n",
       "   Mixed Forest_percentage  Other Ocean/Mexico_acres  \\\n",
       "0                20.375531                       0.0   \n",
       "1                 0.000141                       0.0   \n",
       "2                 0.539286                       0.0   \n",
       "3                 0.016443                       0.0   \n",
       "4                 0.528012                       0.0   \n",
       "\n",
       "   Other Ocean/Mexico_percentage  Other Tree Crops_acres  \\\n",
       "0                            0.0                8.673405   \n",
       "1                            0.0                0.000000   \n",
       "2                            0.0                1.334370   \n",
       "3                            0.0              790.391830   \n",
       "4                            0.0                0.889580   \n",
       "\n",
       "   Other Tree Crops_percentage  Other_acres  Other_percentage     Rice_acres  \\\n",
       "0                     0.002360          0.0               0.0       0.889580   \n",
       "1                     0.000000          0.0               0.0       0.000000   \n",
       "2                     0.000417          0.0               0.0       1.111975   \n",
       "3                     0.078443          0.0               0.0  105624.281300   \n",
       "4                     0.000140          0.0               0.0       0.000000   \n",
       "\n",
       "   Rice_percentage  Shrubland_acres  Shrubland_percentage  Tomatoes_acres  \\\n",
       "0         0.000242     30958.051185              8.423298        4.670295   \n",
       "1         0.000000    247783.390805             52.535935        0.000000   \n",
       "2         0.000347     52457.865415             16.379744        0.000000   \n",
       "3        10.482705     55372.129495              5.495419       94.295480   \n",
       "4         0.000000    106091.088405             16.715249        0.000000   \n",
       "\n",
       "   Tomatoes_percentage  Vegs & Fruits_acres  Vegs & Fruits_percentage  \\\n",
       "0             0.001271           164.127510                  0.044657   \n",
       "1             0.000000             0.000000                  0.000000   \n",
       "2             0.000000             1.779160                  0.000556   \n",
       "3             0.009358           469.475845                  0.046593   \n",
       "4             0.000000             0.667185                  0.000105   \n",
       "\n",
       "   Walnuts_acres  Walnuts_percentage   Water_acres  Water_percentage  \\\n",
       "0       4.670295            0.001271  19403.518960          5.279454   \n",
       "1       0.000000            0.000000   2650.503610          0.561969   \n",
       "2     122.094855            0.038124   5822.745890          1.818128   \n",
       "3   42057.340845            4.173990  21360.372565          2.119915   \n",
       "4     425.664030            0.067066  13178.682910          2.076376   \n",
       "\n",
       "   Wetlands_acres  Wetlands_percentage  Winter Wheat_acres  \\\n",
       "0     4497.494085             1.223712          624.485160   \n",
       "1     1297.452430             0.275091            0.000000   \n",
       "2      105.860020             0.033054          479.483620   \n",
       "3    11589.893030             1.150241         4257.085090   \n",
       "4      245.079290             0.038614           11.786935   \n",
       "\n",
       "   Winter Wheat_percentage  max_elevation  min_elevation  \\\n",
       "0                 0.169915           1242            -42   \n",
       "1                 0.000000           3556           1442   \n",
       "2                 0.149717           3121             43   \n",
       "3                 0.422495           2192             -1   \n",
       "4                 0.001857           3522            787   \n",
       "\n",
       "   Avg Air Temp (F)_Weekly  Avg Rel Hum (%)_Weekly  \\\n",
       "0                44.214286               82.785714   \n",
       "1                29.657143               76.514286   \n",
       "2                34.114286               83.571429   \n",
       "3                40.985714               81.285714   \n",
       "4                41.928571               93.014286   \n",
       "\n",
       "   Avg Wind Speed (mph)_Weekly  Dew Point (F)_Weekly  Max Air Temp (F)_Weekly  \\\n",
       "0                     2.392857             39.321429                54.157143   \n",
       "1                     3.228571             21.328571                34.428571   \n",
       "2                     3.157143             29.585714                40.071429   \n",
       "3                     3.142857             35.557143                50.114286   \n",
       "4                     5.657143             39.000000                50.142857   \n",
       "\n",
       "   Max Rel Hum (%)_Weekly  Min Air Temp (F)_Weekly  Min Rel Hum (%)_Weekly  \\\n",
       "0               96.500000                35.771429               60.785714   \n",
       "1               91.857143                22.857143               55.428571   \n",
       "2               96.000000                27.757143               66.571429   \n",
       "3               91.285714                32.171429               62.857143   \n",
       "4              100.000000                35.571429               74.142857   \n",
       "\n",
       "   Precip (in)_Weekly  Avg Air Temp (F)_month  Avg Rel Hum (%)_month  \\\n",
       "0            0.095714               45.506897              78.189655   \n",
       "1            0.000000               30.789655              68.162069   \n",
       "2            0.141429               34.289655              76.724138   \n",
       "3            0.117143               42.389655              77.448276   \n",
       "4            0.000000               42.931034              87.017241   \n",
       "\n",
       "   Avg Wind Speed (mph)_month  Dew Point (F)_month  Max Air Temp (F)_month  \\\n",
       "0                    2.915517            38.932759               55.896552   \n",
       "1                    4.968966            19.600000               39.344828   \n",
       "2                    3.606897            27.410345               41.200000   \n",
       "3                    3.848276            35.586207               52.455172   \n",
       "4                    6.268966            37.196552               52.827586   \n",
       "\n",
       "   Max Rel Hum (%)_month  Min Air Temp (F)_month  Min Rel Hum (%)_month  \\\n",
       "0              95.448276               35.725862              55.810345   \n",
       "1              86.000000               22.758621              46.344828   \n",
       "2              93.172414               27.768966              58.310345   \n",
       "3              88.965517               33.365517              58.862069   \n",
       "4              97.551724               34.344828              61.275862   \n",
       "\n",
       "   Precip (in)_month  Alpine  Amador  Butte  Calaveras  Colusa  Contra Costa  \\\n",
       "0           0.130172       0       0      0          0       0             0   \n",
       "1           0.000000       1       0      0          0       0             0   \n",
       "2           0.155517       0       1      0          0       0             0   \n",
       "3           0.175517       0       0      1          0       0             0   \n",
       "4           0.000000       0       0      0          1       0             0   \n",
       "\n",
       "   Del Norte  El Dorado  Fresno  Glenn  Humboldt  Imperial  Inyo  Kern  Kings  \\\n",
       "0          0          0       0      0         0         0     0     0      0   \n",
       "1          0          0       0      0         0         0     0     0      0   \n",
       "2          0          0       0      0         0         0     0     0      0   \n",
       "3          0          0       0      0         0         0     0     0      0   \n",
       "4          0          0       0      0         0         0     0     0      0   \n",
       "\n",
       "   Lake  Lassen  Los Angeles  Madera  Marin  Mariposa  Mendocino  Merced  \\\n",
       "0     0       0            0       0      0         0          0       0   \n",
       "1     0       0            0       0      0         0          0       0   \n",
       "2     0       0            0       0      0         0          0       0   \n",
       "3     0       0            0       0      0         0          0       0   \n",
       "4     0       0            0       0      0         0          0       0   \n",
       "\n",
       "   Modoc  Mono  Monterey  Napa  Nevada  Orange  Placer  Plumas  Riverside  \\\n",
       "0      0     0         0     0       0       0       0       0          0   \n",
       "1      0     0         0     0       0       0       0       0          0   \n",
       "2      0     0         0     0       0       0       0       0          0   \n",
       "3      0     0         0     0       0       0       0       0          0   \n",
       "4      0     0         0     0       0       0       0       0          0   \n",
       "\n",
       "   Sacramento  San Benito  San Bernardino  San Diego  San Francisco  \\\n",
       "0           0           0               0          0              0   \n",
       "1           0           0               0          0              0   \n",
       "2           0           0               0          0              0   \n",
       "3           0           0               0          0              0   \n",
       "4           0           0               0          0              0   \n",
       "\n",
       "   San Joaquin  San Luis Obispo  San Mateo  Santa Barbara  Santa Clara  \\\n",
       "0            0                0          0              0            0   \n",
       "1            0                0          0              0            0   \n",
       "2            0                0          0              0            0   \n",
       "3            0                0          0              0            0   \n",
       "4            0                0          0              0            0   \n",
       "\n",
       "   Santa Cruz  Shasta  Sierra  Siskiyou  Solano  Sonoma  Stanislaus  Sutter  \\\n",
       "0           0       0       0         0       0       0           0       0   \n",
       "1           0       0       0         0       0       0           0       0   \n",
       "2           0       0       0         0       0       0           0       0   \n",
       "3           0       0       0         0       0       0           0       0   \n",
       "4           0       0       0         0       0       0           0       0   \n",
       "\n",
       "   Tehama  Trinity  Tulare  Tuolumne  Ventura  Yolo  Yuba  2  3  4  5  6  7  \\\n",
       "0       0        0       0         0        0     0     0  0  0  0  0  0  0   \n",
       "1       0        0       0         0        0     0     0  0  0  0  0  0  0   \n",
       "2       0        0       0         0        0     0     0  0  0  0  0  0  0   \n",
       "3       0        0       0         0        0     0     0  0  0  0  0  0  0   \n",
       "4       0        0       0         0        0     0     0  0  0  0  0  0  0   \n",
       "\n",
       "   8  9  10  11  12  \n",
       "0  0  0   0   0   0  \n",
       "1  0  0   0   0   0  \n",
       "2  0  0   0   0   0  \n",
       "3  0  0   0   0   0  \n",
       "4  0  0   0   0   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split our data into a training dataset and a test dataset. We want to do this before we work on the class imbalance so that we have a holdout set of data to test the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into target variable and features\n",
    "y = df2.fire_started\n",
    "X = df2.drop(columns = ['fire_started'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create a training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,  y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling Minority Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high class imbalance in the target variable. As seen in the EDA, the grand majority of the target variable are instances of no wildfire. In this case, we use upsampling the instances of wildfire to resolve the class imbalance issue. The first step to combine the `X_train` and `y_train` dataframes back into one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the X_train and y_train back into one training dataframe\n",
    "training = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to split the dataframe into the majority class and the minority class. In this case, the majority class are observations of weeks with no wildfire incidents. The minority class are observations of weeks with one or more wildfire incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the target variable by class into two dataframes\n",
    "no_fire = training[training.fire_started == 0] # 0 = No Wildfire\n",
    "fire = training[training.fire_started == 1] # 1 = Wildfire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to upsample the minority class with replacement. The minority class should be equal to the same number of observations and then recombine the two class dataframes back into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the minority class (wildfire)\n",
    "fire_upsampled = resample(fire,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=no_fire.shape[0], # match number in majority class\n",
    "                          random_state=42) # reproducible result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe combining the target classes\n",
    "resampled_df = pd.concat([no_fire, fire_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    12812\n",
       "0.0    12812\n",
       "Name: fire_started, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double-check the different target classes\n",
    "resampled_df.fire_started.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no longer any class imbalance between incidents of wildfire and incidents with no wildfires. We split the dataframe back into training target variable and the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = resampled_df.fire_started\n",
    "X_train = resampled_df.drop(columns = ['fire_started'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of model we tried was logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our base model, we run the resampled training data through a logistic regression model with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malcolmkatzenbach/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a logistic regression model\n",
    "logreg = LogisticRegression(random_state = 0) # random state for consistant results\n",
    "# Train model on resampled training data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the target variable on the training dataset\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "# Use the model to predict the target variable on the test dataset\n",
    "y_hat_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.680963106303334 Test 0.1476340694006309\n",
      "Accuracy Score: Training 0.6075163908835467 Test 0.40471469486671074\n",
      "Recall Score: Training 0.8377302528879176 Test 0.8068965517241379\n"
     ]
    }
   ],
   "source": [
    "# Print the f1 score metric on both the training and test predictions to check for overfitting\n",
    "print('F1 Score: Training', f1_score(y_train, y_hat_train), 'Test', f1_score(y_test, y_hat_test))\n",
    "# Print the accuracy score metric on both the training and test predictions to check for overfitting\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, y_hat_train), 'Test', accuracy_score(y_test, y_hat_test))\n",
    "# Print the recall score metric on both the training and test predictions to check for overfitting\n",
    "print('Recall Score: Training', recall_score(y_train, y_hat_train), 'Test', recall_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our metrics show that the recall score is higher than the accuracy score, and we have a low f1 score. We use a confusion matrix to find the false positive and false negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1603, 2646],\n",
       "       [  56,  234]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, the majority of predictions are false positives. We want to try to decrease the number of false positives in our next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Scaled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first iteration we want to check how normalizing the features will change our score. To normalize our data, we will use a Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insantiate the StandardScaler()\n",
    "ss = StandardScaler()\n",
    "# Fit the feature training data\n",
    "ss.fit(X_train)\n",
    "\n",
    "# Transform both the training and test features\n",
    "X_train_scaled = ss.transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Standar Scaler\n",
    "pickle_out = open(\"ss.pickle\",\"wb\")\n",
    "pickle.dump(ss, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new logistic regression model\n",
    "logreg1 = LogisticRegression(solver = 'liblinear')\n",
    "# Fit the data to the new scaled data\n",
    "logreg1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to predict target variable on the training dataset\n",
    "y_hat_train1 = logreg1.predict(X_train_scaled)\n",
    "# Use model to predict target variable on the test dataset\n",
    "y_hat_test1 = logreg1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.8126948775055679 Test 0.28343558282208586\n",
      "Accuracy Score: Training 0.8030752419606619 Test 0.7426745979290592\n",
      "Recall Score: Training 0.8544333437402435 Test 0.7965517241379311\n"
     ]
    }
   ],
   "source": [
    "# Print the f1 score metric on both the training and test predictions to check for overfitting\n",
    "print('F1 Score: Training', f1_score(y_train, y_hat_train1), 'Test', f1_score(y_test, y_hat_test1))\n",
    "# Print the accuracy score metric on both the training and test predictions to check for overfitting\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, y_hat_train1), 'Test', accuracy_score(y_test, y_hat_test1))\n",
    "# Print the recall score metric on both the training and test predictions to check for overfitting\n",
    "print('Recall Score: Training', recall_score(y_train, y_hat_train1), 'Test', recall_score(y_test, y_hat_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from our metrics show an increase in score across the board. However, the recall and accuracy score are still much better than the f1 score. The f1 score is highly overfit, while the recall and accuracy scores are slightly overfit. We check the confusion matrix next to check the value counts for false positive and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3140, 1109],\n",
       "       [  59,  231]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows as in the previous model, the most error occurs from the model predicting false positives, however we have decrease the number of false positives by more than half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle logistic model\n",
    "with open(\"../models/best_logistic.pickle\", \"wb\") as best_logistic:\n",
    "    pickle.dump(logreg1, best_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8011629344880593\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(logreg1, X_train_scaled, y_train, cv=10)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbor (KNN) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second type of model we used was K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base KNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our base knn model, we chose k as 3. Due to how knn models function, the number of nearest neighbors should always be odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a knn model using 3 nearest neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit knn model using the scaled data from the previous scaled logistic model\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target variable for both the train and test datasets.\n",
    "knn_train = knn.predict(X_train_scaled)\n",
    "knn_test = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.975706343766659 Test 0.2788339670468948\n",
      "Accuracy Score: Training 0.9751014673743366 Test 0.874641991628112\n",
      "Recall Score: Training 1.0 Test 0.3793103448275862\n"
     ]
    }
   ],
   "source": [
    "# Print the f1 score metric on both the training and test predictions to check for overfitting\n",
    "print('F1 Score: Training', f1_score(y_train, knn_train), 'Test', f1_score(y_test, knn_test))\n",
    "# Print the sccuracy score metric on both the training and test predictions to check for overfitting\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, knn_train), 'Test', accuracy_score(y_test, knn_test))\n",
    "# Print the recall score metric on both the training and test predictions to check for overfitting\n",
    "print('Recall Score: Training', recall_score(y_train, knn_train), 'Test', recall_score(y_test, knn_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that while the test accuracy is high, the recall score and the f1 score are highly overfit. We check the confusion matrix to check the values of the false positive and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3860,  389],\n",
       "       [ 180,  110]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, knn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous models, the f1 score and recall are highly overfit. The main error is in the prediction of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Number of Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the number (k) nearest neighbors, want to find the value of k that will return the max value for a given metric. Due to the business problem of wildfires, we want to reduce the number of false positives and thus maximise recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find the max recall score and return the score along with the k value\n",
    "def max_value(l):\n",
    "    max_val = max(l)\n",
    "    max_idx = l.index(max_val)\n",
    "    return max_idx, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.7413793103448276\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list for recall scores\n",
    "k_scores = []\n",
    "# Choose a range of k values to test\n",
    "k_range = list(range(1, 21))\n",
    "# Iterate through the different k values\n",
    "for k in k_range:\n",
    "    # Instantiate new knn model with k nearest neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    # Fit knn model on scaled training data\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    # Use model to predict target variable on testing set\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    # Find the recall score\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    # Append recall score to list of recall scorees\n",
    "    k_scores.append(recall)\n",
    "\n",
    "# Find max recall score\n",
    "idx, val = max_value(k_scores)\n",
    "# Print max recall score and it corresponding k value\n",
    "print(idx + 1, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best k value is 19 with a recall score of 74%. We rerun the model using k=19 and check the f1 and accuracy metrics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.883275080458179 Test 0.24826789838337182\n",
      "Accuracy Score: Training 0.8683655947549173 Test 0.7131526768010575\n",
      "Recall Score: Training 0.9960974086793631 Test 0.7413793103448276\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 19)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "knn_train1 = knn.predict(X_train_scaled)\n",
    "print('F1 Score: Training', f1_score(y_train, knn_train1), 'Test', f1_score(y_test, y_pred))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, knn_train1), 'Test', accuracy_score(y_test, y_pred))\n",
    "print('Recall Score: Training', recall_score(y_train, knn_train1), 'Test', recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a highly increased test scores for recall, however the accuracy is much worse and the f1 score slight worse. The model is still highly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3022, 1227],\n",
       "       [  75,  215]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of error from this model is still predicting false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle KNN model\n",
    "with open(\"../models/best_knn.pickle\", \"wb\") as best_knn:\n",
    "    pickle.dump(knn, best_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8476827201973194\n"
     ]
    }
   ],
   "source": [
    "scores_knn = cross_val_score(knn, X_train_scaled, y_train, cv=10)\n",
    "print(scores_knn.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our base model we will instantiate a Decision Tree Classifier with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 0)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 Test 0.24087591240875914\n",
      "Accuracy Score: Training 1.0 Test 0.9083498567966513\n",
      "Recall Score: Training 1.0 Test 0.22758620689655173\n"
     ]
    }
   ],
   "source": [
    "dt_train = dt.predict(X_train)\n",
    "dt_test = dt.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train), 'Test', f1_score(y_test, dt_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train), 'Test', accuracy_score(y_test, dt_test))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train), 'Test', recall_score(y_test, dt_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model has a high accuracy score, but a low f1 and recall score. The model is also highly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4057,  192],\n",
       "       [ 224,   66]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the error is from predicting false negatives, while the false positives are also high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first tuning, we want to check a wide range for the parameters of max depth, max_features and the min_sample_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth' : range(1, 21, 1), 'max_features' : range(55, 75, 1), 'min_samples_split' : range(15, 25, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the grid search, we use the f1 scoring metric so that ideally both accuracy and recall scores will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4000 candidates, totalling 40000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9768 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 11218 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 14418 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 16168 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 18018 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 19968 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 22018 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 24168 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 26418 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done 28768 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 31218 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 33768 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 36418 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done 39168 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 40000 out of 40000 | elapsed: 18.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(1, 21),\n",
       "                         'max_features': range(55, 75),\n",
       "                         'min_samples_split': range(15, 25)},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtg = DecisionTreeClassifier(random_state = 0)\n",
    "grid_model = GridSearchCV(dtg, parameters, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "\n",
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9471360686532485\n",
      "{'max_depth': 20, 'max_features': 66, 'min_samples_split': 15}\n",
      "DecisionTreeClassifier(max_depth=20, max_features=66, min_samples_split=15,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_model.best_score_)\n",
    "print(grid_model.best_params_)\n",
    "print(grid_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the parameters to the range tested, the `max_depth` parameter is at the top end, while `min_samples_split` is at the bottoom of the ranges tested. The `max_features` is in the center of the range tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 Test 0.24087591240875914\n",
      "Accuracy Score: Training 1.0 Test 0.9083498567966513\n",
      "Recall Score: Training 1.0 Test 0.22758620689655173\n"
     ]
    }
   ],
   "source": [
    "dt_train2 = dt.predict(X_train)\n",
    "dt_test2 = dt.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train2), 'Test', f1_score(y_test, dt_test2))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train2), 'Test', accuracy_score(y_test, dt_test2))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train2), 'Test', recall_score(y_test, dt_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still highly overfit. The test recall and f1 score are still much worse than the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4057,  192],\n",
       "       [ 224,   66]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model the majority of the error is still in predicting false negatives, hence the low recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearch 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous model, the results showed that it was highly overfit and could not generalize well. By reducing the `max_depth`, it should reduce the overfitting. In the previous model the `min_samples_split` was at a low end of the range, so for model we reduce the range. The `max_features` will test the same range of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters2 = {'max_depth' : range(1, 15, 1), 'max_features' : range(55, 75, 1), 'min_samples_split' : range(10, 20, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2800 candidates, totalling 28000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 804 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1504 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2404 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3504 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4804 tasks      | elapsed:   56.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6304 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8004 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9904 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 12004 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 14304 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 16804 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 18728 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 20178 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 21728 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 23378 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 25128 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 26978 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 28000 out of 28000 | elapsed: 10.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(1, 15),\n",
       "                         'max_features': range(55, 75),\n",
       "                         'min_samples_split': range(10, 20)},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtg2 = DecisionTreeClassifier(random_state = 0)\n",
    "dtg2_model = GridSearchCV(dtg2, parameters2, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "\n",
    "dtg2_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9242727259779653\n",
      "{'max_depth': 14, 'max_features': 73, 'min_samples_split': 13}\n",
      "DecisionTreeClassifier(max_depth=14, max_features=73, min_samples_split=13,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(dtg2_model.best_score_)\n",
    "print(dtg2_model.best_params_)\n",
    "print(dtg2_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.9281367719501593 Test 0.2643312101910828\n",
      "Accuracy Score: Training 0.9225725881985638 Test 0.7964309319233311\n",
      "Recall Score: Training 1.0 Test 0.5724137931034483\n"
     ]
    }
   ],
   "source": [
    "dt_train3 = dtg2_model.predict(X_train)\n",
    "dt_test3 = dtg2_model.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train3), 'Test', f1_score(y_test, dt_test3))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train3), 'Test', accuracy_score(y_test, dt_test3))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train3), 'Test', recall_score(y_test, dt_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reducing the max depth, we were able to reduce overfitting in our recall score. From the confusion matrix, we can sse that while the number of false. negatives decreased, the number of false positives increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3449,  800],\n",
       "       [ 124,  166]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearch 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the increase in test recall score and increase in the test f1 score, we reduce the possible maximum depth by a couple. We rerun the gridsearch with the same ranges for the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters3 = {'max_depth' : range(10, 13, 1), 'max_features' : range(35, 65, 1), 'min_samples_split' : range(14, 30, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1440 candidates, totalling 14400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:   38.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9768 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11218 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 14400 out of 14400 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(10, 13),\n",
       "                         'max_features': range(35, 65),\n",
       "                         'min_samples_split': range(14, 30)},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtg3 = DecisionTreeClassifier(random_state = 0)\n",
    "dtg3_model = GridSearchCV(dtg3, parameters3, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "\n",
    "dtg3_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9101123648575825\n",
      "{'max_depth': 12, 'max_features': 43, 'min_samples_split': 14}\n",
      "DecisionTreeClassifier(max_depth=12, max_features=43, min_samples_split=14,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(dtg3_model.best_score_)\n",
    "print(dtg3_model.best_params_)\n",
    "print(dtg3_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.9239783001808318 Test 0.2807843137254902\n",
      "Accuracy Score: Training 0.9179675304402123 Test 0.7979731218330028\n",
      "Recall Score: Training 0.9970340305963159 Test 0.6172413793103448\n"
     ]
    }
   ],
   "source": [
    "dt_train4 = dtg3_model.predict(X_train)\n",
    "dt_test4 = dtg3_model.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train4), 'Test', f1_score(y_test, dt_test4))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train4), 'Test', accuracy_score(y_test, dt_test4))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train4), 'Test', recall_score(y_test, dt_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score for recall and f1 both increased and the test accuracy remained the same. This successfully reduced overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3443,  806],\n",
       "       [ 111,  179]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows the predicted false negatives reduced by 13 predictions, however the false positives increased by 6 predictions since the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearch 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this iteration, we increased the `min_samples_split` to decrease the overfitting. Considering the parameter has always chosen the smallest in the range tested, we tested multiple minimums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters4 = {\n",
    "    'max_depth' : range(8, 12, 1), \n",
    "    'max_features' : range(64, 75, 1), \n",
    "    'min_samples_split' : range(400, 402, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 88 candidates, totalling 880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 880 out of 880 | elapsed:   25.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(8, 12),\n",
       "                         'max_features': range(64, 75),\n",
       "                         'min_samples_split': range(400, 402)},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtg4 = DecisionTreeClassifier(random_state = 0)\n",
    "dtg4_model = GridSearchCV(dtg4, parameters4, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "\n",
    "dtg4_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836401284806693\n",
      "{'max_depth': 10, 'max_features': 74, 'min_samples_split': 400}\n",
      "DecisionTreeClassifier(max_depth=10, max_features=74, min_samples_split=400,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(dtg4_model.best_score_)\n",
    "print(dtg4_model.best_params_)\n",
    "print(dtg4_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.8480333945086185 Test 0.25458996328029376\n",
      "Accuracy Score: Training 0.8351935685295035 Test 0.7316589557171183\n",
      "Recall Score: Training 0.9196846706212926 Test 0.7172413793103448\n"
     ]
    }
   ],
   "source": [
    "dt_train5 = dtg4_model.best_estimator_.predict(X_train)\n",
    "dt_test5 = dtg4_model.best_estimator_.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train5), 'Test', f1_score(y_test, dt_test5))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train5), 'Test', accuracy_score(y_test, dt_test5))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train5), 'Test', recall_score(y_test, dt_test5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model found diminishing returns with increasing the minimum sample at 400 samples. The model is still overfit, but the overfitting seen with the recall score was reduced by 18%. The difference between the training and test score for accuracy also decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3113, 1136],\n",
       "       [  82,  208]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_test5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous model, the number of false negatives decreased, while the number of false positives increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Decision Tree Model\n",
    "with open(\"../models/best_decision_tree.pickle\", \"wb\") as best_decision_tree:\n",
    "    pickle.dump(dtg4_model.best_estimator_, best_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model we instantiate a Random Forest Classifier with a set `random_state` for reproducibility and kept everything else at their default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state = 0)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train = rfc.predict(X_train)\n",
    "rfc_test = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 0.27692307692307694\n",
      "Accuracy Score: Training 1.0 0.9275170742454285\n",
      "Recall Score: Training 1.0 0.21724137931034482\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score: Training', f1_score(y_train, rfc_train), f1_score(y_test, rfc_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, rfc_train), accuracy_score(y_test, rfc_test))\n",
    "print('Recall Score: Training', recall_score(y_train, rfc_train), recall_score(y_test, rfc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous models, the base model was able to have perfect training scores, however from the f1 score and recall, the model is highly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4147,  102],\n",
       "       [ 227,   63]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rfc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of error from this model occurs in the predictions of false negatives. We want to maximize the recall score, while not boosting the number of false positives too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters - GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gridSearch to reduce the overfitting of the model and increase the test scores. The chosen parameters to be tuned are `n_estimators`, `criterion`, `min_samples_split`, and `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [100,300],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'max_features': list(range(3,7))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree=GridSearchCV(RandomForestClassifier(random_state = 0), param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': [3, 4, 5, 6],\n",
       "                         'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'n_estimators': [100, 300]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9842912884846985\n",
      "{'criterion': 'gini', 'max_features': 5, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "RandomForestClassifier(max_features=5, min_samples_split=3, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_tree.best_score_)\n",
    "print(grid_tree.best_params_)\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the best paramters, the number of estimators is 100 and we can further tune this parameter by also checking 50 and 150. The max features if 5, which is at the high end of the range tested. The `min_samples_split` is at the low end of the range checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 0.28389830508474573\n",
      "Accuracy Score: Training 1.0 0.9255342586472791\n",
      "Recall Score: Training 1.0 0.23103448275862068\n"
     ]
    }
   ],
   "source": [
    "grid_tree_train = grid_tree.best_estimator_.predict(X_train)\n",
    "grid_tree_test = grid_tree.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, grid_tree_train), f1_score(y_test, grid_tree_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, grid_tree_train), accuracy_score(y_test, grid_tree_test))\n",
    "print('Recall Score: Training', recall_score(y_train, grid_tree_train), recall_score(y_test, grid_tree_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still highly overfit, however the test scores did increase slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4134,  115],\n",
       "       [ 223,   67]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, grid_tree_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the error continues to be from the number of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearch2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second iteration of tuning, we are trying maximize the test metrics by finding the best `n_estimators` and increasing the range of `max_features`, while tuning the other parameters using the same range as the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = { \n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'max_features': list(range(4,10))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree2 =GridSearchCV(RandomForestClassifier(random_state = 0), \n",
    "                         param_grid2, \n",
    "                         cv=5, \n",
    "                         scoring='f1', \n",
    "                         verbose=1, \n",
    "                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': [4, 5, 6, 7, 8, 9],\n",
       "                         'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'n_estimators': [50, 100, 150]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9848576101055144\n",
      "{'criterion': 'entropy', 'max_features': 9, 'min_samples_split': 3, 'n_estimators': 50}\n",
      "RandomForestClassifier(criterion='entropy', max_features=9, min_samples_split=3,\n",
      "                       n_estimators=50, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_tree2.best_score_)\n",
    "print(grid_tree2.best_params_)\n",
    "print(grid_tree2.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of estimators decreased and the number of max features incresed. The `min_samples_split` remained constant compared to the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 0.2733188720173536\n",
      "Accuracy Score: Training 1.0 0.9261951971799955\n",
      "Recall Score: Training 1.0 0.21724137931034482\n"
     ]
    }
   ],
   "source": [
    "grid_tree_train2 = grid_tree2.best_estimator_.predict(X_train)\n",
    "grid_tree_test2 = grid_tree2.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, grid_tree_train2), f1_score(y_test, grid_tree_test2))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, grid_tree_train2), accuracy_score(y_test, grid_tree_test2))\n",
    "print('Recall Score: Training', recall_score(y_train, grid_tree_train2), recall_score(y_test, grid_tree_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 and Recall Test Score both did slightly worse, however the number of features was at the high end of the range, so by increasing the range of max features should boost the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters - GridSearch 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous model, we will check another range around 50 for `n_estimators` and we will increase in the range of the `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid3 = { \n",
    "    'n_estimators': [35, 50, 65],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': list(range(2,10)),\n",
    "    'max_features': list(range(10,25))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree3 =GridSearchCV(RandomForestClassifier(random_state = 0), \n",
    "                         param_grid3, \n",
    "                         cv=5, \n",
    "                         scoring='f1', \n",
    "                         verbose=1, \n",
    "                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed: 24.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': [10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "                                          19, 20, 21, 22, 23, 24],\n",
       "                         'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'n_estimators': [35, 50, 65]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9854639621231124\n",
      "{'criterion': 'entropy', 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 50}\n",
      "RandomForestClassifier(criterion='entropy', max_features=20,\n",
      "                       min_samples_split=3, n_estimators=50, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_tree3.best_score_)\n",
    "print(grid_tree3.best_params_)\n",
    "print(grid_tree3.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 0.2538293216630197\n",
      "Accuracy Score: Training 1.0 0.9248733201145627\n",
      "Recall Score: Training 1.0 0.2\n"
     ]
    }
   ],
   "source": [
    "grid_tree_train3 = grid_tree3.best_estimator_.predict(X_train)\n",
    "grid_tree_test3 = grid_tree3.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, grid_tree_train3), f1_score(y_test, grid_tree_test3))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, grid_tree_train3), accuracy_score(y_test, grid_tree_test3))\n",
    "print('Recall Score: Training', recall_score(y_train, grid_tree_train3), recall_score(y_test, grid_tree_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test scores were worse than the previous model. This means that the overfitting was greater. In the next model we will try to reduce overfitting by increasing the `min_samples_split`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters - GridSearch 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this gridSearch, we checked increasing values of `min_samples_split` and `max_features` to reduce overfitting in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid4 = { \n",
    "    'n_estimators': [75, 100, 125],\n",
    "    'criterion': ['gini'],\n",
    "    'min_samples_split': list(range(1200, 1202)),\n",
    "    'max_features': list(range(30, 40))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree4 =GridSearchCV(RandomForestClassifier(random_state = 0), \n",
    "                         param_grid4, \n",
    "                         cv=5, \n",
    "                         scoring='f1', \n",
    "                         verbose=1, \n",
    "                         n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini'],\n",
       "                         'max_features': [30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "                                          39],\n",
       "                         'min_samples_split': [1200, 1201],\n",
       "                         'n_estimators': [75, 100, 125]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802184679073717\n",
      "{'criterion': 'gini', 'max_features': 36, 'min_samples_split': 1200, 'n_estimators': 100}\n",
      "RandomForestClassifier(max_features=36, min_samples_split=1200, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_tree4.best_score_)\n",
    "print(grid_tree4.best_params_)\n",
    "print(grid_tree4.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.8202098246411393 0.28629304523970295\n",
      "Accuracy Score: Training 0.8147439900093663 0.7671293236395682\n",
      "Recall Score: Training 0.8451451763971277 0.7310344827586207\n"
     ]
    }
   ],
   "source": [
    "grid_tree_train4 = grid_tree4.best_estimator_.predict(X_train)\n",
    "grid_tree_test4 = grid_tree4.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, grid_tree_train4), f1_score(y_test, grid_tree_test4))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, grid_tree_train4), accuracy_score(y_test, grid_tree_test4))\n",
    "print('Recall Score: Training', recall_score(y_train, grid_tree_train4), recall_score(y_test, grid_tree_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this tuning, we found that there was a decreasing return on overfitting of recall or accuracy after the min_samples_split reached over 1000 samples. The f1 score did not move above 30% throughout the different tunings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3270,  979],\n",
       "       [  78,  212]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, grid_tree_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of error is due to predicting false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Random Forest model\n",
    "with open(\"../models/best_random_forest.pickle\", \"wb\") as best_random_forest:\n",
    "    pickle.dump(grid_tree4.best_estimator_, best_random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_clf = AdaBoostClassifier(random_state = 0)\n",
    "adaboost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8085388698095536\n",
      "Recall: 0.8634093037777084\n",
      "Test:\n",
      "Accuracy: 0.7400308437981934\n",
      "Recall: 0.7620689655172413\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, adaboost_train_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_train, adaboost_train_preds)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, adaboost_test_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_test, adaboost_test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_adaboost1 = {\n",
    "    'n_estimators' : [50, 75, 100],\n",
    "    'learning_rate' : [0.5, .75, 1.0, 1.25, 1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=AdaBoostClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'n_estimators': [50, 75, 100]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_clf1 = AdaBoostClassifier(random_state = 0)\n",
    "adaboost_model1 = GridSearchCV(adaboost_clf1, para_adaboost1, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "adaboost_model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8259039750485042\n",
      "{'learning_rate': 1.0, 'n_estimators': 100}\n",
      "AdaBoostClassifier(n_estimators=100, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(adaboost_model1.best_score_)\n",
    "print(adaboost_model1.best_params_)\n",
    "print(adaboost_model1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_train_preds1 = adaboost_model1.best_estimator_.predict(X_train)\n",
    "adaboost_test_preds1 = adaboost_model1.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8217686543865127\n",
      "Recall: 0.8726974711208242\n",
      "Test:\n",
      "Accuracy: 0.7565543071161048\n",
      "Recall: 0.7655172413793103\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, adaboost_train_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_train, adaboost_train_preds1)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, adaboost_test_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_test, adaboost_test_preds1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3212, 1037],\n",
       "       [  68,  222]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, adaboost_test_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_adaboost2 = {\n",
    "    'n_estimators' : [100, 125, 150],\n",
    "    'learning_rate' : [0.5, .75, 1.0, 1.25, 1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=AdaBoostClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'n_estimators': [100, 125, 150]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_clf2 = AdaBoostClassifier(random_state = 0)\n",
    "adaboost_model2 = GridSearchCV(adaboost_clf2, para_adaboost2, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "adaboost_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8404693396132688\n",
      "{'learning_rate': 1.25, 'n_estimators': 150}\n",
      "AdaBoostClassifier(learning_rate=1.25, n_estimators=150, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(adaboost_model2.best_score_)\n",
    "print(adaboost_model2.best_params_)\n",
    "print(adaboost_model2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_train_preds2 = adaboost_model2.best_estimator_.predict(X_train)\n",
    "adaboost_test_preds2 = adaboost_model2.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8359740867936309\n",
      "Recall: 0.880502653762098\n",
      "Test:\n",
      "Accuracy: 0.7779246530072703\n",
      "Recall: 0.7\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, adaboost_train_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_train, adaboost_train_preds2)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, adaboost_test_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_test, adaboost_test_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3328,  921],\n",
       "       [  87,  203]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, adaboost_test_preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_clf = GradientBoostingClassifier(random_state = 0)\n",
    "gbt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_preds = gbt_clf.predict(X_train)\n",
    "gbt_test_preds = gbt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8481891976272244\n",
      "Recall: 0.9120355916328442\n",
      "F1: 0.8573000733675715\n",
      "Test:\n",
      "Accuracy: 0.7684512007050011\n",
      "Recall: 0.7586206896551724\n",
      "F1: 0.29510395707578807\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, gbt_train_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_train, gbt_train_preds)))\n",
    "print('F1: {}'.format(f1_score(y_train, gbt_train_preds)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, gbt_test_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_test, gbt_test_preds)))\n",
    "print('F1: {}'.format(f1_score(y_test, gbt_test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3268,  981],\n",
       "       [  70,  220]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, gbt_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base score is similar to the tuned models from other types of models. In Gradient Boosting the accuracy is slightly higher than the recall score. There is some overfitting in the model and the majority of the error is in the prediction of false positives as seen in all previous models. The f1 score however, is slightly higher than the previous models.\n",
    "\n",
    "To increase the metric scores, GridSearchCV will be used to tune the parameters of gradient boosting and to validate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three parameters that will be tuned are `n_estimators`, `learning_rate` and `max_depth`. The default values for these are 100, 0.1, and 3 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_gbt1 = {\n",
    "    'n_estimators' : [75, 100, 125],\n",
    "    'learning_rate' : [0.1, 0.5, .75, 1.0, 1.25, 1.5],\n",
    "    'max_depth' : [2, 3, 4, 5]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 22.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GradientBoostingClassifier(random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'max_depth': [2, 3, 4, 5],\n",
       "                         'n_estimators': [75, 100, 125]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_clf1 = GradientBoostingClassifier(random_state = 0)\n",
    "gbt_model1 = GridSearchCV(gbt_clf1, para_gbt1, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "gbt_model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9728599745423188\n",
      "{'learning_rate': 1.5, 'max_depth': 5, 'n_estimators': 125}\n",
      "GradientBoostingClassifier(learning_rate=1.5, max_depth=5, n_estimators=125,\n",
      "                           random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(gbt_model1.best_score_)\n",
    "print(gbt_model1.best_params_)\n",
    "print(gbt_model1.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best parameters, the `max_depth` and `n_estimators` are both at the highest range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_preds1 = gbt_model1.best_estimator_.predict(X_train)\n",
    "gbt_test_preds1 = gbt_model1.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "Test:\n",
      "Accuracy: 0.9041639127561136\n",
      "Recall: 0.3\n",
      "F1: 0.28571428571428564\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, gbt_train_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_train, gbt_train_preds1)))\n",
    "print('F1: {}'.format(f1_score(y_train, gbt_train_preds1)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, gbt_test_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_test, gbt_test_preds1)))\n",
    "print('F1: {}'.format(f1_score(y_test, gbt_test_preds1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4017,  232],\n",
       "       [ 203,   87]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, gbt_test_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training score is much higher than default model, however the except for accuracy, the other metric scores are much lower. The model is highly overfit to the training data. The most likely culprit for this is the `max_depth` parameter. The higher the number of splits, the better the model will do on the training data, but it will also increase the overfitting of the model. The best parameter for the `max_depth` will usually be the max number in the range. So for the next tuning, we will decrease the possible `max_depth`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_gbt2 = {\n",
    "    'n_estimators' : [200, 250, 275],\n",
    "    'learning_rate' : [0.1, 0.5, .75, 1.0, 1.25, 1.5],\n",
    "    'max_depth' : [3]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GradientBoostingClassifier(random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'max_depth': [3], 'n_estimators': [200, 250, 275]},\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_clf2 = GradientBoostingClassifier(random_state = 0)\n",
    "gbt_model2 = GridSearchCV(gbt_clf2, para_gbt2, cv = 10, scoring = 'recall', verbose = 1, n_jobs = -1)\n",
    "gbt_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "{'learning_rate': 0.75, 'max_depth': 3, 'n_estimators': 250}\n",
      "GradientBoostingClassifier(learning_rate=0.75, n_estimators=250, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(gbt_model2.best_score_)\n",
    "print(gbt_model2.best_params_)\n",
    "print(gbt_model2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_preds2 = gbt_model2.best_estimator_.predict(X_train)\n",
    "gbt_test_preds2 = gbt_model2.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.9908289103965032\n",
      "Recall: 1.0\n",
      "F1: 0.9909122549209173\n",
      "Test:\n",
      "Accuracy: 0.8960123375192773\n",
      "Recall: 0.36551724137931035\n",
      "F1: 0.30994152046783624\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, gbt_train_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_train, gbt_train_preds2)))\n",
    "print('F1: {}'.format(f1_score(y_train, gbt_train_preds2)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, gbt_test_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_test, gbt_test_preds2)))\n",
    "print('F1: {}'.format(f1_score(y_test, gbt_test_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
