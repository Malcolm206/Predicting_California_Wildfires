{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the California Wildfire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/california_wildfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>year</th>\n",
       "      <th>acres_burned</th>\n",
       "      <th>fire_started</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Alfalfa &amp; Hay_acres</th>\n",
       "      <th>Alfalfa &amp; Hay_percentage</th>\n",
       "      <th>Almonds_acres</th>\n",
       "      <th>Almonds_percentage</th>\n",
       "      <th>Barren_acres</th>\n",
       "      <th>Barren_percentage</th>\n",
       "      <th>Corn_acres</th>\n",
       "      <th>Corn_percentage</th>\n",
       "      <th>Cotton_acres</th>\n",
       "      <th>Cotton_percentage</th>\n",
       "      <th>Deciduous Forest_acres</th>\n",
       "      <th>Deciduous Forest_percentage</th>\n",
       "      <th>Evergreen Forest_acres</th>\n",
       "      <th>Evergreen Forest_percentage</th>\n",
       "      <th>Fallow_acres</th>\n",
       "      <th>Fallow_percentage</th>\n",
       "      <th>Fruit Trees_acres</th>\n",
       "      <th>Fruit Trees_percentage</th>\n",
       "      <th>Grain Crops_acres</th>\n",
       "      <th>Grain Crops_percentage</th>\n",
       "      <th>Grapes_acres</th>\n",
       "      <th>Grapes_percentage</th>\n",
       "      <th>Grassland_acres</th>\n",
       "      <th>Grassland_percentage</th>\n",
       "      <th>High Intensity Developed_acres</th>\n",
       "      <th>High Intensity Developed_percentage</th>\n",
       "      <th>Low Intensity Developed_acres</th>\n",
       "      <th>Low Intensity Developed_percentage</th>\n",
       "      <th>Mixed Forest_acres</th>\n",
       "      <th>Mixed Forest_percentage</th>\n",
       "      <th>Other Ocean/Mexico_acres</th>\n",
       "      <th>Other Ocean/Mexico_percentage</th>\n",
       "      <th>Other Tree Crops_acres</th>\n",
       "      <th>Other Tree Crops_percentage</th>\n",
       "      <th>Other_acres</th>\n",
       "      <th>Other_percentage</th>\n",
       "      <th>Rice_acres</th>\n",
       "      <th>Rice_percentage</th>\n",
       "      <th>Shrubland_acres</th>\n",
       "      <th>Shrubland_percentage</th>\n",
       "      <th>Tomatoes_acres</th>\n",
       "      <th>Tomatoes_percentage</th>\n",
       "      <th>Vegs &amp; Fruits_acres</th>\n",
       "      <th>Vegs &amp; Fruits_percentage</th>\n",
       "      <th>Walnuts_acres</th>\n",
       "      <th>Walnuts_percentage</th>\n",
       "      <th>Water_acres</th>\n",
       "      <th>Water_percentage</th>\n",
       "      <th>Wetlands_acres</th>\n",
       "      <th>Wetlands_percentage</th>\n",
       "      <th>Winter Wheat_acres</th>\n",
       "      <th>Winter Wheat_percentage</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>min_elevation</th>\n",
       "      <th>Avg Air Temp (F)_Weekly</th>\n",
       "      <th>Avg Rel Hum (%)_Weekly</th>\n",
       "      <th>Avg Wind Speed (mph)_Weekly</th>\n",
       "      <th>Dew Point (F)_Weekly</th>\n",
       "      <th>Max Air Temp (F)_Weekly</th>\n",
       "      <th>Max Rel Hum (%)_Weekly</th>\n",
       "      <th>Min Air Temp (F)_Weekly</th>\n",
       "      <th>Min Rel Hum (%)_Weekly</th>\n",
       "      <th>Precip (in)_Weekly</th>\n",
       "      <th>Avg Air Temp (F)_month</th>\n",
       "      <th>Avg Rel Hum (%)_month</th>\n",
       "      <th>Avg Wind Speed (mph)_month</th>\n",
       "      <th>Dew Point (F)_month</th>\n",
       "      <th>Max Air Temp (F)_month</th>\n",
       "      <th>Max Rel Hum (%)_month</th>\n",
       "      <th>Min Air Temp (F)_month</th>\n",
       "      <th>Min Rel Hum (%)_month</th>\n",
       "      <th>Precip (in)_month</th>\n",
       "      <th>Population</th>\n",
       "      <th>county_acres</th>\n",
       "      <th>pop_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1102.856805</td>\n",
       "      <td>0.300074</td>\n",
       "      <td>4.225505</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>194.595625</td>\n",
       "      <td>0.052947</td>\n",
       "      <td>4.670295</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.337480</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>7838.756565</td>\n",
       "      <td>2.132827</td>\n",
       "      <td>1536.749450</td>\n",
       "      <td>0.418130</td>\n",
       "      <td>1.779160</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>991.214515</td>\n",
       "      <td>0.269697</td>\n",
       "      <td>3722.447510</td>\n",
       "      <td>1.012831</td>\n",
       "      <td>153671.386680</td>\n",
       "      <td>41.812059</td>\n",
       "      <td>28431.421590</td>\n",
       "      <td>7.735834</td>\n",
       "      <td>39470.886995</td>\n",
       "      <td>10.739534</td>\n",
       "      <td>74885.956375</td>\n",
       "      <td>20.375531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.673405</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>30958.051185</td>\n",
       "      <td>8.423298</td>\n",
       "      <td>4.670295</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>164.127510</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>4.670295</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>19403.518960</td>\n",
       "      <td>5.279454</td>\n",
       "      <td>4497.494085</td>\n",
       "      <td>1.223712</td>\n",
       "      <td>624.485160</td>\n",
       "      <td>0.169915</td>\n",
       "      <td>1242</td>\n",
       "      <td>-42</td>\n",
       "      <td>44.214286</td>\n",
       "      <td>82.785714</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>39.321429</td>\n",
       "      <td>54.157143</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>35.771429</td>\n",
       "      <td>60.785714</td>\n",
       "      <td>0.095714</td>\n",
       "      <td>45.506897</td>\n",
       "      <td>78.189655</td>\n",
       "      <td>2.915517</td>\n",
       "      <td>38.932759</td>\n",
       "      <td>55.896552</td>\n",
       "      <td>95.448276</td>\n",
       "      <td>35.725862</td>\n",
       "      <td>55.810345</td>\n",
       "      <td>0.130172</td>\n",
       "      <td>1567167</td>\n",
       "      <td>528000</td>\n",
       "      <td>2.968119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>Alpine</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>189.035750</td>\n",
       "      <td>0.040080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15482.472715</td>\n",
       "      <td>3.282650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>194.595625</td>\n",
       "      <td>0.041259</td>\n",
       "      <td>195088.007530</td>\n",
       "      <td>41.363269</td>\n",
       "      <td>0.444790</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.222395</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5644.829890</td>\n",
       "      <td>1.196837</td>\n",
       "      <td>121.427670</td>\n",
       "      <td>0.025746</td>\n",
       "      <td>3192.480225</td>\n",
       "      <td>0.676881</td>\n",
       "      <td>0.667185</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>247783.390805</td>\n",
       "      <td>52.535935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2650.503610</td>\n",
       "      <td>0.561969</td>\n",
       "      <td>1297.452430</td>\n",
       "      <td>0.275091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3556</td>\n",
       "      <td>1442</td>\n",
       "      <td>29.657143</td>\n",
       "      <td>76.514286</td>\n",
       "      <td>3.228571</td>\n",
       "      <td>21.328571</td>\n",
       "      <td>34.428571</td>\n",
       "      <td>91.857143</td>\n",
       "      <td>22.857143</td>\n",
       "      <td>55.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.789655</td>\n",
       "      <td>68.162069</td>\n",
       "      <td>4.968966</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>39.344828</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>22.758621</td>\n",
       "      <td>46.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1164</td>\n",
       "      <td>465280</td>\n",
       "      <td>0.002502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>Amador</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1326.808570</td>\n",
       "      <td>0.414290</td>\n",
       "      <td>16.679625</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>1873.010690</td>\n",
       "      <td>0.584840</td>\n",
       "      <td>242.632945</td>\n",
       "      <td>0.075761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17190.911105</td>\n",
       "      <td>5.367789</td>\n",
       "      <td>114386.866695</td>\n",
       "      <td>35.716810</td>\n",
       "      <td>168.130620</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>12.009330</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>120.093300</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>2587.343430</td>\n",
       "      <td>0.807887</td>\n",
       "      <td>112912.610240</td>\n",
       "      <td>35.256480</td>\n",
       "      <td>440.119705</td>\n",
       "      <td>0.137425</td>\n",
       "      <td>8263.975805</td>\n",
       "      <td>2.580391</td>\n",
       "      <td>1727.119570</td>\n",
       "      <td>0.539286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.334370</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.111975</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>52457.865415</td>\n",
       "      <td>16.379744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.779160</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>122.094855</td>\n",
       "      <td>0.038124</td>\n",
       "      <td>5822.745890</td>\n",
       "      <td>1.818128</td>\n",
       "      <td>105.860020</td>\n",
       "      <td>0.033054</td>\n",
       "      <td>479.483620</td>\n",
       "      <td>0.149717</td>\n",
       "      <td>3121</td>\n",
       "      <td>43</td>\n",
       "      <td>34.114286</td>\n",
       "      <td>83.571429</td>\n",
       "      <td>3.157143</td>\n",
       "      <td>29.585714</td>\n",
       "      <td>40.071429</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>27.757143</td>\n",
       "      <td>66.571429</td>\n",
       "      <td>0.141429</td>\n",
       "      <td>34.289655</td>\n",
       "      <td>76.724138</td>\n",
       "      <td>3.606897</td>\n",
       "      <td>27.410345</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>93.172414</td>\n",
       "      <td>27.768966</td>\n",
       "      <td>58.310345</td>\n",
       "      <td>0.155517</td>\n",
       "      <td>37304</td>\n",
       "      <td>384640</td>\n",
       "      <td>0.096984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>Butte</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3777.156680</td>\n",
       "      <td>0.374865</td>\n",
       "      <td>46196.556585</td>\n",
       "      <td>4.584787</td>\n",
       "      <td>1869.452370</td>\n",
       "      <td>0.185534</td>\n",
       "      <td>2023.349710</td>\n",
       "      <td>0.200808</td>\n",
       "      <td>9.118195</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>33181.556395</td>\n",
       "      <td>3.293111</td>\n",
       "      <td>408193.790775</td>\n",
       "      <td>40.511281</td>\n",
       "      <td>56434.510410</td>\n",
       "      <td>5.600855</td>\n",
       "      <td>10563.317710</td>\n",
       "      <td>1.048359</td>\n",
       "      <td>2628.041715</td>\n",
       "      <td>0.260821</td>\n",
       "      <td>247.525635</td>\n",
       "      <td>0.024566</td>\n",
       "      <td>170758.216925</td>\n",
       "      <td>16.946936</td>\n",
       "      <td>4421.657390</td>\n",
       "      <td>0.438828</td>\n",
       "      <td>25520.048645</td>\n",
       "      <td>2.532743</td>\n",
       "      <td>165.684275</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>790.391830</td>\n",
       "      <td>0.078443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105624.281300</td>\n",
       "      <td>10.482705</td>\n",
       "      <td>55372.129495</td>\n",
       "      <td>5.495419</td>\n",
       "      <td>94.295480</td>\n",
       "      <td>0.009358</td>\n",
       "      <td>469.475845</td>\n",
       "      <td>0.046593</td>\n",
       "      <td>42057.340845</td>\n",
       "      <td>4.173990</td>\n",
       "      <td>21360.372565</td>\n",
       "      <td>2.119915</td>\n",
       "      <td>11589.893030</td>\n",
       "      <td>1.150241</td>\n",
       "      <td>4257.085090</td>\n",
       "      <td>0.422495</td>\n",
       "      <td>2192</td>\n",
       "      <td>-1</td>\n",
       "      <td>40.985714</td>\n",
       "      <td>81.285714</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>35.557143</td>\n",
       "      <td>50.114286</td>\n",
       "      <td>91.285714</td>\n",
       "      <td>32.171429</td>\n",
       "      <td>62.857143</td>\n",
       "      <td>0.117143</td>\n",
       "      <td>42.389655</td>\n",
       "      <td>77.448276</td>\n",
       "      <td>3.848276</td>\n",
       "      <td>35.586207</td>\n",
       "      <td>52.455172</td>\n",
       "      <td>88.965517</td>\n",
       "      <td>33.365517</td>\n",
       "      <td>58.862069</td>\n",
       "      <td>0.175517</td>\n",
       "      <td>222185</td>\n",
       "      <td>1065600</td>\n",
       "      <td>0.208507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>Calaveras</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>31.802485</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>28.466560</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>218.391890</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34882.878145</td>\n",
       "      <td>5.495994</td>\n",
       "      <td>255438.004310</td>\n",
       "      <td>40.245698</td>\n",
       "      <td>28.688955</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>12.231725</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>2.223950</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>522.183460</td>\n",
       "      <td>0.082273</td>\n",
       "      <td>207502.763615</td>\n",
       "      <td>32.693230</td>\n",
       "      <td>465.027945</td>\n",
       "      <td>0.073268</td>\n",
       "      <td>12257.745215</td>\n",
       "      <td>1.931277</td>\n",
       "      <td>3351.270255</td>\n",
       "      <td>0.528012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106091.088405</td>\n",
       "      <td>16.715249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667185</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>425.664030</td>\n",
       "      <td>0.067066</td>\n",
       "      <td>13178.682910</td>\n",
       "      <td>2.076376</td>\n",
       "      <td>245.079290</td>\n",
       "      <td>0.038614</td>\n",
       "      <td>11.786935</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>3522</td>\n",
       "      <td>787</td>\n",
       "      <td>41.928571</td>\n",
       "      <td>93.014286</td>\n",
       "      <td>5.657143</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>50.142857</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>35.571429</td>\n",
       "      <td>74.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.931034</td>\n",
       "      <td>87.017241</td>\n",
       "      <td>6.268966</td>\n",
       "      <td>37.196552</td>\n",
       "      <td>52.827586</td>\n",
       "      <td>97.551724</td>\n",
       "      <td>34.344828</td>\n",
       "      <td>61.275862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45424</td>\n",
       "      <td>663040</td>\n",
       "      <td>0.068509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>18149</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>Tulare</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>42714.518070</td>\n",
       "      <td>1.418197</td>\n",
       "      <td>70462.742220</td>\n",
       "      <td>2.339487</td>\n",
       "      <td>161563.962835</td>\n",
       "      <td>5.364207</td>\n",
       "      <td>26770.130940</td>\n",
       "      <td>0.888815</td>\n",
       "      <td>22104.061445</td>\n",
       "      <td>0.733894</td>\n",
       "      <td>23778.918190</td>\n",
       "      <td>0.789502</td>\n",
       "      <td>872890.589620</td>\n",
       "      <td>28.981501</td>\n",
       "      <td>107153.246925</td>\n",
       "      <td>3.557676</td>\n",
       "      <td>156216.030270</td>\n",
       "      <td>5.186647</td>\n",
       "      <td>113485.944550</td>\n",
       "      <td>3.767933</td>\n",
       "      <td>57675.474510</td>\n",
       "      <td>1.914927</td>\n",
       "      <td>423885.759580</td>\n",
       "      <td>14.073752</td>\n",
       "      <td>8425.434575</td>\n",
       "      <td>0.279739</td>\n",
       "      <td>43858.740345</td>\n",
       "      <td>1.456187</td>\n",
       "      <td>459.245675</td>\n",
       "      <td>0.015248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98145.804635</td>\n",
       "      <td>3.258613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>704249.128305</td>\n",
       "      <td>23.382308</td>\n",
       "      <td>524.185015</td>\n",
       "      <td>0.017404</td>\n",
       "      <td>4647.833105</td>\n",
       "      <td>0.154316</td>\n",
       "      <td>40724.750005</td>\n",
       "      <td>1.352133</td>\n",
       "      <td>8273.538790</td>\n",
       "      <td>0.274696</td>\n",
       "      <td>3639.494175</td>\n",
       "      <td>0.120838</td>\n",
       "      <td>20034.231180</td>\n",
       "      <td>0.665172</td>\n",
       "      <td>4409</td>\n",
       "      <td>149</td>\n",
       "      <td>48.457143</td>\n",
       "      <td>90.384615</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>45.915385</td>\n",
       "      <td>55.550000</td>\n",
       "      <td>98.285714</td>\n",
       "      <td>42.450000</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>0.009286</td>\n",
       "      <td>46.139655</td>\n",
       "      <td>84.741379</td>\n",
       "      <td>2.160345</td>\n",
       "      <td>41.820690</td>\n",
       "      <td>55.243103</td>\n",
       "      <td>97.758621</td>\n",
       "      <td>38.431034</td>\n",
       "      <td>64.431034</td>\n",
       "      <td>0.016207</td>\n",
       "      <td>475834</td>\n",
       "      <td>3100160</td>\n",
       "      <td>0.153487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18150</th>\n",
       "      <td>18150</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>Tuolumne</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>58.712280</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>3.335925</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>133226.391935</td>\n",
       "      <td>9.465577</td>\n",
       "      <td>15.122860</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.222395</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>18974.741400</td>\n",
       "      <td>1.348133</td>\n",
       "      <td>671704.066400</td>\n",
       "      <td>47.723777</td>\n",
       "      <td>100.300145</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>73.612745</td>\n",
       "      <td>0.005230</td>\n",
       "      <td>41.587865</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>2.001555</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>108617.718000</td>\n",
       "      <td>7.717160</td>\n",
       "      <td>531.746445</td>\n",
       "      <td>0.037780</td>\n",
       "      <td>12267.975385</td>\n",
       "      <td>0.871625</td>\n",
       "      <td>485.933075</td>\n",
       "      <td>0.034525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.111975</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>428594.528915</td>\n",
       "      <td>30.451133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.447900</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.444790</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>30500.807065</td>\n",
       "      <td>2.167046</td>\n",
       "      <td>2276.435220</td>\n",
       "      <td>0.161738</td>\n",
       "      <td>1.779160</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>3959</td>\n",
       "      <td>374</td>\n",
       "      <td>50.442857</td>\n",
       "      <td>83.700000</td>\n",
       "      <td>3.471429</td>\n",
       "      <td>45.800000</td>\n",
       "      <td>58.714286</td>\n",
       "      <td>93.142857</td>\n",
       "      <td>26.142857</td>\n",
       "      <td>36.714286</td>\n",
       "      <td>0.161429</td>\n",
       "      <td>48.479310</td>\n",
       "      <td>80.844828</td>\n",
       "      <td>4.944828</td>\n",
       "      <td>42.372414</td>\n",
       "      <td>58.448276</td>\n",
       "      <td>93.379310</td>\n",
       "      <td>36.896552</td>\n",
       "      <td>51.689655</td>\n",
       "      <td>0.081034</td>\n",
       "      <td>54740</td>\n",
       "      <td>1467520</td>\n",
       "      <td>0.037301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>18151</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>Ventura</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1149.114965</td>\n",
       "      <td>0.106102</td>\n",
       "      <td>3.780715</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>21708.420740</td>\n",
       "      <td>2.004423</td>\n",
       "      <td>170.799360</td>\n",
       "      <td>0.015771</td>\n",
       "      <td>78.505435</td>\n",
       "      <td>0.007249</td>\n",
       "      <td>68.497660</td>\n",
       "      <td>0.006325</td>\n",
       "      <td>144714.205660</td>\n",
       "      <td>13.362027</td>\n",
       "      <td>1861.890940</td>\n",
       "      <td>0.171916</td>\n",
       "      <td>22172.114315</td>\n",
       "      <td>2.047238</td>\n",
       "      <td>604.469610</td>\n",
       "      <td>0.055813</td>\n",
       "      <td>12.009330</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>56354.893000</td>\n",
       "      <td>5.203467</td>\n",
       "      <td>13949.281585</td>\n",
       "      <td>1.287992</td>\n",
       "      <td>63969.697800</td>\n",
       "      <td>5.906572</td>\n",
       "      <td>68343.317870</td>\n",
       "      <td>6.310405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.735605</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>648960.841725</td>\n",
       "      <td>59.921086</td>\n",
       "      <td>9.785380</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>4056.929590</td>\n",
       "      <td>0.374592</td>\n",
       "      <td>81.618965</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>5719.332215</td>\n",
       "      <td>0.528088</td>\n",
       "      <td>8922.042610</td>\n",
       "      <td>0.823807</td>\n",
       "      <td>181.919110</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>2332</td>\n",
       "      <td>34</td>\n",
       "      <td>54.692857</td>\n",
       "      <td>79.571429</td>\n",
       "      <td>2.907143</td>\n",
       "      <td>48.357143</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>44.800000</td>\n",
       "      <td>53.714286</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>54.527586</td>\n",
       "      <td>62.913793</td>\n",
       "      <td>3.968966</td>\n",
       "      <td>41.179310</td>\n",
       "      <td>67.987931</td>\n",
       "      <td>89.500000</td>\n",
       "      <td>43.296552</td>\n",
       "      <td>37.379310</td>\n",
       "      <td>0.032586</td>\n",
       "      <td>859073</td>\n",
       "      <td>1192960</td>\n",
       "      <td>0.720119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18152</th>\n",
       "      <td>18152</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>Yolo</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>49157.968405</td>\n",
       "      <td>8.216211</td>\n",
       "      <td>28688.732605</td>\n",
       "      <td>4.795005</td>\n",
       "      <td>898.920590</td>\n",
       "      <td>0.150245</td>\n",
       "      <td>6687.640045</td>\n",
       "      <td>1.117765</td>\n",
       "      <td>26.465005</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>258.867780</td>\n",
       "      <td>0.043267</td>\n",
       "      <td>5127.539120</td>\n",
       "      <td>0.857011</td>\n",
       "      <td>86129.357995</td>\n",
       "      <td>14.395570</td>\n",
       "      <td>8848.207470</td>\n",
       "      <td>1.478880</td>\n",
       "      <td>48040.211135</td>\n",
       "      <td>8.029390</td>\n",
       "      <td>19023.445905</td>\n",
       "      <td>3.179559</td>\n",
       "      <td>96715.582390</td>\n",
       "      <td>16.164941</td>\n",
       "      <td>5373.952780</td>\n",
       "      <td>0.898197</td>\n",
       "      <td>14182.351545</td>\n",
       "      <td>2.370423</td>\n",
       "      <td>11843.423330</td>\n",
       "      <td>1.979497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3236.292040</td>\n",
       "      <td>0.540910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38908.005250</td>\n",
       "      <td>6.503043</td>\n",
       "      <td>87093.217925</td>\n",
       "      <td>14.556669</td>\n",
       "      <td>29441.762075</td>\n",
       "      <td>4.920865</td>\n",
       "      <td>13333.025040</td>\n",
       "      <td>2.228468</td>\n",
       "      <td>18932.708745</td>\n",
       "      <td>3.164393</td>\n",
       "      <td>3213.607750</td>\n",
       "      <td>0.537119</td>\n",
       "      <td>4433.666720</td>\n",
       "      <td>0.741038</td>\n",
       "      <td>18707.645005</td>\n",
       "      <td>3.126776</td>\n",
       "      <td>346</td>\n",
       "      <td>-26</td>\n",
       "      <td>50.828571</td>\n",
       "      <td>89.571429</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>47.800000</td>\n",
       "      <td>57.928571</td>\n",
       "      <td>98.428571</td>\n",
       "      <td>43.585714</td>\n",
       "      <td>72.428571</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>48.689655</td>\n",
       "      <td>81.620690</td>\n",
       "      <td>4.986207</td>\n",
       "      <td>42.989655</td>\n",
       "      <td>57.137931</td>\n",
       "      <td>95.379310</td>\n",
       "      <td>40.768966</td>\n",
       "      <td>63.758621</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>221270</td>\n",
       "      <td>661760</td>\n",
       "      <td>0.334366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18153</th>\n",
       "      <td>18153</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>Yuba</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4708.546940</td>\n",
       "      <td>1.201779</td>\n",
       "      <td>1606.359085</td>\n",
       "      <td>0.409997</td>\n",
       "      <td>5447.343130</td>\n",
       "      <td>1.390344</td>\n",
       "      <td>1025.463345</td>\n",
       "      <td>0.261733</td>\n",
       "      <td>6.449455</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>26529.944340</td>\n",
       "      <td>6.771329</td>\n",
       "      <td>124401.980730</td>\n",
       "      <td>31.751548</td>\n",
       "      <td>35910.120650</td>\n",
       "      <td>9.165464</td>\n",
       "      <td>11661.059430</td>\n",
       "      <td>2.976293</td>\n",
       "      <td>677.859960</td>\n",
       "      <td>0.173013</td>\n",
       "      <td>26.909795</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>78849.924855</td>\n",
       "      <td>20.125139</td>\n",
       "      <td>1853.217535</td>\n",
       "      <td>0.473003</td>\n",
       "      <td>16694.303070</td>\n",
       "      <td>4.260945</td>\n",
       "      <td>958.967240</td>\n",
       "      <td>0.244761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1653.729220</td>\n",
       "      <td>0.422087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38336.894890</td>\n",
       "      <td>9.784858</td>\n",
       "      <td>14227.942520</td>\n",
       "      <td>3.631447</td>\n",
       "      <td>145.446330</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>552.873970</td>\n",
       "      <td>0.141112</td>\n",
       "      <td>16699.418155</td>\n",
       "      <td>4.262250</td>\n",
       "      <td>7790.941640</td>\n",
       "      <td>1.988509</td>\n",
       "      <td>1868.562790</td>\n",
       "      <td>0.476920</td>\n",
       "      <td>163.015535</td>\n",
       "      <td>0.041607</td>\n",
       "      <td>2541</td>\n",
       "      <td>199</td>\n",
       "      <td>50.600000</td>\n",
       "      <td>84.285714</td>\n",
       "      <td>3.257143</td>\n",
       "      <td>45.900000</td>\n",
       "      <td>57.414286</td>\n",
       "      <td>95.142857</td>\n",
       "      <td>44.071429</td>\n",
       "      <td>68.857143</td>\n",
       "      <td>0.141429</td>\n",
       "      <td>48.844828</td>\n",
       "      <td>73.379310</td>\n",
       "      <td>3.717241</td>\n",
       "      <td>40.279310</td>\n",
       "      <td>57.558621</td>\n",
       "      <td>89.655172</td>\n",
       "      <td>40.820690</td>\n",
       "      <td>55.586207</td>\n",
       "      <td>0.132759</td>\n",
       "      <td>74727</td>\n",
       "      <td>408960</td>\n",
       "      <td>0.182724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18154 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        date     county  year  acres_burned  fire_started  \\\n",
       "0               0  2013-01-06    Alameda  2013           0.0           0.0   \n",
       "1               1  2013-01-06     Alpine  2013           0.0           0.0   \n",
       "2               2  2013-01-06     Amador  2013           0.0           0.0   \n",
       "3               3  2013-01-06      Butte  2013           0.0           0.0   \n",
       "4               4  2013-01-06  Calaveras  2013           0.0           0.0   \n",
       "...           ...         ...        ...   ...           ...           ...   \n",
       "18149       18149  2018-12-30     Tulare  2018           0.0           0.0   \n",
       "18150       18150  2018-12-30   Tuolumne  2018           0.0           0.0   \n",
       "18151       18151  2018-12-30    Ventura  2018           0.0           0.0   \n",
       "18152       18152  2018-12-30       Yolo  2018           0.0           0.0   \n",
       "18153       18153  2018-12-30       Yuba  2018           0.0           0.0   \n",
       "\n",
       "       Unnamed: 0.1  Alfalfa & Hay_acres  Alfalfa & Hay_percentage  \\\n",
       "0                 2          1102.856805                  0.300074   \n",
       "1                 2           189.035750                  0.040080   \n",
       "2                 2          1326.808570                  0.414290   \n",
       "3                 2          3777.156680                  0.374865   \n",
       "4                 2            31.802485                  0.005011   \n",
       "...             ...                  ...                       ...   \n",
       "18149             2         42714.518070                  1.418197   \n",
       "18150             2            58.712280                  0.004171   \n",
       "18151             2          1149.114965                  0.106102   \n",
       "18152             2         49157.968405                  8.216211   \n",
       "18153             2          4708.546940                  1.201779   \n",
       "\n",
       "       Almonds_acres  Almonds_percentage   Barren_acres  Barren_percentage  \\\n",
       "0           4.225505            0.001150     194.595625           0.052947   \n",
       "1           0.000000            0.000000   15482.472715           3.282650   \n",
       "2          16.679625            0.005208    1873.010690           0.584840   \n",
       "3       46196.556585            4.584787    1869.452370           0.185534   \n",
       "4          28.466560            0.004485     218.391890           0.034409   \n",
       "...              ...                 ...            ...                ...   \n",
       "18149   70462.742220            2.339487  161563.962835           5.364207   \n",
       "18150       3.335925            0.000237  133226.391935           9.465577   \n",
       "18151       3.780715            0.000349   21708.420740           2.004423   \n",
       "18152   28688.732605            4.795005     898.920590           0.150245   \n",
       "18153    1606.359085            0.409997    5447.343130           1.390344   \n",
       "\n",
       "         Corn_acres  Corn_percentage  Cotton_acres  Cotton_percentage  \\\n",
       "0          4.670295         0.001271      0.000000           0.000000   \n",
       "1          0.000000         0.000000      0.000000           0.000000   \n",
       "2        242.632945         0.075761      0.000000           0.000000   \n",
       "3       2023.349710         0.200808      9.118195           0.000905   \n",
       "4          0.889580         0.000140      0.000000           0.000000   \n",
       "...             ...              ...           ...                ...   \n",
       "18149  26770.130940         0.888815  22104.061445           0.733894   \n",
       "18150     15.122860         0.001074      0.222395           0.000016   \n",
       "18151    170.799360         0.015771     78.505435           0.007249   \n",
       "18152   6687.640045         1.117765     26.465005           0.004423   \n",
       "18153   1025.463345         0.261733      6.449455           0.001646   \n",
       "\n",
       "       Deciduous Forest_acres  Deciduous Forest_percentage  \\\n",
       "0                    5.337480                     0.001452   \n",
       "1                  194.595625                     0.041259   \n",
       "2                17190.911105                     5.367789   \n",
       "3                33181.556395                     3.293111   \n",
       "4                34882.878145                     5.495994   \n",
       "...                       ...                          ...   \n",
       "18149            23778.918190                     0.789502   \n",
       "18150            18974.741400                     1.348133   \n",
       "18151               68.497660                     0.006325   \n",
       "18152              258.867780                     0.043267   \n",
       "18153            26529.944340                     6.771329   \n",
       "\n",
       "       Evergreen Forest_acres  Evergreen Forest_percentage   Fallow_acres  \\\n",
       "0                 7838.756565                     2.132827    1536.749450   \n",
       "1               195088.007530                    41.363269       0.444790   \n",
       "2               114386.866695                    35.716810     168.130620   \n",
       "3               408193.790775                    40.511281   56434.510410   \n",
       "4               255438.004310                    40.245698      28.688955   \n",
       "...                       ...                          ...            ...   \n",
       "18149           872890.589620                    28.981501  107153.246925   \n",
       "18150           671704.066400                    47.723777     100.300145   \n",
       "18151           144714.205660                    13.362027    1861.890940   \n",
       "18152             5127.539120                     0.857011   86129.357995   \n",
       "18153           124401.980730                    31.751548   35910.120650   \n",
       "\n",
       "       Fallow_percentage  Fruit Trees_acres  Fruit Trees_percentage  \\\n",
       "0               0.418130           1.779160                0.000484   \n",
       "1               0.000094           0.222395                0.000047   \n",
       "2               0.052498          12.009330                0.003750   \n",
       "3               5.600855       10563.317710                1.048359   \n",
       "4               0.004520          12.231725                0.001927   \n",
       "...                  ...                ...                     ...   \n",
       "18149           3.557676      156216.030270                5.186647   \n",
       "18150           0.007126          73.612745                0.005230   \n",
       "18151           0.171916       22172.114315                2.047238   \n",
       "18152          14.395570        8848.207470                1.478880   \n",
       "18153           9.165464       11661.059430                2.976293   \n",
       "\n",
       "       Grain Crops_acres  Grain Crops_percentage  Grapes_acres  \\\n",
       "0             991.214515                0.269697   3722.447510   \n",
       "1               0.000000                0.000000      0.000000   \n",
       "2             120.093300                0.037499   2587.343430   \n",
       "3            2628.041715                0.260821    247.525635   \n",
       "4               2.223950                0.000350    522.183460   \n",
       "...                  ...                     ...           ...   \n",
       "18149      113485.944550                3.767933  57675.474510   \n",
       "18150          41.587865                0.002955      2.001555   \n",
       "18151         604.469610                0.055813     12.009330   \n",
       "18152       48040.211135                8.029390  19023.445905   \n",
       "18153         677.859960                0.173013     26.909795   \n",
       "\n",
       "       Grapes_percentage  Grassland_acres  Grassland_percentage  \\\n",
       "0               1.012831    153671.386680             41.812059   \n",
       "1               0.000000      5644.829890              1.196837   \n",
       "2               0.807887    112912.610240             35.256480   \n",
       "3               0.024566    170758.216925             16.946936   \n",
       "4               0.082273    207502.763615             32.693230   \n",
       "...                  ...              ...                   ...   \n",
       "18149           1.914927    423885.759580             14.073752   \n",
       "18150           0.000142    108617.718000              7.717160   \n",
       "18151           0.001109     56354.893000              5.203467   \n",
       "18152           3.179559     96715.582390             16.164941   \n",
       "18153           0.006868     78849.924855             20.125139   \n",
       "\n",
       "       High Intensity Developed_acres  High Intensity Developed_percentage  \\\n",
       "0                        28431.421590                             7.735834   \n",
       "1                          121.427670                             0.025746   \n",
       "2                          440.119705                             0.137425   \n",
       "3                         4421.657390                             0.438828   \n",
       "4                          465.027945                             0.073268   \n",
       "...                               ...                                  ...   \n",
       "18149                     8425.434575                             0.279739   \n",
       "18150                      531.746445                             0.037780   \n",
       "18151                    13949.281585                             1.287992   \n",
       "18152                     5373.952780                             0.898197   \n",
       "18153                     1853.217535                             0.473003   \n",
       "\n",
       "       Low Intensity Developed_acres  Low Intensity Developed_percentage  \\\n",
       "0                       39470.886995                           10.739534   \n",
       "1                        3192.480225                            0.676881   \n",
       "2                        8263.975805                            2.580391   \n",
       "3                       25520.048645                            2.532743   \n",
       "4                       12257.745215                            1.931277   \n",
       "...                              ...                                 ...   \n",
       "18149                   43858.740345                            1.456187   \n",
       "18150                   12267.975385                            0.871625   \n",
       "18151                   63969.697800                            5.906572   \n",
       "18152                   14182.351545                            2.370423   \n",
       "18153                   16694.303070                            4.260945   \n",
       "\n",
       "       Mixed Forest_acres  Mixed Forest_percentage  Other Ocean/Mexico_acres  \\\n",
       "0            74885.956375                20.375531                       0.0   \n",
       "1                0.667185                 0.000141                       0.0   \n",
       "2             1727.119570                 0.539286                       0.0   \n",
       "3              165.684275                 0.016443                       0.0   \n",
       "4             3351.270255                 0.528012                       0.0   \n",
       "...                   ...                      ...                       ...   \n",
       "18149          459.245675                 0.015248                       0.0   \n",
       "18150          485.933075                 0.034525                       0.0   \n",
       "18151        68343.317870                 6.310405                       0.0   \n",
       "18152        11843.423330                 1.979497                       0.0   \n",
       "18153          958.967240                 0.244761                       0.0   \n",
       "\n",
       "       Other Ocean/Mexico_percentage  Other Tree Crops_acres  \\\n",
       "0                                0.0                8.673405   \n",
       "1                                0.0                0.000000   \n",
       "2                                0.0                1.334370   \n",
       "3                                0.0              790.391830   \n",
       "4                                0.0                0.889580   \n",
       "...                              ...                     ...   \n",
       "18149                            0.0            98145.804635   \n",
       "18150                            0.0                1.111975   \n",
       "18151                            0.0               88.735605   \n",
       "18152                            0.0             3236.292040   \n",
       "18153                            0.0             1653.729220   \n",
       "\n",
       "       Other Tree Crops_percentage  Other_acres  Other_percentage  \\\n",
       "0                         0.002360          0.0               0.0   \n",
       "1                         0.000000          0.0               0.0   \n",
       "2                         0.000417          0.0               0.0   \n",
       "3                         0.078443          0.0               0.0   \n",
       "4                         0.000140          0.0               0.0   \n",
       "...                            ...          ...               ...   \n",
       "18149                     3.258613          0.0               0.0   \n",
       "18150                     0.000079          0.0               0.0   \n",
       "18151                     0.008193          0.0               0.0   \n",
       "18152                     0.540910          0.0               0.0   \n",
       "18153                     0.422087          0.0               0.0   \n",
       "\n",
       "          Rice_acres  Rice_percentage  Shrubland_acres  Shrubland_percentage  \\\n",
       "0           0.889580         0.000242     30958.051185              8.423298   \n",
       "1           0.000000         0.000000    247783.390805             52.535935   \n",
       "2           1.111975         0.000347     52457.865415             16.379744   \n",
       "3      105624.281300        10.482705     55372.129495              5.495419   \n",
       "4           0.000000         0.000000    106091.088405             16.715249   \n",
       "...              ...              ...              ...                   ...   \n",
       "18149       0.889580         0.000030    704249.128305             23.382308   \n",
       "18150       0.000000         0.000000    428594.528915             30.451133   \n",
       "18151       0.000000         0.000000    648960.841725             59.921086   \n",
       "18152   38908.005250         6.503043     87093.217925             14.556669   \n",
       "18153   38336.894890         9.784858     14227.942520              3.631447   \n",
       "\n",
       "       Tomatoes_acres  Tomatoes_percentage  Vegs & Fruits_acres  \\\n",
       "0            4.670295             0.001271           164.127510   \n",
       "1            0.000000             0.000000             0.000000   \n",
       "2            0.000000             0.000000             1.779160   \n",
       "3           94.295480             0.009358           469.475845   \n",
       "4            0.000000             0.000000             0.667185   \n",
       "...               ...                  ...                  ...   \n",
       "18149      524.185015             0.017404          4647.833105   \n",
       "18150        0.000000             0.000000             4.447900   \n",
       "18151        9.785380             0.000904          4056.929590   \n",
       "18152    29441.762075             4.920865         13333.025040   \n",
       "18153      145.446330             0.037123           552.873970   \n",
       "\n",
       "       Vegs & Fruits_percentage  Walnuts_acres  Walnuts_percentage  \\\n",
       "0                      0.044657       4.670295            0.001271   \n",
       "1                      0.000000       0.000000            0.000000   \n",
       "2                      0.000556     122.094855            0.038124   \n",
       "3                      0.046593   42057.340845            4.173990   \n",
       "4                      0.000105     425.664030            0.067066   \n",
       "...                         ...            ...                 ...   \n",
       "18149                  0.154316   40724.750005            1.352133   \n",
       "18150                  0.000316       0.444790            0.000032   \n",
       "18151                  0.374592      81.618965            0.007536   \n",
       "18152                  2.228468   18932.708745            3.164393   \n",
       "18153                  0.141112   16699.418155            4.262250   \n",
       "\n",
       "        Water_acres  Water_percentage  Wetlands_acres  Wetlands_percentage  \\\n",
       "0      19403.518960          5.279454     4497.494085             1.223712   \n",
       "1       2650.503610          0.561969     1297.452430             0.275091   \n",
       "2       5822.745890          1.818128      105.860020             0.033054   \n",
       "3      21360.372565          2.119915    11589.893030             1.150241   \n",
       "4      13178.682910          2.076376      245.079290             0.038614   \n",
       "...             ...               ...             ...                  ...   \n",
       "18149   8273.538790          0.274696     3639.494175             0.120838   \n",
       "18150  30500.807065          2.167046     2276.435220             0.161738   \n",
       "18151   5719.332215          0.528088     8922.042610             0.823807   \n",
       "18152   3213.607750          0.537119     4433.666720             0.741038   \n",
       "18153   7790.941640          1.988509     1868.562790             0.476920   \n",
       "\n",
       "       Winter Wheat_acres  Winter Wheat_percentage  max_elevation  \\\n",
       "0              624.485160                 0.169915           1242   \n",
       "1                0.000000                 0.000000           3556   \n",
       "2              479.483620                 0.149717           3121   \n",
       "3             4257.085090                 0.422495           2192   \n",
       "4               11.786935                 0.001857           3522   \n",
       "...                   ...                      ...            ...   \n",
       "18149        20034.231180                 0.665172           4409   \n",
       "18150            1.779160                 0.000126           3959   \n",
       "18151          181.919110                 0.016797           2332   \n",
       "18152        18707.645005                 3.126776            346   \n",
       "18153          163.015535                 0.041607           2541   \n",
       "\n",
       "       min_elevation  Avg Air Temp (F)_Weekly  Avg Rel Hum (%)_Weekly  \\\n",
       "0                -42                44.214286               82.785714   \n",
       "1               1442                29.657143               76.514286   \n",
       "2                 43                34.114286               83.571429   \n",
       "3                 -1                40.985714               81.285714   \n",
       "4                787                41.928571               93.014286   \n",
       "...              ...                      ...                     ...   \n",
       "18149            149                48.457143               90.384615   \n",
       "18150            374                50.442857               83.700000   \n",
       "18151             34                54.692857               79.571429   \n",
       "18152            -26                50.828571               89.571429   \n",
       "18153            199                50.600000               84.285714   \n",
       "\n",
       "       Avg Wind Speed (mph)_Weekly  Dew Point (F)_Weekly  \\\n",
       "0                         2.392857             39.321429   \n",
       "1                         3.228571             21.328571   \n",
       "2                         3.157143             29.585714   \n",
       "3                         3.142857             35.557143   \n",
       "4                         5.657143             39.000000   \n",
       "...                            ...                   ...   \n",
       "18149                     2.100000             45.915385   \n",
       "18150                     3.471429             45.800000   \n",
       "18151                     2.907143             48.357143   \n",
       "18152                     4.000000             47.800000   \n",
       "18153                     3.257143             45.900000   \n",
       "\n",
       "       Max Air Temp (F)_Weekly  Max Rel Hum (%)_Weekly  \\\n",
       "0                    54.157143               96.500000   \n",
       "1                    34.428571               91.857143   \n",
       "2                    40.071429               96.000000   \n",
       "3                    50.114286               91.285714   \n",
       "4                    50.142857              100.000000   \n",
       "...                        ...                     ...   \n",
       "18149                55.550000               98.285714   \n",
       "18150                58.714286               93.142857   \n",
       "18151                68.500000               97.500000   \n",
       "18152                57.928571               98.428571   \n",
       "18153                57.414286               95.142857   \n",
       "\n",
       "       Min Air Temp (F)_Weekly  Min Rel Hum (%)_Weekly  Precip (in)_Weekly  \\\n",
       "0                    35.771429               60.785714            0.095714   \n",
       "1                    22.857143               55.428571            0.000000   \n",
       "2                    27.757143               66.571429            0.141429   \n",
       "3                    32.171429               62.857143            0.117143   \n",
       "4                    35.571429               74.142857            0.000000   \n",
       "...                        ...                     ...                 ...   \n",
       "18149                42.450000               73.500000            0.009286   \n",
       "18150                26.142857               36.714286            0.161429   \n",
       "18151                44.800000               53.714286            0.012857   \n",
       "18152                43.585714               72.428571            0.035714   \n",
       "18153                44.071429               68.857143            0.141429   \n",
       "\n",
       "       Avg Air Temp (F)_month  Avg Rel Hum (%)_month  \\\n",
       "0                   45.506897              78.189655   \n",
       "1                   30.789655              68.162069   \n",
       "2                   34.289655              76.724138   \n",
       "3                   42.389655              77.448276   \n",
       "4                   42.931034              87.017241   \n",
       "...                       ...                    ...   \n",
       "18149               46.139655              84.741379   \n",
       "18150               48.479310              80.844828   \n",
       "18151               54.527586              62.913793   \n",
       "18152               48.689655              81.620690   \n",
       "18153               48.844828              73.379310   \n",
       "\n",
       "       Avg Wind Speed (mph)_month  Dew Point (F)_month  \\\n",
       "0                        2.915517            38.932759   \n",
       "1                        4.968966            19.600000   \n",
       "2                        3.606897            27.410345   \n",
       "3                        3.848276            35.586207   \n",
       "4                        6.268966            37.196552   \n",
       "...                           ...                  ...   \n",
       "18149                    2.160345            41.820690   \n",
       "18150                    4.944828            42.372414   \n",
       "18151                    3.968966            41.179310   \n",
       "18152                    4.986207            42.989655   \n",
       "18153                    3.717241            40.279310   \n",
       "\n",
       "       Max Air Temp (F)_month  Max Rel Hum (%)_month  Min Air Temp (F)_month  \\\n",
       "0                   55.896552              95.448276               35.725862   \n",
       "1                   39.344828              86.000000               22.758621   \n",
       "2                   41.200000              93.172414               27.768966   \n",
       "3                   52.455172              88.965517               33.365517   \n",
       "4                   52.827586              97.551724               34.344828   \n",
       "...                       ...                    ...                     ...   \n",
       "18149               55.243103              97.758621               38.431034   \n",
       "18150               58.448276              93.379310               36.896552   \n",
       "18151               67.987931              89.500000               43.296552   \n",
       "18152               57.137931              95.379310               40.768966   \n",
       "18153               57.558621              89.655172               40.820690   \n",
       "\n",
       "       Min Rel Hum (%)_month  Precip (in)_month  Population  county_acres  \\\n",
       "0                  55.810345           0.130172     1567167        528000   \n",
       "1                  46.344828           0.000000        1164        465280   \n",
       "2                  58.310345           0.155517       37304        384640   \n",
       "3                  58.862069           0.175517      222185       1065600   \n",
       "4                  61.275862           0.000000       45424        663040   \n",
       "...                      ...                ...         ...           ...   \n",
       "18149              64.431034           0.016207      475834       3100160   \n",
       "18150              51.689655           0.081034       54740       1467520   \n",
       "18151              37.379310           0.032586      859073       1192960   \n",
       "18152              63.758621           0.061034      221270        661760   \n",
       "18153              55.586207           0.132759       74727        408960   \n",
       "\n",
       "       pop_density  \n",
       "0         2.968119  \n",
       "1         0.002502  \n",
       "2         0.096984  \n",
       "3         0.208507  \n",
       "4         0.068509  \n",
       "...            ...  \n",
       "18149     0.153487  \n",
       "18150     0.037301  \n",
       "18151     0.720119  \n",
       "18152     0.334366  \n",
       "18153     0.182724  \n",
       "\n",
       "[18154 rows x 82 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dummy Variables for Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two features that are categorical. The counties and the month of the year column that we engineered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the county column\n",
    "counties = pd.get_dummies(df.county, drop_first = True)\n",
    "# Drop county column along with unnecessary columns (Unnamed columns, year, and acres burned)\n",
    "df2 = df.drop(columns = ['Unnamed: 0', 'Unnamed: 0.1', 'county', 'year', 'acres_burned'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineer month column from the date column\n",
    "df2['month'] = pd.DatetimeIndex(df2['date']).month\n",
    "# Drop the date column\n",
    "df2.drop(columns = ['date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for the months\n",
    "month = pd.get_dummies(df2.month, drop_first = True)\n",
    "# Drop the month column\n",
    "df2.drop(columns = 'month', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the original dataframe with the dummy variables\n",
    "df2 = pd.concat([df2, counties, month], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to split our data into a training dataset and a test dataset. We want to do this before we work on the class imbalance so that we have a holdout set of data to test the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into target variable and features\n",
    "y = df2.fire_started\n",
    "X = df2.drop(columns = ['fire_started'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create a training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,  y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling Minority Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high class imbalance in the target variable. As seen in the EDA, the grand majority of the target variable are instances of no wildfire. In this case, we use upsampling the instances of wildfire to resolve the class imbalance issue. The first step to combine the `X_train` and `y_train` dataframes back into one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the X_train and y_train back into one training dataframe\n",
    "training = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to split the dataframe into the majority class and the minority class. In this case, the majority class are observations of weeks with no wildfire incidents. The minority class are observations of weeks with one or more wildfire incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the target variable by class into two dataframes\n",
    "no_fire = training[training.fire_started == 0] # 0 = No Wildfire\n",
    "fire = training[training.fire_started == 1] # 1 = Wildfire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to upsample the minority class with replacement. The minority class should be equal to the same number of observations and then recombine the two class dataframes back into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the minority class (wildfire)\n",
    "fire_upsampled = resample(fire,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=no_fire.shape[0], # match number in majority class\n",
    "                          random_state=42) # reproducible result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe combining the target classes\n",
    "resampled_df = pd.concat([no_fire, fire_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    12812\n",
       "0.0    12812\n",
       "Name: fire_started, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double-check the different target classes\n",
    "resampled_df.fire_started.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no longer any class imbalance between incidents of wildfire and incidents with no wildfires. We split the dataframe back into training target variable and the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = resampled_df.fire_started\n",
    "X_train = resampled_df.drop(columns = ['fire_started'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of model we tried was logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our base model, we run the resampled training data through a logistic regression model with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malcolmkatzenbach/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "The results of our metrics show that the recall score is higher than the accuracy score, and we have a low f1 score. We use a confusion matrix to find the false positive and false negative values.# Instantiate a logistic regression model\n",
    "logreg = LogisticRegression(random_state = 0) # random state for consistant results\n",
    "# Train model on resampled training data\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the target variable on the training dataset\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "# Use the model to predict the target variable on the test dataset\n",
    "y_hat_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.6259625586091242 Test 0.1420159334949775\n",
      "Accuracy Score: Training 0.5734857945675929 Test 0.45428508482044505\n",
      "Recall Score: Training 0.7137839525444896 Test 0.7068965517241379\n"
     ]
    }
   ],
   "source": [
    "# Print the f1 score metric on both the training and test predictions to check for overfitting\n",
    "print('F1 Score: Training', f1_score(y_train, y_hat_train), 'Test', f1_score(y_test, y_hat_test))\n",
    "# Print the accuracy score metric on both the training and test predictions to check for overfitting\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, y_hat_train), 'Test', accuracy_score(y_test, y_hat_test))\n",
    "# Print the recall score metric on both the training and test predictions to check for overfitting\n",
    "print('Recall Score: Training', recall_score(y_train, y_hat_train), 'Test', recall_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our metrics show that the recall score is higher than the accuracy score, and we have a low f1 score. We use a confusion matrix to find the false positive and false negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1857, 2392],\n",
       "       [  85,  205]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, the majority of predictions are false positives. We want to try to decrease the number of false positives in our next model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Scaled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first iteration we want to check how normalizing the features will change our score. To normalize our data, we will use a Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insantiate the StandardScaler()\n",
    "ss = StandardScaler()\n",
    "# Fit the feature training data\n",
    "ss.fit(X_train)\n",
    "\n",
    "# Transform both the training and test features\n",
    "X_train_scaled = ss.transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Standar Scaler\n",
    "pickle_out = open(\"../Models/ss.pickle\",\"wb\")\n",
    "pickle.dump(ss, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new logistic regression model\n",
    "logreg1 = LogisticRegression(solver = 'liblinear')\n",
    "# Fit the data to the new scaled data\n",
    "logreg1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model to predict target variable on the training dataset\n",
    "y_hat_train1 = logreg1.predict(X_train_scaled)\n",
    "# Use model to predict target variable on the test dataset\n",
    "y_hat_test1 = logreg1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.8157728940443985 Test 0.28536436007348437\n",
      "Accuracy Score: Training 0.8060021854511396 Test 0.7428949107732981\n",
      "Recall Score: Training 0.8590384014985951 Test 0.803448275862069\n"
     ]
    }
   ],
   "source": [
    "# Print the f1 score metric on both the training and test predictions to check for overfitting\n",
    "print('F1 Score: Training', f1_score(y_train, y_hat_train1), 'Test', f1_score(y_test, y_hat_test1))\n",
    "# Print the accuracy score metric on both the training and test predictions to check for overfitting\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, y_hat_train1), 'Test', accuracy_score(y_test, y_hat_test1))\n",
    "# Print the recall score metric on both the training and test predictions to check for overfitting\n",
    "print('Recall Score: Training', recall_score(y_train, y_hat_train1), 'Test', recall_score(y_test, y_hat_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from our metrics show an increase in score across the board. However, the recall and accuracy score are still much better than the f1 score. The f1 score is highly overfit, while the recall and accuracy scores are slightly overfit. We check the confusion matrix next to check the value counts for false positive and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3139, 1110],\n",
       "       [  57,  233]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows as in the previous model, the most error occurs from the model predicting false positives, however we have decrease the number of false positives by more than half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8017092607432437\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(logreg1, X_train_scaled, y_train, cv=10)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle logistic model\n",
    "with open(\"../models/best_logistic.pickle\", \"wb\") as best_logistic:\n",
    "    pickle.dump(logreg1, best_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alfalfa &amp; Hay_acres</th>\n",
       "      <th>Alfalfa &amp; Hay_percentage</th>\n",
       "      <th>Almonds_acres</th>\n",
       "      <th>Almonds_percentage</th>\n",
       "      <th>Barren_acres</th>\n",
       "      <th>Barren_percentage</th>\n",
       "      <th>Corn_acres</th>\n",
       "      <th>Corn_percentage</th>\n",
       "      <th>Cotton_acres</th>\n",
       "      <th>Cotton_percentage</th>\n",
       "      <th>Deciduous Forest_acres</th>\n",
       "      <th>Deciduous Forest_percentage</th>\n",
       "      <th>Evergreen Forest_acres</th>\n",
       "      <th>Evergreen Forest_percentage</th>\n",
       "      <th>Fallow_acres</th>\n",
       "      <th>Fallow_percentage</th>\n",
       "      <th>Fruit Trees_acres</th>\n",
       "      <th>Fruit Trees_percentage</th>\n",
       "      <th>Grain Crops_acres</th>\n",
       "      <th>Grain Crops_percentage</th>\n",
       "      <th>Grapes_acres</th>\n",
       "      <th>Grapes_percentage</th>\n",
       "      <th>Grassland_acres</th>\n",
       "      <th>Grassland_percentage</th>\n",
       "      <th>High Intensity Developed_acres</th>\n",
       "      <th>High Intensity Developed_percentage</th>\n",
       "      <th>Low Intensity Developed_acres</th>\n",
       "      <th>Low Intensity Developed_percentage</th>\n",
       "      <th>Mixed Forest_acres</th>\n",
       "      <th>Mixed Forest_percentage</th>\n",
       "      <th>Other Ocean/Mexico_acres</th>\n",
       "      <th>Other Ocean/Mexico_percentage</th>\n",
       "      <th>Other Tree Crops_acres</th>\n",
       "      <th>Other Tree Crops_percentage</th>\n",
       "      <th>Other_acres</th>\n",
       "      <th>Other_percentage</th>\n",
       "      <th>Rice_acres</th>\n",
       "      <th>Rice_percentage</th>\n",
       "      <th>Shrubland_acres</th>\n",
       "      <th>Shrubland_percentage</th>\n",
       "      <th>Tomatoes_acres</th>\n",
       "      <th>Tomatoes_percentage</th>\n",
       "      <th>Vegs &amp; Fruits_acres</th>\n",
       "      <th>Vegs &amp; Fruits_percentage</th>\n",
       "      <th>Walnuts_acres</th>\n",
       "      <th>Walnuts_percentage</th>\n",
       "      <th>Water_acres</th>\n",
       "      <th>Water_percentage</th>\n",
       "      <th>Wetlands_acres</th>\n",
       "      <th>Wetlands_percentage</th>\n",
       "      <th>Winter Wheat_acres</th>\n",
       "      <th>Winter Wheat_percentage</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>min_elevation</th>\n",
       "      <th>Avg Air Temp (F)_Weekly</th>\n",
       "      <th>Avg Rel Hum (%)_Weekly</th>\n",
       "      <th>Avg Wind Speed (mph)_Weekly</th>\n",
       "      <th>Dew Point (F)_Weekly</th>\n",
       "      <th>Max Air Temp (F)_Weekly</th>\n",
       "      <th>Max Rel Hum (%)_Weekly</th>\n",
       "      <th>Min Air Temp (F)_Weekly</th>\n",
       "      <th>Min Rel Hum (%)_Weekly</th>\n",
       "      <th>Precip (in)_Weekly</th>\n",
       "      <th>Avg Air Temp (F)_month</th>\n",
       "      <th>Avg Rel Hum (%)_month</th>\n",
       "      <th>Avg Wind Speed (mph)_month</th>\n",
       "      <th>Dew Point (F)_month</th>\n",
       "      <th>Max Air Temp (F)_month</th>\n",
       "      <th>Max Rel Hum (%)_month</th>\n",
       "      <th>Min Air Temp (F)_month</th>\n",
       "      <th>Min Rel Hum (%)_month</th>\n",
       "      <th>Precip (in)_month</th>\n",
       "      <th>Population</th>\n",
       "      <th>county_acres</th>\n",
       "      <th>pop_density</th>\n",
       "      <th>Alpine</th>\n",
       "      <th>Amador</th>\n",
       "      <th>Butte</th>\n",
       "      <th>Calaveras</th>\n",
       "      <th>Colusa</th>\n",
       "      <th>Contra Costa</th>\n",
       "      <th>Del Norte</th>\n",
       "      <th>El Dorado</th>\n",
       "      <th>Fresno</th>\n",
       "      <th>Glenn</th>\n",
       "      <th>Humboldt</th>\n",
       "      <th>Imperial</th>\n",
       "      <th>Inyo</th>\n",
       "      <th>Kern</th>\n",
       "      <th>Kings</th>\n",
       "      <th>Lake</th>\n",
       "      <th>Lassen</th>\n",
       "      <th>Los Angeles</th>\n",
       "      <th>Madera</th>\n",
       "      <th>Marin</th>\n",
       "      <th>Mariposa</th>\n",
       "      <th>Mendocino</th>\n",
       "      <th>Merced</th>\n",
       "      <th>Modoc</th>\n",
       "      <th>Mono</th>\n",
       "      <th>Monterey</th>\n",
       "      <th>Napa</th>\n",
       "      <th>Nevada</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Placer</th>\n",
       "      <th>Plumas</th>\n",
       "      <th>Riverside</th>\n",
       "      <th>Sacramento</th>\n",
       "      <th>San Benito</th>\n",
       "      <th>San Bernardino</th>\n",
       "      <th>San Diego</th>\n",
       "      <th>San Francisco</th>\n",
       "      <th>San Joaquin</th>\n",
       "      <th>San Luis Obispo</th>\n",
       "      <th>San Mateo</th>\n",
       "      <th>Santa Barbara</th>\n",
       "      <th>Santa Clara</th>\n",
       "      <th>Santa Cruz</th>\n",
       "      <th>Shasta</th>\n",
       "      <th>Sierra</th>\n",
       "      <th>Siskiyou</th>\n",
       "      <th>Solano</th>\n",
       "      <th>Sonoma</th>\n",
       "      <th>Stanislaus</th>\n",
       "      <th>Sutter</th>\n",
       "      <th>Tehama</th>\n",
       "      <th>Trinity</th>\n",
       "      <th>Tulare</th>\n",
       "      <th>Tuolumne</th>\n",
       "      <th>Ventura</th>\n",
       "      <th>Yolo</th>\n",
       "      <th>Yuba</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>fire_started</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12930</th>\n",
       "      <td>17.791600</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>3.780715</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>120475.152215</td>\n",
       "      <td>8.559617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17857.428920</td>\n",
       "      <td>1.268749</td>\n",
       "      <td>673694.279255</td>\n",
       "      <td>47.865180</td>\n",
       "      <td>9.562985</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>1.779160</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>1.556765</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.889580</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>107346.063390</td>\n",
       "      <td>7.626811</td>\n",
       "      <td>556.432290</td>\n",
       "      <td>0.039534</td>\n",
       "      <td>11897.687710</td>\n",
       "      <td>0.845317</td>\n",
       "      <td>265.094840</td>\n",
       "      <td>0.018835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.556765</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.451974e+05</td>\n",
       "      <td>31.630749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222395</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>29309.214655</td>\n",
       "      <td>2.082385</td>\n",
       "      <td>816.189650</td>\n",
       "      <td>0.057989</td>\n",
       "      <td>30.912905</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>3959</td>\n",
       "      <td>374</td>\n",
       "      <td>60.528571</td>\n",
       "      <td>60.714286</td>\n",
       "      <td>11.685714</td>\n",
       "      <td>44.942857</td>\n",
       "      <td>73.714286</td>\n",
       "      <td>88.285714</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>59.082759</td>\n",
       "      <td>70.362069</td>\n",
       "      <td>7.817241</td>\n",
       "      <td>47.824138</td>\n",
       "      <td>70.793103</td>\n",
       "      <td>93.379310</td>\n",
       "      <td>48.689655</td>\n",
       "      <td>41.827586</td>\n",
       "      <td>0.098276</td>\n",
       "      <td>54725</td>\n",
       "      <td>1467520</td>\n",
       "      <td>0.037291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17287</th>\n",
       "      <td>5199.595100</td>\n",
       "      <td>0.516035</td>\n",
       "      <td>36487.680465</td>\n",
       "      <td>3.621228</td>\n",
       "      <td>2509.060390</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>348.270570</td>\n",
       "      <td>0.034564</td>\n",
       "      <td>221.505420</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>36955.154755</td>\n",
       "      <td>3.667622</td>\n",
       "      <td>389571.545450</td>\n",
       "      <td>38.663112</td>\n",
       "      <td>82134.254215</td>\n",
       "      <td>8.151432</td>\n",
       "      <td>19776.252980</td>\n",
       "      <td>1.962698</td>\n",
       "      <td>919.380930</td>\n",
       "      <td>0.091244</td>\n",
       "      <td>42.477445</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>148690.405865</td>\n",
       "      <td>14.756811</td>\n",
       "      <td>5127.316725</td>\n",
       "      <td>0.508862</td>\n",
       "      <td>28198.796420</td>\n",
       "      <td>2.798596</td>\n",
       "      <td>583.786875</td>\n",
       "      <td>0.057938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2598.240785</td>\n",
       "      <td>0.257863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79618.521975</td>\n",
       "      <td>7.901757</td>\n",
       "      <td>8.180177e+04</td>\n",
       "      <td>8.118435</td>\n",
       "      <td>777.715315</td>\n",
       "      <td>0.077185</td>\n",
       "      <td>516.401190</td>\n",
       "      <td>0.051250</td>\n",
       "      <td>51968.374020</td>\n",
       "      <td>5.157612</td>\n",
       "      <td>21834.741100</td>\n",
       "      <td>2.166994</td>\n",
       "      <td>11160.893075</td>\n",
       "      <td>1.107665</td>\n",
       "      <td>563.104140</td>\n",
       "      <td>0.055885</td>\n",
       "      <td>2192</td>\n",
       "      <td>-1</td>\n",
       "      <td>64.800000</td>\n",
       "      <td>61.142857</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>51.114286</td>\n",
       "      <td>81.900000</td>\n",
       "      <td>90.714286</td>\n",
       "      <td>50.757143</td>\n",
       "      <td>31.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.113793</td>\n",
       "      <td>61.172414</td>\n",
       "      <td>2.162069</td>\n",
       "      <td>54.089655</td>\n",
       "      <td>86.786207</td>\n",
       "      <td>92.413793</td>\n",
       "      <td>52.775862</td>\n",
       "      <td>32.620690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>227621</td>\n",
       "      <td>1065600</td>\n",
       "      <td>0.213608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11545</th>\n",
       "      <td>3504.945200</td>\n",
       "      <td>0.347849</td>\n",
       "      <td>43919.676575</td>\n",
       "      <td>4.358818</td>\n",
       "      <td>2064.937575</td>\n",
       "      <td>0.204935</td>\n",
       "      <td>270.209925</td>\n",
       "      <td>0.026817</td>\n",
       "      <td>81.396570</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>36126.733380</td>\n",
       "      <td>3.585405</td>\n",
       "      <td>391415.422395</td>\n",
       "      <td>38.846108</td>\n",
       "      <td>63940.786450</td>\n",
       "      <td>6.345817</td>\n",
       "      <td>8684.302355</td>\n",
       "      <td>0.861875</td>\n",
       "      <td>568.886410</td>\n",
       "      <td>0.056459</td>\n",
       "      <td>393.416755</td>\n",
       "      <td>0.039045</td>\n",
       "      <td>148383.278370</td>\n",
       "      <td>14.726330</td>\n",
       "      <td>4998.105230</td>\n",
       "      <td>0.496038</td>\n",
       "      <td>27114.398400</td>\n",
       "      <td>2.690974</td>\n",
       "      <td>808.850615</td>\n",
       "      <td>0.080275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>968.085435</td>\n",
       "      <td>0.096078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103580.471250</td>\n",
       "      <td>10.279866</td>\n",
       "      <td>8.430928e+04</td>\n",
       "      <td>8.367292</td>\n",
       "      <td>838.873940</td>\n",
       "      <td>0.083254</td>\n",
       "      <td>1137.772820</td>\n",
       "      <td>0.112919</td>\n",
       "      <td>49604.092775</td>\n",
       "      <td>4.922969</td>\n",
       "      <td>22076.706860</td>\n",
       "      <td>2.191008</td>\n",
       "      <td>11046.582045</td>\n",
       "      <td>1.096320</td>\n",
       "      <td>1768.040250</td>\n",
       "      <td>0.175470</td>\n",
       "      <td>2192</td>\n",
       "      <td>-1</td>\n",
       "      <td>58.814286</td>\n",
       "      <td>69.285714</td>\n",
       "      <td>2.542857</td>\n",
       "      <td>48.728571</td>\n",
       "      <td>74.457143</td>\n",
       "      <td>95.285714</td>\n",
       "      <td>46.557143</td>\n",
       "      <td>39.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.148276</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>3.082759</td>\n",
       "      <td>50.639286</td>\n",
       "      <td>73.182759</td>\n",
       "      <td>94.285714</td>\n",
       "      <td>49.851724</td>\n",
       "      <td>48.785714</td>\n",
       "      <td>0.135172</td>\n",
       "      <td>224432</td>\n",
       "      <td>1065600</td>\n",
       "      <td>0.210616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>64668.685285</td>\n",
       "      <td>7.824309</td>\n",
       "      <td>150447.103970</td>\n",
       "      <td>18.202699</td>\n",
       "      <td>3492.935870</td>\n",
       "      <td>0.422613</td>\n",
       "      <td>3602.576605</td>\n",
       "      <td>0.435878</td>\n",
       "      <td>365.172590</td>\n",
       "      <td>0.044182</td>\n",
       "      <td>239.519415</td>\n",
       "      <td>0.028980</td>\n",
       "      <td>350.272125</td>\n",
       "      <td>0.042380</td>\n",
       "      <td>6562.654055</td>\n",
       "      <td>0.794020</td>\n",
       "      <td>9814.513745</td>\n",
       "      <td>1.187465</td>\n",
       "      <td>36891.549785</td>\n",
       "      <td>4.463534</td>\n",
       "      <td>8749.241695</td>\n",
       "      <td>1.058577</td>\n",
       "      <td>263669.065655</td>\n",
       "      <td>31.901502</td>\n",
       "      <td>10116.970945</td>\n",
       "      <td>1.224059</td>\n",
       "      <td>32563.520690</td>\n",
       "      <td>3.939883</td>\n",
       "      <td>43372.362480</td>\n",
       "      <td>5.247652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2757.698000</td>\n",
       "      <td>0.333656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.836650</td>\n",
       "      <td>0.061081</td>\n",
       "      <td>1.153868e+05</td>\n",
       "      <td>13.960723</td>\n",
       "      <td>6909.590255</td>\n",
       "      <td>0.835996</td>\n",
       "      <td>6355.604310</td>\n",
       "      <td>0.768969</td>\n",
       "      <td>37214.912115</td>\n",
       "      <td>4.502658</td>\n",
       "      <td>4093.847160</td>\n",
       "      <td>0.495317</td>\n",
       "      <td>10072.491945</td>\n",
       "      <td>1.218678</td>\n",
       "      <td>8308.010015</td>\n",
       "      <td>1.005192</td>\n",
       "      <td>1026</td>\n",
       "      <td>-2</td>\n",
       "      <td>77.078571</td>\n",
       "      <td>45.384615</td>\n",
       "      <td>5.057143</td>\n",
       "      <td>53.669231</td>\n",
       "      <td>96.321429</td>\n",
       "      <td>81.714286</td>\n",
       "      <td>57.857143</td>\n",
       "      <td>23.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.734483</td>\n",
       "      <td>46.827586</td>\n",
       "      <td>5.229310</td>\n",
       "      <td>55.067241</td>\n",
       "      <td>96.663793</td>\n",
       "      <td>81.155172</td>\n",
       "      <td>59.132759</td>\n",
       "      <td>25.379310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>549976</td>\n",
       "      <td>973440</td>\n",
       "      <td>0.564982</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>85001.815345</td>\n",
       "      <td>12.412499</td>\n",
       "      <td>52873.966460</td>\n",
       "      <td>7.720989</td>\n",
       "      <td>901.811725</td>\n",
       "      <td>0.131688</td>\n",
       "      <td>48174.315320</td>\n",
       "      <td>7.034716</td>\n",
       "      <td>64.494550</td>\n",
       "      <td>0.009418</td>\n",
       "      <td>28.021770</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>29.578535</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>13848.091860</td>\n",
       "      <td>2.022185</td>\n",
       "      <td>9251.409605</td>\n",
       "      <td>1.350949</td>\n",
       "      <td>40089.589885</td>\n",
       "      <td>5.854134</td>\n",
       "      <td>81528.895025</td>\n",
       "      <td>11.905361</td>\n",
       "      <td>129100.075105</td>\n",
       "      <td>18.852004</td>\n",
       "      <td>20984.747410</td>\n",
       "      <td>3.064325</td>\n",
       "      <td>28916.909875</td>\n",
       "      <td>4.222629</td>\n",
       "      <td>6168.570115</td>\n",
       "      <td>0.900773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.794710</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2931.833285</td>\n",
       "      <td>0.428125</td>\n",
       "      <td>1.645545e+04</td>\n",
       "      <td>2.402928</td>\n",
       "      <td>46361.128885</td>\n",
       "      <td>6.769943</td>\n",
       "      <td>12267.308200</td>\n",
       "      <td>1.791349</td>\n",
       "      <td>51725.741075</td>\n",
       "      <td>7.553317</td>\n",
       "      <td>7040.358515</td>\n",
       "      <td>1.028077</td>\n",
       "      <td>7132.874835</td>\n",
       "      <td>1.041587</td>\n",
       "      <td>23909.464055</td>\n",
       "      <td>3.491410</td>\n",
       "      <td>1889</td>\n",
       "      <td>6</td>\n",
       "      <td>49.300000</td>\n",
       "      <td>88.285714</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>45.928571</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>59.571429</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>53.672414</td>\n",
       "      <td>86.758621</td>\n",
       "      <td>3.168966</td>\n",
       "      <td>49.686207</td>\n",
       "      <td>65.420690</td>\n",
       "      <td>99.551724</td>\n",
       "      <td>43.868966</td>\n",
       "      <td>62.862069</td>\n",
       "      <td>0.083793</td>\n",
       "      <td>712134</td>\n",
       "      <td>919040</td>\n",
       "      <td>0.774867</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>117482.160305</td>\n",
       "      <td>9.732298</td>\n",
       "      <td>129201.042435</td>\n",
       "      <td>10.703097</td>\n",
       "      <td>6547.531195</td>\n",
       "      <td>0.542402</td>\n",
       "      <td>33730.649650</td>\n",
       "      <td>2.794269</td>\n",
       "      <td>39887.210435</td>\n",
       "      <td>3.304282</td>\n",
       "      <td>201.489870</td>\n",
       "      <td>0.016692</td>\n",
       "      <td>320.915985</td>\n",
       "      <td>0.026585</td>\n",
       "      <td>30481.013910</td>\n",
       "      <td>2.525067</td>\n",
       "      <td>2628.041715</td>\n",
       "      <td>0.217709</td>\n",
       "      <td>65542.252845</td>\n",
       "      <td>5.429562</td>\n",
       "      <td>15494.704440</td>\n",
       "      <td>1.283591</td>\n",
       "      <td>462364.542480</td>\n",
       "      <td>38.302576</td>\n",
       "      <td>4733.010390</td>\n",
       "      <td>0.392086</td>\n",
       "      <td>42937.580255</td>\n",
       "      <td>3.556977</td>\n",
       "      <td>18330.463085</td>\n",
       "      <td>1.518507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4119.200190</td>\n",
       "      <td>0.341237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2610.472510</td>\n",
       "      <td>0.216253</td>\n",
       "      <td>3.897472e+04</td>\n",
       "      <td>3.228691</td>\n",
       "      <td>24523.719045</td>\n",
       "      <td>2.031561</td>\n",
       "      <td>9494.932130</td>\n",
       "      <td>0.786566</td>\n",
       "      <td>3940.839400</td>\n",
       "      <td>0.326462</td>\n",
       "      <td>18380.279565</td>\n",
       "      <td>1.522634</td>\n",
       "      <td>86637.752965</td>\n",
       "      <td>7.177127</td>\n",
       "      <td>48572.402370</td>\n",
       "      <td>4.023769</td>\n",
       "      <td>543</td>\n",
       "      <td>2</td>\n",
       "      <td>69.342857</td>\n",
       "      <td>61.571429</td>\n",
       "      <td>5.514286</td>\n",
       "      <td>55.571429</td>\n",
       "      <td>86.928571</td>\n",
       "      <td>93.714286</td>\n",
       "      <td>53.271429</td>\n",
       "      <td>35.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.631034</td>\n",
       "      <td>54.034483</td>\n",
       "      <td>5.165517</td>\n",
       "      <td>55.824138</td>\n",
       "      <td>91.082759</td>\n",
       "      <td>85.344828</td>\n",
       "      <td>57.048276</td>\n",
       "      <td>32.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>264387</td>\n",
       "      <td>1285120</td>\n",
       "      <td>0.205729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>129912.706435</td>\n",
       "      <td>10.762052</td>\n",
       "      <td>98657.980320</td>\n",
       "      <td>8.172891</td>\n",
       "      <td>7078.165665</td>\n",
       "      <td>0.586360</td>\n",
       "      <td>20135.643300</td>\n",
       "      <td>1.668050</td>\n",
       "      <td>43449.533545</td>\n",
       "      <td>3.599387</td>\n",
       "      <td>89.625185</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>289.335895</td>\n",
       "      <td>0.023969</td>\n",
       "      <td>51006.070855</td>\n",
       "      <td>4.225376</td>\n",
       "      <td>1079.950120</td>\n",
       "      <td>0.089464</td>\n",
       "      <td>69300.283555</td>\n",
       "      <td>5.740880</td>\n",
       "      <td>39296.084525</td>\n",
       "      <td>3.255313</td>\n",
       "      <td>475740.267360</td>\n",
       "      <td>39.410630</td>\n",
       "      <td>6323.579430</td>\n",
       "      <td>0.523849</td>\n",
       "      <td>43506.021875</td>\n",
       "      <td>3.604067</td>\n",
       "      <td>16445.443065</td>\n",
       "      <td>1.362351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4938.725765</td>\n",
       "      <td>0.409127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335.594055</td>\n",
       "      <td>0.027801</td>\n",
       "      <td>3.741329e+04</td>\n",
       "      <td>3.099341</td>\n",
       "      <td>37280.296245</td>\n",
       "      <td>3.088324</td>\n",
       "      <td>3742.018270</td>\n",
       "      <td>0.309991</td>\n",
       "      <td>1934.169315</td>\n",
       "      <td>0.160228</td>\n",
       "      <td>16712.761855</td>\n",
       "      <td>1.384496</td>\n",
       "      <td>78488.755375</td>\n",
       "      <td>6.502059</td>\n",
       "      <td>23980.630455</td>\n",
       "      <td>1.986571</td>\n",
       "      <td>543</td>\n",
       "      <td>2</td>\n",
       "      <td>58.271429</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>4.257143</td>\n",
       "      <td>50.242857</td>\n",
       "      <td>69.642857</td>\n",
       "      <td>93.428571</td>\n",
       "      <td>47.857143</td>\n",
       "      <td>51.142857</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>56.789655</td>\n",
       "      <td>64.655172</td>\n",
       "      <td>5.820690</td>\n",
       "      <td>44.658621</td>\n",
       "      <td>70.431034</td>\n",
       "      <td>89.758621</td>\n",
       "      <td>43.913793</td>\n",
       "      <td>39.620690</td>\n",
       "      <td>0.047241</td>\n",
       "      <td>266528</td>\n",
       "      <td>1285120</td>\n",
       "      <td>0.207395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>69618.975590</td>\n",
       "      <td>8.119861</td>\n",
       "      <td>34154.089730</td>\n",
       "      <td>3.983490</td>\n",
       "      <td>4751.024385</td>\n",
       "      <td>0.554126</td>\n",
       "      <td>13503.157215</td>\n",
       "      <td>1.574912</td>\n",
       "      <td>101659.868030</td>\n",
       "      <td>11.856882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.122860</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>210237.332535</td>\n",
       "      <td>24.520584</td>\n",
       "      <td>5202.486235</td>\n",
       "      <td>0.606781</td>\n",
       "      <td>69703.040900</td>\n",
       "      <td>8.129666</td>\n",
       "      <td>8686.526305</td>\n",
       "      <td>1.013135</td>\n",
       "      <td>139265.305765</td>\n",
       "      <td>16.242913</td>\n",
       "      <td>8199.481255</td>\n",
       "      <td>0.956329</td>\n",
       "      <td>26610.451330</td>\n",
       "      <td>3.103653</td>\n",
       "      <td>1914.820950</td>\n",
       "      <td>0.223331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26594.216495</td>\n",
       "      <td>3.101760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.779160</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>1.824840e+04</td>\n",
       "      <td>2.128363</td>\n",
       "      <td>18928.260845</td>\n",
       "      <td>2.207657</td>\n",
       "      <td>7140.213870</td>\n",
       "      <td>0.832784</td>\n",
       "      <td>18896.458360</td>\n",
       "      <td>2.203948</td>\n",
       "      <td>9862.995855</td>\n",
       "      <td>1.150350</td>\n",
       "      <td>375.625155</td>\n",
       "      <td>0.043810</td>\n",
       "      <td>63821.582730</td>\n",
       "      <td>7.443695</td>\n",
       "      <td>524</td>\n",
       "      <td>46</td>\n",
       "      <td>51.114286</td>\n",
       "      <td>64.142857</td>\n",
       "      <td>3.042857</td>\n",
       "      <td>39.442857</td>\n",
       "      <td>67.857143</td>\n",
       "      <td>83.428571</td>\n",
       "      <td>38.271429</td>\n",
       "      <td>41.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006897</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>4.051724</td>\n",
       "      <td>36.358621</td>\n",
       "      <td>65.096552</td>\n",
       "      <td>79.586207</td>\n",
       "      <td>36.913793</td>\n",
       "      <td>39.689655</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>150498</td>\n",
       "      <td>919040</td>\n",
       "      <td>0.163756</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10631</th>\n",
       "      <td>44195.223980</td>\n",
       "      <td>1.532855</td>\n",
       "      <td>16.902020</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>11427.544680</td>\n",
       "      <td>0.396350</td>\n",
       "      <td>2.668740</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>144.556750</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>873885.584850</td>\n",
       "      <td>30.309598</td>\n",
       "      <td>7376.619755</td>\n",
       "      <td>0.255849</td>\n",
       "      <td>4.225505</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>3047.033895</td>\n",
       "      <td>0.105682</td>\n",
       "      <td>3.113530</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>111051.608880</td>\n",
       "      <td>3.851682</td>\n",
       "      <td>976.981235</td>\n",
       "      <td>0.033885</td>\n",
       "      <td>24582.431325</td>\n",
       "      <td>0.852610</td>\n",
       "      <td>1558.766555</td>\n",
       "      <td>0.054064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.446345</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>1.762661e+06</td>\n",
       "      <td>61.135638</td>\n",
       "      <td>0.222395</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>461.469625</td>\n",
       "      <td>0.016005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18000.206510</td>\n",
       "      <td>0.624314</td>\n",
       "      <td>18811.725865</td>\n",
       "      <td>0.652461</td>\n",
       "      <td>4986.985480</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>2666</td>\n",
       "      <td>1145</td>\n",
       "      <td>76.114286</td>\n",
       "      <td>22.285714</td>\n",
       "      <td>4.971429</td>\n",
       "      <td>34.742857</td>\n",
       "      <td>96.057143</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>52.957143</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.486207</td>\n",
       "      <td>28.689655</td>\n",
       "      <td>5.475862</td>\n",
       "      <td>34.003448</td>\n",
       "      <td>86.765517</td>\n",
       "      <td>63.344828</td>\n",
       "      <td>45.572414</td>\n",
       "      <td>13.517241</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>30822</td>\n",
       "      <td>3001600</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14734</th>\n",
       "      <td>1629.265770</td>\n",
       "      <td>0.508731</td>\n",
       "      <td>26.020215</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>1343.265800</td>\n",
       "      <td>0.419429</td>\n",
       "      <td>1.111975</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24917.580590</td>\n",
       "      <td>7.780408</td>\n",
       "      <td>110830.325855</td>\n",
       "      <td>34.606296</td>\n",
       "      <td>51.373245</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>76.059090</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>86.511655</td>\n",
       "      <td>0.027013</td>\n",
       "      <td>689.424500</td>\n",
       "      <td>0.215270</td>\n",
       "      <td>100865.028300</td>\n",
       "      <td>31.494674</td>\n",
       "      <td>589.569145</td>\n",
       "      <td>0.184090</td>\n",
       "      <td>9614.135850</td>\n",
       "      <td>3.001973</td>\n",
       "      <td>1215.833465</td>\n",
       "      <td>0.379639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444790</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.130274e+04</td>\n",
       "      <td>19.141518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.783825</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>222.172605</td>\n",
       "      <td>0.069372</td>\n",
       "      <td>5794.056935</td>\n",
       "      <td>1.809170</td>\n",
       "      <td>267.541185</td>\n",
       "      <td>0.083539</td>\n",
       "      <td>730.345180</td>\n",
       "      <td>0.228047</td>\n",
       "      <td>3121</td>\n",
       "      <td>43</td>\n",
       "      <td>50.257143</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>43.385714</td>\n",
       "      <td>59.371429</td>\n",
       "      <td>93.285714</td>\n",
       "      <td>41.400000</td>\n",
       "      <td>49.428571</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>54.203448</td>\n",
       "      <td>70.896552</td>\n",
       "      <td>2.844828</td>\n",
       "      <td>44.096552</td>\n",
       "      <td>65.131034</td>\n",
       "      <td>88.379310</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>47.517241</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>38382</td>\n",
       "      <td>384640</td>\n",
       "      <td>0.099787</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4539 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Alfalfa & Hay_acres  Alfalfa & Hay_percentage  Almonds_acres  \\\n",
       "12930            17.791600                  0.001264       3.780715   \n",
       "17287          5199.595100                  0.516035   36487.680465   \n",
       "11545          3504.945200                  0.347849   43919.676575   \n",
       "13853         64668.685285                  7.824309  150447.103970   \n",
       "5838          85001.815345                 12.412499   52873.966460   \n",
       "...                    ...                       ...            ...   \n",
       "1879         117482.160305                  9.732298  129201.042435   \n",
       "3619         129912.706435                 10.762052   98657.980320   \n",
       "2799          69618.975590                  8.119861   34154.089730   \n",
       "10631         44195.223980                  1.532855      16.902020   \n",
       "14734          1629.265770                  0.508731      26.020215   \n",
       "\n",
       "       Almonds_percentage   Barren_acres  Barren_percentage    Corn_acres  \\\n",
       "12930            0.000269  120475.152215           8.559617      0.000000   \n",
       "17287            3.621228    2509.060390           0.249012    348.270570   \n",
       "11545            4.358818    2064.937575           0.204935    270.209925   \n",
       "13853           18.202699    3492.935870           0.422613   3602.576605   \n",
       "5838             7.720989     901.811725           0.131688  48174.315320   \n",
       "...                   ...            ...                ...           ...   \n",
       "1879            10.703097    6547.531195           0.542402  33730.649650   \n",
       "3619             8.172891    7078.165665           0.586360  20135.643300   \n",
       "2799             3.983490    4751.024385           0.554126  13503.157215   \n",
       "10631            0.000586   11427.544680           0.396350      2.668740   \n",
       "14734            0.008125    1343.265800           0.419429      1.111975   \n",
       "\n",
       "       Corn_percentage   Cotton_acres  Cotton_percentage  \\\n",
       "12930         0.000000       0.000000           0.000000   \n",
       "17287         0.034564     221.505420           0.021983   \n",
       "11545         0.026817      81.396570           0.008078   \n",
       "13853         0.435878     365.172590           0.044182   \n",
       "5838          7.034716      64.494550           0.009418   \n",
       "...                ...            ...                ...   \n",
       "1879          2.794269   39887.210435           3.304282   \n",
       "3619          1.668050   43449.533545           3.599387   \n",
       "2799          1.574912  101659.868030          11.856882   \n",
       "10631         0.000093       0.000000           0.000000   \n",
       "14734         0.000347       0.000000           0.000000   \n",
       "\n",
       "       Deciduous Forest_acres  Deciduous Forest_percentage  \\\n",
       "12930            17857.428920                     1.268749   \n",
       "17287            36955.154755                     3.667622   \n",
       "11545            36126.733380                     3.585405   \n",
       "13853              239.519415                     0.028980   \n",
       "5838                28.021770                     0.004092   \n",
       "...                       ...                          ...   \n",
       "1879               201.489870                     0.016692   \n",
       "3619                89.625185                     0.007425   \n",
       "2799                 0.000000                     0.000000   \n",
       "10631              144.556750                     0.005014   \n",
       "14734            24917.580590                     7.780408   \n",
       "\n",
       "       Evergreen Forest_acres  Evergreen Forest_percentage   Fallow_acres  \\\n",
       "12930           673694.279255                    47.865180       9.562985   \n",
       "17287           389571.545450                    38.663112   82134.254215   \n",
       "11545           391415.422395                    38.846108   63940.786450   \n",
       "13853              350.272125                     0.042380    6562.654055   \n",
       "5838                29.578535                     0.004319   13848.091860   \n",
       "...                       ...                          ...            ...   \n",
       "1879               320.915985                     0.026585   30481.013910   \n",
       "3619               289.335895                     0.023969   51006.070855   \n",
       "2799                15.122860                     0.001764  210237.332535   \n",
       "10631           873885.584850                    30.309598    7376.619755   \n",
       "14734           110830.325855                    34.606296      51.373245   \n",
       "\n",
       "       Fallow_percentage  Fruit Trees_acres  Fruit Trees_percentage  \\\n",
       "12930           0.000679           1.779160                0.000126   \n",
       "17287           8.151432       19776.252980                1.962698   \n",
       "11545           6.345817        8684.302355                0.861875   \n",
       "13853           0.794020        9814.513745                1.187465   \n",
       "5838            2.022185        9251.409605                1.350949   \n",
       "...                  ...                ...                     ...   \n",
       "1879            2.525067        2628.041715                0.217709   \n",
       "3619            4.225376        1079.950120                0.089464   \n",
       "2799           24.520584        5202.486235                0.606781   \n",
       "10631           0.255849           4.225505                0.000147   \n",
       "14734           0.016041          76.059090                0.023749   \n",
       "\n",
       "       Grain Crops_acres  Grain Crops_percentage  Grapes_acres  \\\n",
       "12930           1.556765                0.000111      0.889580   \n",
       "17287         919.380930                0.091244     42.477445   \n",
       "11545         568.886410                0.056459    393.416755   \n",
       "13853       36891.549785                4.463534   8749.241695   \n",
       "5838        40089.589885                5.854134  81528.895025   \n",
       "...                  ...                     ...           ...   \n",
       "1879        65542.252845                5.429562  15494.704440   \n",
       "3619        69300.283555                5.740880  39296.084525   \n",
       "2799        69703.040900                8.129666   8686.526305   \n",
       "10631        3047.033895                0.105682      3.113530   \n",
       "14734          86.511655                0.027013    689.424500   \n",
       "\n",
       "       Grapes_percentage  Grassland_acres  Grassland_percentage  \\\n",
       "12930           0.000063    107346.063390              7.626811   \n",
       "17287           0.004216    148690.405865             14.756811   \n",
       "11545           0.039045    148383.278370             14.726330   \n",
       "13853           1.058577    263669.065655             31.901502   \n",
       "5838           11.905361    129100.075105             18.852004   \n",
       "...                  ...              ...                   ...   \n",
       "1879            1.283591    462364.542480             38.302576   \n",
       "3619            3.255313    475740.267360             39.410630   \n",
       "2799            1.013135    139265.305765             16.242913   \n",
       "10631           0.000108    111051.608880              3.851682   \n",
       "14734           0.215270    100865.028300             31.494674   \n",
       "\n",
       "       High Intensity Developed_acres  High Intensity Developed_percentage  \\\n",
       "12930                      556.432290                             0.039534   \n",
       "17287                     5127.316725                             0.508862   \n",
       "11545                     4998.105230                             0.496038   \n",
       "13853                    10116.970945                             1.224059   \n",
       "5838                     20984.747410                             3.064325   \n",
       "...                               ...                                  ...   \n",
       "1879                      4733.010390                             0.392086   \n",
       "3619                      6323.579430                             0.523849   \n",
       "2799                      8199.481255                             0.956329   \n",
       "10631                      976.981235                             0.033885   \n",
       "14734                      589.569145                             0.184090   \n",
       "\n",
       "       Low Intensity Developed_acres  Low Intensity Developed_percentage  \\\n",
       "12930                   11897.687710                            0.845317   \n",
       "17287                   28198.796420                            2.798596   \n",
       "11545                   27114.398400                            2.690974   \n",
       "13853                   32563.520690                            3.939883   \n",
       "5838                    28916.909875                            4.222629   \n",
       "...                              ...                                 ...   \n",
       "1879                    42937.580255                            3.556977   \n",
       "3619                    43506.021875                            3.604067   \n",
       "2799                    26610.451330                            3.103653   \n",
       "10631                   24582.431325                            0.852610   \n",
       "14734                    9614.135850                            3.001973   \n",
       "\n",
       "       Mixed Forest_acres  Mixed Forest_percentage  Other Ocean/Mexico_acres  \\\n",
       "12930          265.094840                 0.018835                       0.0   \n",
       "17287          583.786875                 0.057938                       0.0   \n",
       "11545          808.850615                 0.080275                       0.0   \n",
       "13853        43372.362480                 5.247652                       0.0   \n",
       "5838          6168.570115                 0.900773                       0.0   \n",
       "...                   ...                      ...                       ...   \n",
       "1879         18330.463085                 1.518507                       0.0   \n",
       "3619         16445.443065                 1.362351                       0.0   \n",
       "2799          1914.820950                 0.223331                       0.0   \n",
       "10631         1558.766555                 0.054064                       0.0   \n",
       "14734         1215.833465                 0.379639                       0.0   \n",
       "\n",
       "       Other Ocean/Mexico_percentage  Other Tree Crops_acres  \\\n",
       "12930                            0.0                1.556765   \n",
       "17287                            0.0             2598.240785   \n",
       "11545                            0.0              968.085435   \n",
       "13853                            0.0             2757.698000   \n",
       "5838                             0.0               21.794710   \n",
       "...                              ...                     ...   \n",
       "1879                             0.0             4119.200190   \n",
       "3619                             0.0             4938.725765   \n",
       "2799                             0.0            26594.216495   \n",
       "10631                            0.0                0.000000   \n",
       "14734                            0.0                0.444790   \n",
       "\n",
       "       Other Tree Crops_percentage  Other_acres  Other_percentage  \\\n",
       "12930                     0.000111          0.0               0.0   \n",
       "17287                     0.257863          0.0               0.0   \n",
       "11545                     0.096078          0.0               0.0   \n",
       "13853                     0.333656          0.0               0.0   \n",
       "5838                      0.003183          0.0               0.0   \n",
       "...                            ...          ...               ...   \n",
       "1879                      0.341237          0.0               0.0   \n",
       "3619                      0.409127          0.0               0.0   \n",
       "2799                      3.101760          0.0               0.0   \n",
       "10631                     0.000000          0.0               0.0   \n",
       "14734                     0.000139          0.0               0.0   \n",
       "\n",
       "          Rice_acres  Rice_percentage  Shrubland_acres  Shrubland_percentage  \\\n",
       "12930       0.000000         0.000000     4.451974e+05             31.630749   \n",
       "17287   79618.521975         7.901757     8.180177e+04              8.118435   \n",
       "11545  103580.471250        10.279866     8.430928e+04              8.367292   \n",
       "13853     504.836650         0.061081     1.153868e+05             13.960723   \n",
       "5838     2931.833285         0.428125     1.645545e+04              2.402928   \n",
       "...              ...              ...              ...                   ...   \n",
       "1879     2610.472510         0.216253     3.897472e+04              3.228691   \n",
       "3619      335.594055         0.027801     3.741329e+04              3.099341   \n",
       "2799        1.779160         0.000208     1.824840e+04              2.128363   \n",
       "10631       2.446345         0.000085     1.762661e+06             61.135638   \n",
       "14734       0.000000         0.000000     6.130274e+04             19.141518   \n",
       "\n",
       "       Tomatoes_acres  Tomatoes_percentage  Vegs & Fruits_acres  \\\n",
       "12930        0.000000             0.000000             0.000000   \n",
       "17287      777.715315             0.077185           516.401190   \n",
       "11545      838.873940             0.083254          1137.772820   \n",
       "13853     6909.590255             0.835996          6355.604310   \n",
       "5838     46361.128885             6.769943         12267.308200   \n",
       "...               ...                  ...                  ...   \n",
       "1879     24523.719045             2.031561          9494.932130   \n",
       "3619     37280.296245             3.088324          3742.018270   \n",
       "2799     18928.260845             2.207657          7140.213870   \n",
       "10631        0.222395             0.000008           461.469625   \n",
       "14734        0.000000             0.000000             7.783825   \n",
       "\n",
       "       Vegs & Fruits_percentage  Walnuts_acres  Walnuts_percentage  \\\n",
       "12930                  0.000000       0.222395            0.000016   \n",
       "17287                  0.051250   51968.374020            5.157612   \n",
       "11545                  0.112919   49604.092775            4.922969   \n",
       "13853                  0.768969   37214.912115            4.502658   \n",
       "5838                   1.791349   51725.741075            7.553317   \n",
       "...                         ...            ...                 ...   \n",
       "1879                   0.786566    3940.839400            0.326462   \n",
       "3619                   0.309991    1934.169315            0.160228   \n",
       "2799                   0.832784   18896.458360            2.203948   \n",
       "10631                  0.016005       0.000000            0.000000   \n",
       "14734                  0.002430     222.172605            0.069372   \n",
       "\n",
       "        Water_acres  Water_percentage  Wetlands_acres  Wetlands_percentage  \\\n",
       "12930  29309.214655          2.082385      816.189650             0.057989   \n",
       "17287  21834.741100          2.166994    11160.893075             1.107665   \n",
       "11545  22076.706860          2.191008    11046.582045             1.096320   \n",
       "13853   4093.847160          0.495317    10072.491945             1.218678   \n",
       "5838    7040.358515          1.028077     7132.874835             1.041587   \n",
       "...             ...               ...             ...                  ...   \n",
       "1879   18380.279565          1.522634    86637.752965             7.177127   \n",
       "3619   16712.761855          1.384496    78488.755375             6.502059   \n",
       "2799    9862.995855          1.150350      375.625155             0.043810   \n",
       "10631  18000.206510          0.624314    18811.725865             0.652461   \n",
       "14734   5794.056935          1.809170      267.541185             0.083539   \n",
       "\n",
       "       Winter Wheat_acres  Winter Wheat_percentage  max_elevation  \\\n",
       "12930           30.912905                 0.002196           3959   \n",
       "17287          563.104140                 0.055885           2192   \n",
       "11545         1768.040250                 0.175470           2192   \n",
       "13853         8308.010015                 1.005192           1026   \n",
       "5838         23909.464055                 3.491410           1889   \n",
       "...                   ...                      ...            ...   \n",
       "1879         48572.402370                 4.023769            543   \n",
       "3619         23980.630455                 1.986571            543   \n",
       "2799         63821.582730                 7.443695            524   \n",
       "10631         4986.985480                 0.172967           2666   \n",
       "14734          730.345180                 0.228047           3121   \n",
       "\n",
       "       min_elevation  Avg Air Temp (F)_Weekly  Avg Rel Hum (%)_Weekly  \\\n",
       "12930            374                60.528571               60.714286   \n",
       "17287             -1                64.800000               61.142857   \n",
       "11545             -1                58.814286               69.285714   \n",
       "13853             -2                77.078571               45.384615   \n",
       "5838               6                49.300000               88.285714   \n",
       "...              ...                      ...                     ...   \n",
       "1879               2                69.342857               61.571429   \n",
       "3619               2                58.271429               75.000000   \n",
       "2799              46                51.114286               64.142857   \n",
       "10631           1145                76.114286               22.285714   \n",
       "14734             43                50.257143               78.571429   \n",
       "\n",
       "       Avg Wind Speed (mph)_Weekly  Dew Point (F)_Weekly  \\\n",
       "12930                    11.685714             44.942857   \n",
       "17287                     2.242857             51.114286   \n",
       "11545                     2.542857             48.728571   \n",
       "13853                     5.057143             53.669231   \n",
       "5838                      2.257143             45.928571   \n",
       "...                            ...                   ...   \n",
       "1879                      5.514286             55.571429   \n",
       "3619                      4.257143             50.242857   \n",
       "2799                      3.042857             39.442857   \n",
       "10631                     4.971429             34.742857   \n",
       "14734                     2.714286             43.385714   \n",
       "\n",
       "       Max Air Temp (F)_Weekly  Max Rel Hum (%)_Weekly  \\\n",
       "12930                73.714286               88.285714   \n",
       "17287                81.900000               90.714286   \n",
       "11545                74.457143               95.285714   \n",
       "13853                96.321429               81.714286   \n",
       "5838                 64.285714              100.000000   \n",
       "...                        ...                     ...   \n",
       "1879                 86.928571               93.714286   \n",
       "3619                 69.642857               93.428571   \n",
       "2799                 67.857143               83.428571   \n",
       "10631                96.057143               51.000000   \n",
       "14734                59.371429               93.285714   \n",
       "\n",
       "       Min Air Temp (F)_Weekly  Min Rel Hum (%)_Weekly  Precip (in)_Weekly  \\\n",
       "12930                48.000000               33.000000            0.004286   \n",
       "17287                50.757143               31.285714            0.000000   \n",
       "11545                46.557143               39.142857            0.000000   \n",
       "13853                57.857143               23.285714            0.000000   \n",
       "5838                 38.200000               59.571429            0.064286   \n",
       "...                        ...                     ...                 ...   \n",
       "1879                 53.271429               35.571429            0.000000   \n",
       "3619                 47.857143               51.142857            0.034286   \n",
       "2799                 38.271429               41.571429            0.000000   \n",
       "10631                52.957143                8.857143            0.000000   \n",
       "14734                41.400000               49.428571            0.050000   \n",
       "\n",
       "       Avg Air Temp (F)_month  Avg Rel Hum (%)_month  \\\n",
       "12930               59.082759              70.362069   \n",
       "17287               68.113793              61.172414   \n",
       "11545               60.148276              72.000000   \n",
       "13853               77.734483              46.827586   \n",
       "5838                53.672414              86.758621   \n",
       "...                       ...                    ...   \n",
       "1879                73.631034              54.034483   \n",
       "3619                56.789655              64.655172   \n",
       "2799                50.006897              60.000000   \n",
       "10631               68.486207              28.689655   \n",
       "14734               54.203448              70.896552   \n",
       "\n",
       "       Avg Wind Speed (mph)_month  Dew Point (F)_month  \\\n",
       "12930                    7.817241            47.824138   \n",
       "17287                    2.162069            54.089655   \n",
       "11545                    3.082759            50.639286   \n",
       "13853                    5.229310            55.067241   \n",
       "5838                     3.168966            49.686207   \n",
       "...                           ...                  ...   \n",
       "1879                     5.165517            55.824138   \n",
       "3619                     5.820690            44.658621   \n",
       "2799                     4.051724            36.358621   \n",
       "10631                    5.475862            34.003448   \n",
       "14734                    2.844828            44.096552   \n",
       "\n",
       "       Max Air Temp (F)_month  Max Rel Hum (%)_month  Min Air Temp (F)_month  \\\n",
       "12930               70.793103              93.379310               48.689655   \n",
       "17287               86.786207              92.413793               52.775862   \n",
       "11545               73.182759              94.285714               49.851724   \n",
       "13853               96.663793              81.155172               59.132759   \n",
       "5838                65.420690              99.551724               43.868966   \n",
       "...                       ...                    ...                     ...   \n",
       "1879                91.082759              85.344828               57.048276   \n",
       "3619                70.431034              89.758621               43.913793   \n",
       "2799                65.096552              79.586207               36.913793   \n",
       "10631               86.765517              63.344828               45.572414   \n",
       "14734               65.131034              88.379310               44.500000   \n",
       "\n",
       "       Min Rel Hum (%)_month  Precip (in)_month  Population  county_acres  \\\n",
       "12930              41.827586           0.098276       54725       1467520   \n",
       "17287              32.620690           0.000000      227621       1065600   \n",
       "11545              48.785714           0.135172      224432       1065600   \n",
       "13853              25.379310           0.000000      549976        973440   \n",
       "5838               62.862069           0.083793      712134        919040   \n",
       "...                      ...                ...         ...           ...   \n",
       "1879               32.034483           0.000000      264387       1285120   \n",
       "3619               39.620690           0.047241      266528       1285120   \n",
       "2799               39.689655           0.003448      150498        919040   \n",
       "10631              13.517241           0.002069       30822       3001600   \n",
       "14734              47.517241           0.190000       38382        384640   \n",
       "\n",
       "       pop_density  Alpine  Amador  Butte  Calaveras  Colusa  Contra Costa  \\\n",
       "12930     0.037291       0       0      0          0       0             0   \n",
       "17287     0.213608       0       0      1          0       0             0   \n",
       "11545     0.210616       0       0      1          0       0             0   \n",
       "13853     0.564982       0       0      0          0       0             0   \n",
       "5838      0.774867       0       0      0          0       0             0   \n",
       "...            ...     ...     ...    ...        ...     ...           ...   \n",
       "1879      0.205729       0       0      0          0       0             0   \n",
       "3619      0.207395       0       0      0          0       0             0   \n",
       "2799      0.163756       0       0      0          0       0             0   \n",
       "10631     0.010269       0       0      0          0       0             0   \n",
       "14734     0.099787       0       1      0          0       0             0   \n",
       "\n",
       "       Del Norte  El Dorado  Fresno  Glenn  Humboldt  Imperial  Inyo  Kern  \\\n",
       "12930          0          0       0      0         0         0     0     0   \n",
       "17287          0          0       0      0         0         0     0     0   \n",
       "11545          0          0       0      0         0         0     0     0   \n",
       "13853          0          0       0      0         0         0     0     0   \n",
       "5838           0          0       0      0         0         0     0     0   \n",
       "...          ...        ...     ...    ...       ...       ...   ...   ...   \n",
       "1879           0          0       0      0         0         0     0     0   \n",
       "3619           0          0       0      0         0         0     0     0   \n",
       "2799           0          0       0      0         0         0     0     0   \n",
       "10631          0          0       0      0         0         0     0     0   \n",
       "14734          0          0       0      0         0         0     0     0   \n",
       "\n",
       "       Kings  Lake  Lassen  Los Angeles  Madera  Marin  Mariposa  Mendocino  \\\n",
       "12930      0     0       0            0       0      0         0          0   \n",
       "17287      0     0       0            0       0      0         0          0   \n",
       "11545      0     0       0            0       0      0         0          0   \n",
       "13853      0     0       0            0       0      0         0          0   \n",
       "5838       0     0       0            0       0      0         0          0   \n",
       "...      ...   ...     ...          ...     ...    ...       ...        ...   \n",
       "1879       0     0       0            0       0      0         0          0   \n",
       "3619       0     0       0            0       0      0         0          0   \n",
       "2799       1     0       0            0       0      0         0          0   \n",
       "10631      0     0       1            0       0      0         0          0   \n",
       "14734      0     0       0            0       0      0         0          0   \n",
       "\n",
       "       Merced  Modoc  Mono  Monterey  Napa  Nevada  Orange  Placer  Plumas  \\\n",
       "12930       0      0     0         0     0       0       0       0       0   \n",
       "17287       0      0     0         0     0       0       0       0       0   \n",
       "11545       0      0     0         0     0       0       0       0       0   \n",
       "13853       0      0     0         0     0       0       0       0       0   \n",
       "5838        0      0     0         0     0       0       0       0       0   \n",
       "...       ...    ...   ...       ...   ...     ...     ...     ...     ...   \n",
       "1879        1      0     0         0     0       0       0       0       0   \n",
       "3619        1      0     0         0     0       0       0       0       0   \n",
       "2799        0      0     0         0     0       0       0       0       0   \n",
       "10631       0      0     0         0     0       0       0       0       0   \n",
       "14734       0      0     0         0     0       0       0       0       0   \n",
       "\n",
       "       Riverside  Sacramento  San Benito  San Bernardino  San Diego  \\\n",
       "12930          0           0           0               0          0   \n",
       "17287          0           0           0               0          0   \n",
       "11545          0           0           0               0          0   \n",
       "13853          0           0           0               0          0   \n",
       "5838           0           0           0               0          0   \n",
       "...          ...         ...         ...             ...        ...   \n",
       "1879           0           0           0               0          0   \n",
       "3619           0           0           0               0          0   \n",
       "2799           0           0           0               0          0   \n",
       "10631          0           0           0               0          0   \n",
       "14734          0           0           0               0          0   \n",
       "\n",
       "       San Francisco  San Joaquin  San Luis Obispo  San Mateo  Santa Barbara  \\\n",
       "12930              0            0                0          0              0   \n",
       "17287              0            0                0          0              0   \n",
       "11545              0            0                0          0              0   \n",
       "13853              0            0                0          0              0   \n",
       "5838               0            1                0          0              0   \n",
       "...              ...          ...              ...        ...            ...   \n",
       "1879               0            0                0          0              0   \n",
       "3619               0            0                0          0              0   \n",
       "2799               0            0                0          0              0   \n",
       "10631              0            0                0          0              0   \n",
       "14734              0            0                0          0              0   \n",
       "\n",
       "       Santa Clara  Santa Cruz  Shasta  Sierra  Siskiyou  Solano  Sonoma  \\\n",
       "12930            0           0       0       0         0       0       0   \n",
       "17287            0           0       0       0         0       0       0   \n",
       "11545            0           0       0       0         0       0       0   \n",
       "13853            0           0       0       0         0       0       0   \n",
       "5838             0           0       0       0         0       0       0   \n",
       "...            ...         ...     ...     ...       ...     ...     ...   \n",
       "1879             0           0       0       0         0       0       0   \n",
       "3619             0           0       0       0         0       0       0   \n",
       "2799             0           0       0       0         0       0       0   \n",
       "10631            0           0       0       0         0       0       0   \n",
       "14734            0           0       0       0         0       0       0   \n",
       "\n",
       "       Stanislaus  Sutter  Tehama  Trinity  Tulare  Tuolumne  Ventura  Yolo  \\\n",
       "12930           0       0       0        0       0         1        0     0   \n",
       "17287           0       0       0        0       0         0        0     0   \n",
       "11545           0       0       0        0       0         0        0     0   \n",
       "13853           1       0       0        0       0         0        0     0   \n",
       "5838            0       0       0        0       0         0        0     0   \n",
       "...           ...     ...     ...      ...     ...       ...      ...   ...   \n",
       "1879            0       0       0        0       0         0        0     0   \n",
       "3619            0       0       0        0       0         0        0     0   \n",
       "2799            0       0       0        0       0         0        0     0   \n",
       "10631           0       0       0        0       0         0        0     0   \n",
       "14734           0       0       0        0       0         0        0     0   \n",
       "\n",
       "       Yuba  2  3  4  5  6  7  8  9  10  11  12  fire_started  \n",
       "12930     0  0  0  1  0  0  0  0  0   0   0   0           0.0  \n",
       "17287     0  0  0  0  0  0  0  0  1   0   0   0           0.0  \n",
       "11545     0  0  0  0  0  0  0  0  0   1   0   0           0.0  \n",
       "13853     0  0  0  0  0  0  1  0  0   0   0   0           0.0  \n",
       "5838      0  0  0  0  0  0  0  0  0   0   0   1           0.0  \n",
       "...     ... .. .. .. .. .. .. .. ..  ..  ..  ..           ...  \n",
       "1879      0  0  0  0  0  0  0  1  0   0   0   0           0.0  \n",
       "3619      0  0  1  0  0  0  0  0  0   0   0   0           0.0  \n",
       "2799      0  0  0  0  0  0  0  0  0   0   0   1           0.0  \n",
       "10631     0  0  0  0  0  0  1  0  0   0   0   0           0.0  \n",
       "14734     0  0  0  0  0  0  0  0  0   0   1   0           0.0  \n",
       "\n",
       "[4539 rows x 144 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = pd.concat([X_test, y_test], axis = 1)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alfalfa &amp; Hay_acres</th>\n",
       "      <th>Alfalfa &amp; Hay_percentage</th>\n",
       "      <th>Almonds_acres</th>\n",
       "      <th>Almonds_percentage</th>\n",
       "      <th>Barren_acres</th>\n",
       "      <th>Barren_percentage</th>\n",
       "      <th>Corn_acres</th>\n",
       "      <th>Corn_percentage</th>\n",
       "      <th>Cotton_acres</th>\n",
       "      <th>Cotton_percentage</th>\n",
       "      <th>Deciduous Forest_acres</th>\n",
       "      <th>Deciduous Forest_percentage</th>\n",
       "      <th>Evergreen Forest_acres</th>\n",
       "      <th>Evergreen Forest_percentage</th>\n",
       "      <th>Fallow_acres</th>\n",
       "      <th>Fallow_percentage</th>\n",
       "      <th>Fruit Trees_acres</th>\n",
       "      <th>Fruit Trees_percentage</th>\n",
       "      <th>Grain Crops_acres</th>\n",
       "      <th>Grain Crops_percentage</th>\n",
       "      <th>Grapes_acres</th>\n",
       "      <th>Grapes_percentage</th>\n",
       "      <th>Grassland_acres</th>\n",
       "      <th>Grassland_percentage</th>\n",
       "      <th>High Intensity Developed_acres</th>\n",
       "      <th>High Intensity Developed_percentage</th>\n",
       "      <th>Low Intensity Developed_acres</th>\n",
       "      <th>Low Intensity Developed_percentage</th>\n",
       "      <th>Mixed Forest_acres</th>\n",
       "      <th>Mixed Forest_percentage</th>\n",
       "      <th>Other Ocean/Mexico_acres</th>\n",
       "      <th>Other Ocean/Mexico_percentage</th>\n",
       "      <th>Other Tree Crops_acres</th>\n",
       "      <th>Other Tree Crops_percentage</th>\n",
       "      <th>Other_acres</th>\n",
       "      <th>Other_percentage</th>\n",
       "      <th>Rice_acres</th>\n",
       "      <th>Rice_percentage</th>\n",
       "      <th>Shrubland_acres</th>\n",
       "      <th>Shrubland_percentage</th>\n",
       "      <th>Tomatoes_acres</th>\n",
       "      <th>Tomatoes_percentage</th>\n",
       "      <th>Vegs &amp; Fruits_acres</th>\n",
       "      <th>Vegs &amp; Fruits_percentage</th>\n",
       "      <th>Walnuts_acres</th>\n",
       "      <th>Walnuts_percentage</th>\n",
       "      <th>Water_acres</th>\n",
       "      <th>Water_percentage</th>\n",
       "      <th>Wetlands_acres</th>\n",
       "      <th>Wetlands_percentage</th>\n",
       "      <th>Winter Wheat_acres</th>\n",
       "      <th>Winter Wheat_percentage</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>min_elevation</th>\n",
       "      <th>Avg Air Temp (F)_Weekly</th>\n",
       "      <th>Avg Rel Hum (%)_Weekly</th>\n",
       "      <th>Avg Wind Speed (mph)_Weekly</th>\n",
       "      <th>Dew Point (F)_Weekly</th>\n",
       "      <th>Max Air Temp (F)_Weekly</th>\n",
       "      <th>Max Rel Hum (%)_Weekly</th>\n",
       "      <th>Min Air Temp (F)_Weekly</th>\n",
       "      <th>Min Rel Hum (%)_Weekly</th>\n",
       "      <th>Precip (in)_Weekly</th>\n",
       "      <th>Avg Air Temp (F)_month</th>\n",
       "      <th>Avg Rel Hum (%)_month</th>\n",
       "      <th>Avg Wind Speed (mph)_month</th>\n",
       "      <th>Dew Point (F)_month</th>\n",
       "      <th>Max Air Temp (F)_month</th>\n",
       "      <th>Max Rel Hum (%)_month</th>\n",
       "      <th>Min Air Temp (F)_month</th>\n",
       "      <th>Min Rel Hum (%)_month</th>\n",
       "      <th>Precip (in)_month</th>\n",
       "      <th>Population</th>\n",
       "      <th>county_acres</th>\n",
       "      <th>pop_density</th>\n",
       "      <th>Alpine</th>\n",
       "      <th>Amador</th>\n",
       "      <th>Butte</th>\n",
       "      <th>Calaveras</th>\n",
       "      <th>Colusa</th>\n",
       "      <th>Contra Costa</th>\n",
       "      <th>Del Norte</th>\n",
       "      <th>El Dorado</th>\n",
       "      <th>Fresno</th>\n",
       "      <th>Glenn</th>\n",
       "      <th>Humboldt</th>\n",
       "      <th>Imperial</th>\n",
       "      <th>Inyo</th>\n",
       "      <th>Kern</th>\n",
       "      <th>Kings</th>\n",
       "      <th>Lake</th>\n",
       "      <th>Lassen</th>\n",
       "      <th>Los Angeles</th>\n",
       "      <th>Madera</th>\n",
       "      <th>Marin</th>\n",
       "      <th>Mariposa</th>\n",
       "      <th>Mendocino</th>\n",
       "      <th>Merced</th>\n",
       "      <th>Modoc</th>\n",
       "      <th>Mono</th>\n",
       "      <th>Monterey</th>\n",
       "      <th>Napa</th>\n",
       "      <th>Nevada</th>\n",
       "      <th>Orange</th>\n",
       "      <th>Placer</th>\n",
       "      <th>Plumas</th>\n",
       "      <th>Riverside</th>\n",
       "      <th>Sacramento</th>\n",
       "      <th>San Benito</th>\n",
       "      <th>San Bernardino</th>\n",
       "      <th>San Diego</th>\n",
       "      <th>San Francisco</th>\n",
       "      <th>San Joaquin</th>\n",
       "      <th>San Luis Obispo</th>\n",
       "      <th>San Mateo</th>\n",
       "      <th>Santa Barbara</th>\n",
       "      <th>Santa Clara</th>\n",
       "      <th>Santa Cruz</th>\n",
       "      <th>Shasta</th>\n",
       "      <th>Sierra</th>\n",
       "      <th>Siskiyou</th>\n",
       "      <th>Solano</th>\n",
       "      <th>Sonoma</th>\n",
       "      <th>Stanislaus</th>\n",
       "      <th>Sutter</th>\n",
       "      <th>Tehama</th>\n",
       "      <th>Trinity</th>\n",
       "      <th>Tulare</th>\n",
       "      <th>Tuolumne</th>\n",
       "      <th>Ventura</th>\n",
       "      <th>Yolo</th>\n",
       "      <th>Yuba</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>fire_started</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1.167000e+03</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1.167000e+03</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1.167000e+03</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1.167000e+03</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1.167000e+03</td>\n",
       "      <td>1.167000e+03</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23313.990675</td>\n",
       "      <td>1.561762</td>\n",
       "      <td>22064.063504</td>\n",
       "      <td>1.371046</td>\n",
       "      <td>1.075054e+05</td>\n",
       "      <td>2.498277</td>\n",
       "      <td>2427.321938</td>\n",
       "      <td>0.315772</td>\n",
       "      <td>4184.554972</td>\n",
       "      <td>0.215082</td>\n",
       "      <td>9766.566374</td>\n",
       "      <td>0.924501</td>\n",
       "      <td>4.356794e+05</td>\n",
       "      <td>24.284078</td>\n",
       "      <td>25891.105460</td>\n",
       "      <td>1.596966</td>\n",
       "      <td>7149.825641</td>\n",
       "      <td>0.350702</td>\n",
       "      <td>11451.757340</td>\n",
       "      <td>0.843314</td>\n",
       "      <td>18699.079653</td>\n",
       "      <td>1.206556</td>\n",
       "      <td>2.357799e+05</td>\n",
       "      <td>16.593143</td>\n",
       "      <td>47500.717264</td>\n",
       "      <td>2.417373</td>\n",
       "      <td>78592.289681</td>\n",
       "      <td>4.530946</td>\n",
       "      <td>40565.408275</td>\n",
       "      <td>3.897980</td>\n",
       "      <td>567.954905</td>\n",
       "      <td>0.114946</td>\n",
       "      <td>6814.444643</td>\n",
       "      <td>0.278145</td>\n",
       "      <td>63.322355</td>\n",
       "      <td>0.009420</td>\n",
       "      <td>4133.638904</td>\n",
       "      <td>0.570451</td>\n",
       "      <td>1.031320e+06</td>\n",
       "      <td>32.797369</td>\n",
       "      <td>4341.484278</td>\n",
       "      <td>0.310663</td>\n",
       "      <td>4955.152885</td>\n",
       "      <td>0.267650</td>\n",
       "      <td>5332.285066</td>\n",
       "      <td>0.510640</td>\n",
       "      <td>15917.243315</td>\n",
       "      <td>1.138512</td>\n",
       "      <td>10807.791560</td>\n",
       "      <td>0.905401</td>\n",
       "      <td>6908.304861</td>\n",
       "      <td>0.470367</td>\n",
       "      <td>2447.790060</td>\n",
       "      <td>143.879177</td>\n",
       "      <td>67.942395</td>\n",
       "      <td>54.282378</td>\n",
       "      <td>4.934138</td>\n",
       "      <td>48.737328</td>\n",
       "      <td>83.564098</td>\n",
       "      <td>80.874014</td>\n",
       "      <td>53.560680</td>\n",
       "      <td>32.194600</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>67.925345</td>\n",
       "      <td>53.944701</td>\n",
       "      <td>4.960010</td>\n",
       "      <td>48.465399</td>\n",
       "      <td>83.531597</td>\n",
       "      <td>80.477495</td>\n",
       "      <td>53.540971</td>\n",
       "      <td>31.908716</td>\n",
       "      <td>0.008175</td>\n",
       "      <td>9.948443e+05</td>\n",
       "      <td>2.275033e+06</td>\n",
       "      <td>0.633285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016281</td>\n",
       "      <td>0.021422</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.019709</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.016281</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.035133</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.023136</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.035990</td>\n",
       "      <td>0.023993</td>\n",
       "      <td>0.016281</td>\n",
       "      <td>0.015424</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>0.038560</td>\n",
       "      <td>0.008569</td>\n",
       "      <td>0.017138</td>\n",
       "      <td>0.035990</td>\n",
       "      <td>0.032562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.017138</td>\n",
       "      <td>0.017995</td>\n",
       "      <td>0.013710</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.023136</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.031705</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>0.096829</td>\n",
       "      <td>0.187661</td>\n",
       "      <td>0.195373</td>\n",
       "      <td>0.191088</td>\n",
       "      <td>0.157669</td>\n",
       "      <td>0.095116</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>0.048843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34416.785368</td>\n",
       "      <td>2.738412</td>\n",
       "      <td>53584.737254</td>\n",
       "      <td>3.175480</td>\n",
       "      <td>2.602384e+05</td>\n",
       "      <td>3.081949</td>\n",
       "      <td>6517.840324</td>\n",
       "      <td>1.071507</td>\n",
       "      <td>13656.074612</td>\n",
       "      <td>0.966442</td>\n",
       "      <td>13277.021490</td>\n",
       "      <td>1.621746</td>\n",
       "      <td>5.315691e+05</td>\n",
       "      <td>22.986359</td>\n",
       "      <td>52064.720206</td>\n",
       "      <td>2.753752</td>\n",
       "      <td>21119.032207</td>\n",
       "      <td>0.784363</td>\n",
       "      <td>22405.386738</td>\n",
       "      <td>1.791228</td>\n",
       "      <td>40273.794732</td>\n",
       "      <td>2.172341</td>\n",
       "      <td>2.109226e+05</td>\n",
       "      <td>12.426327</td>\n",
       "      <td>141674.679315</td>\n",
       "      <td>5.162288</td>\n",
       "      <td>95943.097878</td>\n",
       "      <td>4.196264</td>\n",
       "      <td>55098.514664</td>\n",
       "      <td>6.247153</td>\n",
       "      <td>4479.724459</td>\n",
       "      <td>1.054207</td>\n",
       "      <td>21427.908933</td>\n",
       "      <td>0.829001</td>\n",
       "      <td>458.387368</td>\n",
       "      <td>0.089430</td>\n",
       "      <td>17452.889024</td>\n",
       "      <td>2.400071</td>\n",
       "      <td>2.030450e+06</td>\n",
       "      <td>21.777035</td>\n",
       "      <td>15038.807774</td>\n",
       "      <td>0.862642</td>\n",
       "      <td>9292.544859</td>\n",
       "      <td>0.417735</td>\n",
       "      <td>13482.866559</td>\n",
       "      <td>1.429747</td>\n",
       "      <td>18886.835276</td>\n",
       "      <td>1.252990</td>\n",
       "      <td>16107.723026</td>\n",
       "      <td>1.931724</td>\n",
       "      <td>13735.313121</td>\n",
       "      <td>1.013197</td>\n",
       "      <td>1116.288834</td>\n",
       "      <td>310.995328</td>\n",
       "      <td>8.752411</td>\n",
       "      <td>15.385079</td>\n",
       "      <td>1.801657</td>\n",
       "      <td>8.494693</td>\n",
       "      <td>9.761221</td>\n",
       "      <td>13.694524</td>\n",
       "      <td>8.782874</td>\n",
       "      <td>14.218517</td>\n",
       "      <td>0.033124</td>\n",
       "      <td>8.184329</td>\n",
       "      <td>14.114112</td>\n",
       "      <td>1.674319</td>\n",
       "      <td>8.018777</td>\n",
       "      <td>8.805658</td>\n",
       "      <td>12.863916</td>\n",
       "      <td>8.307467</td>\n",
       "      <td>12.869292</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>1.952759e+06</td>\n",
       "      <td>2.415818e+06</td>\n",
       "      <td>1.161704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126609</td>\n",
       "      <td>0.144850</td>\n",
       "      <td>0.112690</td>\n",
       "      <td>0.065344</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>0.050659</td>\n",
       "      <td>0.123285</td>\n",
       "      <td>0.158327</td>\n",
       "      <td>0.058470</td>\n",
       "      <td>0.155735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100925</td>\n",
       "      <td>0.126609</td>\n",
       "      <td>0.065344</td>\n",
       "      <td>0.165836</td>\n",
       "      <td>0.141986</td>\n",
       "      <td>0.184194</td>\n",
       "      <td>0.155735</td>\n",
       "      <td>0.050659</td>\n",
       "      <td>0.141986</td>\n",
       "      <td>0.165836</td>\n",
       "      <td>0.150401</td>\n",
       "      <td>0.158327</td>\n",
       "      <td>0.058470</td>\n",
       "      <td>0.186344</td>\n",
       "      <td>0.153093</td>\n",
       "      <td>0.126609</td>\n",
       "      <td>0.123285</td>\n",
       "      <td>0.136060</td>\n",
       "      <td>0.136060</td>\n",
       "      <td>0.192627</td>\n",
       "      <td>0.092211</td>\n",
       "      <td>0.129841</td>\n",
       "      <td>0.186344</td>\n",
       "      <td>0.177564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077249</td>\n",
       "      <td>0.158327</td>\n",
       "      <td>0.041380</td>\n",
       "      <td>0.155735</td>\n",
       "      <td>0.158327</td>\n",
       "      <td>0.041380</td>\n",
       "      <td>0.158327</td>\n",
       "      <td>0.029273</td>\n",
       "      <td>0.165836</td>\n",
       "      <td>0.129841</td>\n",
       "      <td>0.132989</td>\n",
       "      <td>0.116336</td>\n",
       "      <td>0.029273</td>\n",
       "      <td>0.150401</td>\n",
       "      <td>0.108917</td>\n",
       "      <td>0.141986</td>\n",
       "      <td>0.155735</td>\n",
       "      <td>0.175289</td>\n",
       "      <td>0.100925</td>\n",
       "      <td>0.112690</td>\n",
       "      <td>0.071550</td>\n",
       "      <td>0.065344</td>\n",
       "      <td>0.112690</td>\n",
       "      <td>0.295852</td>\n",
       "      <td>0.390608</td>\n",
       "      <td>0.396657</td>\n",
       "      <td>0.393327</td>\n",
       "      <td>0.364587</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.165836</td>\n",
       "      <td>0.108917</td>\n",
       "      <td>0.215632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.227060e+01</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.356610e+01</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098854e+04</td>\n",
       "      <td>0.232814</td>\n",
       "      <td>167.463435</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>6236.622985</td>\n",
       "      <td>0.380858</td>\n",
       "      <td>1.556765</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.612364e+02</td>\n",
       "      <td>0.039680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>614.922175</td>\n",
       "      <td>0.076976</td>\n",
       "      <td>46.702950</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>-96.000000</td>\n",
       "      <td>37.342857</td>\n",
       "      <td>18.714286</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>6.942857</td>\n",
       "      <td>47.842857</td>\n",
       "      <td>32.714286</td>\n",
       "      <td>18.142857</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.437931</td>\n",
       "      <td>20.827586</td>\n",
       "      <td>1.062069</td>\n",
       "      <td>11.710345</td>\n",
       "      <td>53.875862</td>\n",
       "      <td>39.896552</td>\n",
       "      <td>23.224138</td>\n",
       "      <td>8.137931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.203000e+03</td>\n",
       "      <td>2.816000e+05</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1280.995200</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>1.667962</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.674857e+03</td>\n",
       "      <td>0.229446</td>\n",
       "      <td>0.222395</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.374800</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>7.265222e+04</td>\n",
       "      <td>2.942375</td>\n",
       "      <td>161.681165</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>3.113530</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>86.511655</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>12.231725</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>8.073139e+04</td>\n",
       "      <td>6.308975</td>\n",
       "      <td>1571.220675</td>\n",
       "      <td>0.104963</td>\n",
       "      <td>24663.605500</td>\n",
       "      <td>2.244380</td>\n",
       "      <td>2173.688730</td>\n",
       "      <td>0.111733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667185</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.530141e+05</td>\n",
       "      <td>17.967026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.783825</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.444790</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>5819.409965</td>\n",
       "      <td>0.333943</td>\n",
       "      <td>1782.384727</td>\n",
       "      <td>0.106828</td>\n",
       "      <td>30.912905</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>1594.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>62.450000</td>\n",
       "      <td>43.035714</td>\n",
       "      <td>3.614286</td>\n",
       "      <td>44.185714</td>\n",
       "      <td>77.800000</td>\n",
       "      <td>72.714286</td>\n",
       "      <td>48.325000</td>\n",
       "      <td>21.964286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.656897</td>\n",
       "      <td>43.906897</td>\n",
       "      <td>3.693103</td>\n",
       "      <td>44.396552</td>\n",
       "      <td>78.091379</td>\n",
       "      <td>72.603448</td>\n",
       "      <td>48.505172</td>\n",
       "      <td>23.241379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.474100e+04</td>\n",
       "      <td>8.940800e+05</td>\n",
       "      <td>0.059214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5285.217175</td>\n",
       "      <td>0.404875</td>\n",
       "      <td>51.150850</td>\n",
       "      <td>0.004742</td>\n",
       "      <td>1.426486e+04</td>\n",
       "      <td>0.986759</td>\n",
       "      <td>28.021770</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>4.003110</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>673.634455</td>\n",
       "      <td>0.046113</td>\n",
       "      <td>2.370851e+05</td>\n",
       "      <td>14.242575</td>\n",
       "      <td>5446.453550</td>\n",
       "      <td>0.305767</td>\n",
       "      <td>75.614300</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>970.976570</td>\n",
       "      <td>0.103876</td>\n",
       "      <td>466.584710</td>\n",
       "      <td>0.082785</td>\n",
       "      <td>1.691419e+05</td>\n",
       "      <td>14.073752</td>\n",
       "      <td>5883.459725</td>\n",
       "      <td>0.419584</td>\n",
       "      <td>44496.346810</td>\n",
       "      <td>3.389045</td>\n",
       "      <td>15105.290795</td>\n",
       "      <td>0.833451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.573870</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.223950</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>4.594994e+05</td>\n",
       "      <td>27.101000</td>\n",
       "      <td>6.671850</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>545.312540</td>\n",
       "      <td>0.056812</td>\n",
       "      <td>44.923790</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>8771.036405</td>\n",
       "      <td>0.672726</td>\n",
       "      <td>5898.804980</td>\n",
       "      <td>0.368101</td>\n",
       "      <td>626.709110</td>\n",
       "      <td>0.052035</td>\n",
       "      <td>2543.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>67.785714</td>\n",
       "      <td>54.357143</td>\n",
       "      <td>4.514286</td>\n",
       "      <td>50.300000</td>\n",
       "      <td>83.857143</td>\n",
       "      <td>84.285714</td>\n",
       "      <td>54.157143</td>\n",
       "      <td>29.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>52.906897</td>\n",
       "      <td>4.624138</td>\n",
       "      <td>50.068966</td>\n",
       "      <td>83.865517</td>\n",
       "      <td>83.482759</td>\n",
       "      <td>54.175000</td>\n",
       "      <td>29.482759</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>2.715000e+05</td>\n",
       "      <td>1.675520e+06</td>\n",
       "      <td>0.168535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42714.518070</td>\n",
       "      <td>1.545373</td>\n",
       "      <td>1309.239365</td>\n",
       "      <td>0.116178</td>\n",
       "      <td>8.455480e+04</td>\n",
       "      <td>4.281213</td>\n",
       "      <td>872.789178</td>\n",
       "      <td>0.051745</td>\n",
       "      <td>205.937770</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>18435.878315</td>\n",
       "      <td>1.230902</td>\n",
       "      <td>6.642107e+05</td>\n",
       "      <td>38.816545</td>\n",
       "      <td>26366.261620</td>\n",
       "      <td>1.954134</td>\n",
       "      <td>2991.212750</td>\n",
       "      <td>0.268567</td>\n",
       "      <td>12578.883595</td>\n",
       "      <td>0.734633</td>\n",
       "      <td>18599.005048</td>\n",
       "      <td>1.372162</td>\n",
       "      <td>2.839570e+05</td>\n",
       "      <td>24.828624</td>\n",
       "      <td>17398.183245</td>\n",
       "      <td>1.271070</td>\n",
       "      <td>83097.002170</td>\n",
       "      <td>5.101042</td>\n",
       "      <td>58076.452695</td>\n",
       "      <td>5.781396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>809.740195</td>\n",
       "      <td>0.073655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.933120</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>9.055804e+05</td>\n",
       "      <td>48.213479</td>\n",
       "      <td>864.449365</td>\n",
       "      <td>0.077185</td>\n",
       "      <td>6637.156380</td>\n",
       "      <td>0.346162</td>\n",
       "      <td>1176.247155</td>\n",
       "      <td>0.105589</td>\n",
       "      <td>18380.279565</td>\n",
       "      <td>1.384496</td>\n",
       "      <td>11398.633330</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>5218.498675</td>\n",
       "      <td>0.385216</td>\n",
       "      <td>3263.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>73.900000</td>\n",
       "      <td>65.357143</td>\n",
       "      <td>5.914286</td>\n",
       "      <td>54.635714</td>\n",
       "      <td>90.542857</td>\n",
       "      <td>91.428571</td>\n",
       "      <td>59.358571</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>73.832759</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>5.944828</td>\n",
       "      <td>53.913331</td>\n",
       "      <td>90.137931</td>\n",
       "      <td>90.568966</td>\n",
       "      <td>59.328448</td>\n",
       "      <td>38.370690</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>9.750430e+05</td>\n",
       "      <td>2.739840e+06</td>\n",
       "      <td>0.506247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>143063.589970</td>\n",
       "      <td>15.049031</td>\n",
       "      <td>257660.175150</td>\n",
       "      <td>18.202699</td>\n",
       "      <td>1.426202e+06</td>\n",
       "      <td>11.445274</td>\n",
       "      <td>71774.205535</td>\n",
       "      <td>10.480920</td>\n",
       "      <td>101002.246015</td>\n",
       "      <td>11.780182</td>\n",
       "      <td>48653.354150</td>\n",
       "      <td>7.858461</td>\n",
       "      <td>2.536800e+06</td>\n",
       "      <td>75.276745</td>\n",
       "      <td>316346.879725</td>\n",
       "      <td>26.043178</td>\n",
       "      <td>156216.030270</td>\n",
       "      <td>5.186647</td>\n",
       "      <td>145049.799715</td>\n",
       "      <td>12.896578</td>\n",
       "      <td>237949.751090</td>\n",
       "      <td>11.905361</td>\n",
       "      <td>1.173072e+06</td>\n",
       "      <td>44.879905</td>\n",
       "      <td>729325.721320</td>\n",
       "      <td>24.854284</td>\n",
       "      <td>446830.918915</td>\n",
       "      <td>23.748502</td>\n",
       "      <td>221501.416890</td>\n",
       "      <td>32.333614</td>\n",
       "      <td>64571.498670</td>\n",
       "      <td>13.748892</td>\n",
       "      <td>142952.837260</td>\n",
       "      <td>8.360338</td>\n",
       "      <td>5745.797220</td>\n",
       "      <td>1.160056</td>\n",
       "      <td>143233.722145</td>\n",
       "      <td>30.934494</td>\n",
       "      <td>1.056885e+07</td>\n",
       "      <td>88.883931</td>\n",
       "      <td>114893.927295</td>\n",
       "      <td>6.769943</td>\n",
       "      <td>60650.897215</td>\n",
       "      <td>3.080650</td>\n",
       "      <td>91422.136600</td>\n",
       "      <td>13.350034</td>\n",
       "      <td>111154.577765</td>\n",
       "      <td>6.015273</td>\n",
       "      <td>97417.461010</td>\n",
       "      <td>12.865427</td>\n",
       "      <td>83055.414305</td>\n",
       "      <td>5.902165</td>\n",
       "      <td>4409.000000</td>\n",
       "      <td>1283.000000</td>\n",
       "      <td>94.742857</td>\n",
       "      <td>94.585714</td>\n",
       "      <td>13.685714</td>\n",
       "      <td>65.635714</td>\n",
       "      <td>109.142857</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>80.571429</td>\n",
       "      <td>80.714286</td>\n",
       "      <td>0.698571</td>\n",
       "      <td>88.341379</td>\n",
       "      <td>93.637931</td>\n",
       "      <td>14.589655</td>\n",
       "      <td>65.748276</td>\n",
       "      <td>103.551724</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>73.172414</td>\n",
       "      <td>80.206897</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.028373e+07</td>\n",
       "      <td>1.290496e+07</td>\n",
       "      <td>6.537556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Alfalfa & Hay_acres  Alfalfa & Hay_percentage  Almonds_acres  \\\n",
       "count          1167.000000               1167.000000    1167.000000   \n",
       "mean          23313.990675                  1.561762   22064.063504   \n",
       "std           34416.785368                  2.738412   53584.737254   \n",
       "min               0.000000                  0.000000       0.000000   \n",
       "25%            1280.995200                  0.073565       1.667962   \n",
       "50%            5285.217175                  0.404875      51.150850   \n",
       "75%           42714.518070                  1.545373    1309.239365   \n",
       "max          143063.589970                 15.049031  257660.175150   \n",
       "\n",
       "       Almonds_percentage  Barren_acres  Barren_percentage    Corn_acres  \\\n",
       "count         1167.000000  1.167000e+03        1167.000000   1167.000000   \n",
       "mean             1.371046  1.075054e+05           2.498277   2427.321938   \n",
       "std              3.175480  2.602384e+05           3.081949   6517.840324   \n",
       "min              0.000000  6.227060e+01           0.011052      0.000000   \n",
       "25%              0.000079  1.674857e+03           0.229446      0.222395   \n",
       "50%              0.004742  1.426486e+04           0.986759     28.021770   \n",
       "75%              0.116178  8.455480e+04           4.281213    872.789178   \n",
       "max             18.202699  1.426202e+06          11.445274  71774.205535   \n",
       "\n",
       "       Corn_percentage   Cotton_acres  Cotton_percentage  \\\n",
       "count      1167.000000    1167.000000        1167.000000   \n",
       "mean          0.315772    4184.554972           0.215082   \n",
       "std           1.071507   13656.074612           0.966442   \n",
       "min           0.000000       0.000000           0.000000   \n",
       "25%           0.000016       0.000000           0.000000   \n",
       "50%           0.001663       4.003110           0.000328   \n",
       "75%           0.051745     205.937770           0.016341   \n",
       "max          10.480920  101002.246015          11.780182   \n",
       "\n",
       "       Deciduous Forest_acres  Deciduous Forest_percentage  \\\n",
       "count             1167.000000                  1167.000000   \n",
       "mean              9766.566374                     0.924501   \n",
       "std              13277.021490                     1.621746   \n",
       "min                  0.000000                     0.000000   \n",
       "25%                 53.374800                     0.002096   \n",
       "50%                673.634455                     0.046113   \n",
       "75%              18435.878315                     1.230902   \n",
       "max              48653.354150                     7.858461   \n",
       "\n",
       "       Evergreen Forest_acres  Evergreen Forest_percentage   Fallow_acres  \\\n",
       "count            1.167000e+03                  1167.000000    1167.000000   \n",
       "mean             4.356794e+05                    24.284078   25891.105460   \n",
       "std              5.315691e+05                    22.986359   52064.720206   \n",
       "min              1.356610e+01                     0.001582       0.000000   \n",
       "25%              7.265222e+04                     2.942375     161.681165   \n",
       "50%              2.370851e+05                    14.242575    5446.453550   \n",
       "75%              6.642107e+05                    38.816545   26366.261620   \n",
       "max              2.536800e+06                    75.276745  316346.879725   \n",
       "\n",
       "       Fallow_percentage  Fruit Trees_acres  Fruit Trees_percentage  \\\n",
       "count        1167.000000        1167.000000             1167.000000   \n",
       "mean            1.596966        7149.825641                0.350702   \n",
       "std             2.753752       21119.032207                0.784363   \n",
       "min             0.000000           0.000000                0.000000   \n",
       "25%             0.009403           3.113530                0.000245   \n",
       "50%             0.305767          75.614300                0.006983   \n",
       "75%             1.954134        2991.212750                0.268567   \n",
       "max            26.043178      156216.030270                5.186647   \n",
       "\n",
       "       Grain Crops_acres  Grain Crops_percentage   Grapes_acres  \\\n",
       "count        1167.000000             1167.000000    1167.000000   \n",
       "mean        11451.757340                0.843314   18699.079653   \n",
       "std         22405.386738                1.791228   40273.794732   \n",
       "min             0.000000                0.000000       0.000000   \n",
       "25%            86.511655                0.005620      12.231725   \n",
       "50%           970.976570                0.103876     466.584710   \n",
       "75%         12578.883595                0.734633   18599.005048   \n",
       "max        145049.799715               12.896578  237949.751090   \n",
       "\n",
       "       Grapes_percentage  Grassland_acres  Grassland_percentage  \\\n",
       "count        1167.000000     1.167000e+03           1167.000000   \n",
       "mean            1.206556     2.357799e+05             16.593143   \n",
       "std             2.172341     2.109226e+05             12.426327   \n",
       "min             0.000000     1.098854e+04              0.232814   \n",
       "25%             0.000641     8.073139e+04              6.308975   \n",
       "50%             0.082785     1.691419e+05             14.073752   \n",
       "75%             1.372162     2.839570e+05             24.828624   \n",
       "max            11.905361     1.173072e+06             44.879905   \n",
       "\n",
       "       High Intensity Developed_acres  High Intensity Developed_percentage  \\\n",
       "count                     1167.000000                          1167.000000   \n",
       "mean                     47500.717264                             2.417373   \n",
       "std                     141674.679315                             5.162288   \n",
       "min                        167.463435                             0.007315   \n",
       "25%                       1571.220675                             0.104963   \n",
       "50%                       5883.459725                             0.419584   \n",
       "75%                      17398.183245                             1.271070   \n",
       "max                     729325.721320                            24.854284   \n",
       "\n",
       "       Low Intensity Developed_acres  Low Intensity Developed_percentage  \\\n",
       "count                    1167.000000                         1167.000000   \n",
       "mean                    78592.289681                            4.530946   \n",
       "std                     95943.097878                            4.196264   \n",
       "min                      6236.622985                            0.380858   \n",
       "25%                     24663.605500                            2.244380   \n",
       "50%                     44496.346810                            3.389045   \n",
       "75%                     83097.002170                            5.101042   \n",
       "max                    446830.918915                           23.748502   \n",
       "\n",
       "       Mixed Forest_acres  Mixed Forest_percentage  Other Ocean/Mexico_acres  \\\n",
       "count         1167.000000              1167.000000               1167.000000   \n",
       "mean         40565.408275                 3.897980                567.954905   \n",
       "std          55098.514664                 6.247153               4479.724459   \n",
       "min              1.556765                 0.000068                  0.000000   \n",
       "25%           2173.688730                 0.111733                  0.000000   \n",
       "50%          15105.290795                 0.833451                  0.000000   \n",
       "75%          58076.452695                 5.781396                  0.000000   \n",
       "max         221501.416890                32.333614              64571.498670   \n",
       "\n",
       "       Other Ocean/Mexico_percentage  Other Tree Crops_acres  \\\n",
       "count                    1167.000000             1167.000000   \n",
       "mean                        0.114946             6814.444643   \n",
       "std                         1.054207            21427.908933   \n",
       "min                         0.000000                0.000000   \n",
       "25%                         0.000000                0.667185   \n",
       "50%                         0.000000               23.573870   \n",
       "75%                         0.000000              809.740195   \n",
       "max                        13.748892           142952.837260   \n",
       "\n",
       "       Other Tree Crops_percentage  Other_acres  Other_percentage  \\\n",
       "count                  1167.000000  1167.000000       1167.000000   \n",
       "mean                      0.278145    63.322355          0.009420   \n",
       "std                       0.829001   458.387368          0.089430   \n",
       "min                       0.000000     0.000000          0.000000   \n",
       "25%                       0.000033     0.000000          0.000000   \n",
       "50%                       0.001699     0.000000          0.000000   \n",
       "75%                       0.073655     0.000000          0.000000   \n",
       "max                       8.360338  5745.797220          1.160056   \n",
       "\n",
       "          Rice_acres  Rice_percentage  Shrubland_acres  Shrubland_percentage  \\\n",
       "count    1167.000000      1167.000000     1.167000e+03           1167.000000   \n",
       "mean     4133.638904         0.570451     1.031320e+06             32.797369   \n",
       "std     17452.889024         2.400071     2.030450e+06             21.777035   \n",
       "min         0.000000         0.000000     1.612364e+02              0.039680   \n",
       "25%         0.000000         0.000000     1.530141e+05             17.967026   \n",
       "50%         2.223950         0.000151     4.594994e+05             27.101000   \n",
       "75%        56.933120         0.005551     9.055804e+05             48.213479   \n",
       "max    143233.722145        30.934494     1.056885e+07             88.883931   \n",
       "\n",
       "       Tomatoes_acres  Tomatoes_percentage  Vegs & Fruits_acres  \\\n",
       "count     1167.000000          1167.000000          1167.000000   \n",
       "mean      4341.484278             0.310663          4955.152885   \n",
       "std      15038.807774             0.862642          9292.544859   \n",
       "min          0.000000             0.000000             0.000000   \n",
       "25%          0.000000             0.000000             7.783825   \n",
       "50%          6.671850             0.000503           545.312540   \n",
       "75%        864.449365             0.077185          6637.156380   \n",
       "max     114893.927295             6.769943         60650.897215   \n",
       "\n",
       "       Vegs & Fruits_percentage  Walnuts_acres  Walnuts_percentage  \\\n",
       "count               1167.000000    1167.000000         1167.000000   \n",
       "mean                   0.267650    5332.285066            0.510640   \n",
       "std                    0.417735   13482.866559            1.429747   \n",
       "min                    0.000000       0.000000            0.000000   \n",
       "25%                    0.000848       0.444790            0.000018   \n",
       "50%                    0.056812      44.923790            0.005226   \n",
       "75%                    0.346162    1176.247155            0.105589   \n",
       "max                    3.080650   91422.136600           13.350034   \n",
       "\n",
       "         Water_acres  Water_percentage  Wetlands_acres  Wetlands_percentage  \\\n",
       "count    1167.000000       1167.000000     1167.000000          1167.000000   \n",
       "mean    15917.243315          1.138512    10807.791560             0.905401   \n",
       "std     18886.835276          1.252990    16107.723026             1.931724   \n",
       "min       614.922175          0.076976       46.702950             0.002294   \n",
       "25%      5819.409965          0.333943     1782.384727             0.106828   \n",
       "50%      8771.036405          0.672726     5898.804980             0.368101   \n",
       "75%     18380.279565          1.384496    11398.633330             0.999948   \n",
       "max    111154.577765          6.015273    97417.461010            12.865427   \n",
       "\n",
       "       Winter Wheat_acres  Winter Wheat_percentage  max_elevation  \\\n",
       "count         1167.000000              1167.000000    1167.000000   \n",
       "mean          6908.304861                 0.470367    2447.790060   \n",
       "std          13735.313121                 1.013197    1116.288834   \n",
       "min              0.000000                 0.000000      84.000000   \n",
       "25%             30.912905                 0.002333    1594.000000   \n",
       "50%            626.709110                 0.052035    2543.000000   \n",
       "75%           5218.498675                 0.385216    3263.000000   \n",
       "max          83055.414305                 5.902165    4409.000000   \n",
       "\n",
       "       min_elevation  Avg Air Temp (F)_Weekly  Avg Rel Hum (%)_Weekly  \\\n",
       "count    1167.000000              1167.000000             1167.000000   \n",
       "mean      143.879177                67.942395               54.282378   \n",
       "std       310.995328                 8.752411               15.385079   \n",
       "min       -96.000000                37.342857               18.714286   \n",
       "25%        -3.000000                62.450000               43.035714   \n",
       "50%        34.000000                67.785714               54.357143   \n",
       "75%       114.000000                73.900000               65.357143   \n",
       "max      1283.000000                94.742857               94.585714   \n",
       "\n",
       "       Avg Wind Speed (mph)_Weekly  Dew Point (F)_Weekly  \\\n",
       "count                  1167.000000           1167.000000   \n",
       "mean                      4.934138             48.737328   \n",
       "std                       1.801657              8.494693   \n",
       "min                       0.657143              6.942857   \n",
       "25%                       3.614286             44.185714   \n",
       "50%                       4.514286             50.300000   \n",
       "75%                       5.914286             54.635714   \n",
       "max                      13.685714             65.635714   \n",
       "\n",
       "       Max Air Temp (F)_Weekly  Max Rel Hum (%)_Weekly  \\\n",
       "count              1167.000000             1167.000000   \n",
       "mean                 83.564098               80.874014   \n",
       "std                   9.761221               13.694524   \n",
       "min                  47.842857               32.714286   \n",
       "25%                  77.800000               72.714286   \n",
       "50%                  83.857143               84.285714   \n",
       "75%                  90.542857               91.428571   \n",
       "max                 109.142857              100.000000   \n",
       "\n",
       "       Min Air Temp (F)_Weekly  Min Rel Hum (%)_Weekly  Precip (in)_Weekly  \\\n",
       "count              1167.000000             1167.000000         1167.000000   \n",
       "mean                 53.560680               32.194600            0.008351   \n",
       "std                   8.782874               14.218517            0.033124   \n",
       "min                  18.142857                5.428571            0.000000   \n",
       "25%                  48.325000               21.964286            0.000000   \n",
       "50%                  54.157143               29.285714            0.000000   \n",
       "75%                  59.358571               40.250000            0.001429   \n",
       "max                  80.571429               80.714286            0.698571   \n",
       "\n",
       "       Avg Air Temp (F)_month  Avg Rel Hum (%)_month  \\\n",
       "count             1167.000000            1167.000000   \n",
       "mean                67.925345              53.944701   \n",
       "std                  8.184329              14.114112   \n",
       "min                 39.437931              20.827586   \n",
       "25%                 62.656897              43.906897   \n",
       "50%                 67.700000              52.906897   \n",
       "75%                 73.832759              64.000000   \n",
       "max                 88.341379              93.637931   \n",
       "\n",
       "       Avg Wind Speed (mph)_month  Dew Point (F)_month  \\\n",
       "count                 1167.000000          1167.000000   \n",
       "mean                     4.960010            48.465399   \n",
       "std                      1.674319             8.018777   \n",
       "min                      1.062069            11.710345   \n",
       "25%                      3.693103            44.396552   \n",
       "50%                      4.624138            50.068966   \n",
       "75%                      5.944828            53.913331   \n",
       "max                     14.589655            65.748276   \n",
       "\n",
       "       Max Air Temp (F)_month  Max Rel Hum (%)_month  Min Air Temp (F)_month  \\\n",
       "count             1167.000000            1167.000000             1167.000000   \n",
       "mean                83.531597              80.477495               53.540971   \n",
       "std                  8.805658              12.863916                8.307467   \n",
       "min                 53.875862              39.896552               23.224138   \n",
       "25%                 78.091379              72.603448               48.505172   \n",
       "50%                 83.865517              83.482759               54.175000   \n",
       "75%                 90.137931              90.568966               59.328448   \n",
       "max                103.551724             100.000000               73.172414   \n",
       "\n",
       "       Min Rel Hum (%)_month  Precip (in)_month    Population  county_acres  \\\n",
       "count            1167.000000        1167.000000  1.167000e+03  1.167000e+03   \n",
       "mean               31.908716           0.008175  9.948443e+05  2.275033e+06   \n",
       "std                12.869292           0.033988  1.952759e+06  2.415818e+06   \n",
       "min                 8.137931           0.000000  3.203000e+03  2.816000e+05   \n",
       "25%                23.241379           0.000000  6.474100e+04  8.940800e+05   \n",
       "50%                29.482759           0.000690  2.715000e+05  1.675520e+06   \n",
       "75%                38.370690           0.007586  9.750430e+05  2.739840e+06   \n",
       "max                80.206897           1.050000  1.028373e+07  1.290496e+07   \n",
       "\n",
       "       pop_density  Alpine       Amador        Butte    Calaveras  \\\n",
       "count  1167.000000  1167.0  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.633285     0.0     0.016281     0.021422     0.012853   \n",
       "std       1.161704     0.0     0.126609     0.144850     0.112690   \n",
       "min       0.002872     0.0     0.000000     0.000000     0.000000   \n",
       "25%       0.059214     0.0     0.000000     0.000000     0.000000   \n",
       "50%       0.168535     0.0     0.000000     0.000000     0.000000   \n",
       "75%       0.506247     0.0     0.000000     0.000000     0.000000   \n",
       "max       6.537556     0.0     1.000000     1.000000     1.000000   \n",
       "\n",
       "            Colusa  Contra Costa    Del Norte    El Dorado       Fresno  \\\n",
       "count  1167.000000   1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.004284      0.019709     0.002571     0.015424     0.025707   \n",
       "std       0.065344      0.139057     0.050659     0.123285     0.158327   \n",
       "min       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000      0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             Glenn     Humboldt  Imperial         Inyo         Kern  \\\n",
       "count  1167.000000  1167.000000    1167.0  1167.000000  1167.000000   \n",
       "mean      0.003428     0.024850       0.0     0.010283     0.016281   \n",
       "std       0.058470     0.155735       0.0     0.100925     0.126609   \n",
       "min       0.000000     0.000000       0.0     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.0     0.000000     0.000000   \n",
       "50%       0.000000     0.000000       0.0     0.000000     0.000000   \n",
       "75%       0.000000     0.000000       0.0     0.000000     0.000000   \n",
       "max       1.000000     1.000000       0.0     1.000000     1.000000   \n",
       "\n",
       "             Kings         Lake       Lassen  Los Angeles       Madera  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.004284     0.028278     0.020566     0.035133     0.024850   \n",
       "std       0.065344     0.165836     0.141986     0.184194     0.155735   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             Marin     Mariposa    Mendocino       Merced        Modoc  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.002571     0.020566     0.028278     0.023136     0.025707   \n",
       "std       0.050659     0.141986     0.165836     0.150401     0.158327   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              Mono     Monterey         Napa       Nevada       Orange  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.003428     0.035990     0.023993     0.016281     0.015424   \n",
       "std       0.058470     0.186344     0.153093     0.126609     0.123285   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "            Placer       Plumas    Riverside   Sacramento   San Benito  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.018852     0.018852     0.038560     0.008569     0.017138   \n",
       "std       0.136060     0.136060     0.192627     0.092211     0.129841   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       San Bernardino    San Diego  San Francisco  San Joaquin  \\\n",
       "count     1167.000000  1167.000000         1167.0  1167.000000   \n",
       "mean         0.035990     0.032562            0.0     0.005998   \n",
       "std          0.186344     0.177564            0.0     0.077249   \n",
       "min          0.000000     0.000000            0.0     0.000000   \n",
       "25%          0.000000     0.000000            0.0     0.000000   \n",
       "50%          0.000000     0.000000            0.0     0.000000   \n",
       "75%          0.000000     0.000000            0.0     0.000000   \n",
       "max          1.000000     1.000000            0.0     1.000000   \n",
       "\n",
       "       San Luis Obispo    San Mateo  Santa Barbara  Santa Clara   Santa Cruz  \\\n",
       "count      1167.000000  1167.000000    1167.000000  1167.000000  1167.000000   \n",
       "mean          0.025707     0.001714       0.024850     0.025707     0.001714   \n",
       "std           0.158327     0.041380       0.155735     0.158327     0.041380   \n",
       "min           0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%           0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%           0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "75%           0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "max           1.000000     1.000000       1.000000     1.000000     1.000000   \n",
       "\n",
       "            Shasta       Sierra     Siskiyou       Solano       Sonoma  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.025707     0.000857     0.028278     0.017138     0.017995   \n",
       "std       0.158327     0.029273     0.165836     0.129841     0.132989   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "        Stanislaus       Sutter       Tehama      Trinity       Tulare  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.013710     0.000857     0.023136     0.011997     0.020566   \n",
       "std       0.116336     0.029273     0.150401     0.108917     0.141986   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          Tuolumne      Ventura         Yolo         Yuba            2  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.024850     0.031705     0.010283     0.012853     0.005141   \n",
       "std       0.155735     0.175289     0.100925     0.112690     0.071550   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                 3            4            5            6            7  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.004284     0.012853     0.096829     0.187661     0.195373   \n",
       "std       0.065344     0.112690     0.295852     0.390608     0.396657   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                 8            9           10           11           12  \\\n",
       "count  1167.000000  1167.000000  1167.000000  1167.000000  1167.000000   \n",
       "mean      0.191088     0.157669     0.095116     0.028278     0.011997   \n",
       "std       0.393327     0.364587     0.293500     0.165836     0.108917   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       fire_started  \n",
       "count   1167.000000  \n",
       "mean       0.048843  \n",
       "std        0.215632  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.loc[predicted.fire_started != y_hat_test1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the statistics on the observations that were incorrectly predicted shows that for ground cover, the most common errors were in counties with large perecentages of Grassland (16.6%) and Shrubland (32.79%). There does not seem to be any specific range of elevations for errors. Most of the errors occured when the precipitation was low and the temperature cooler. The errors also seem to occur mostly where the the population was less than 1 person per acre. Some of the top counties with erros are Monterey, Riverside, San Bernadino, San Diego and Ventura with more than 3%. Most of the error occurs in the summer months (June, July, August, and September). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second type of model we used was K Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our base knn model, we chose k as 3. Due to how knn models function, the number of nearest neighbors should always be odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a knn model using 3 nearest neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit knn model using the scaled data from the previous scaled logistic model\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target variable for both the train and test datasets.\n",
    "knn_train = knn.predict(X_train_scaled)\n",
    "knn_test = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.975706343766659 Test 0.2788339670468948\n",
      "Accuracy Score: Training 0.9751014673743366 Test 0.874641991628112\n",
      "Recall Score: Training 1.0 Test 0.3793103448275862\n"
     ]
    }
   ],
   "source": [
    "# Print the f1 score metric on both the training and test predictions to check for overfitting\n",
    "print('F1 Score: Training', f1_score(y_train, knn_train), 'Test', f1_score(y_test, knn_test))\n",
    "# Print the sccuracy score metric on both the training and test predictions to check for overfitting\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, knn_train), 'Test', accuracy_score(y_test, knn_test))\n",
    "# Print the recall score metric on both the training and test predictions to check for overfitting\n",
    "print('Recall Score: Training', recall_score(y_train, knn_train), 'Test', recall_score(y_test, knn_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that while the test accuracy is high, the recall score and the f1 score are highly overfit. We check the confusion matrix to check the values of the false positive and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Number of Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune the number (k) nearest neighbors, want to find the value of k that will return the max value for a given metric. Due to the business problem of wildfires, we want to reduce the number of false positives and thus maximise recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find the max recall score and return the score along with the k value\n",
    "def max_value(l):\n",
    "    max_val = max(l)\n",
    "    max_idx = l.index(max_val)\n",
    "    return max_idx, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.7413793103448276\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list for recall scores\n",
    "k_scores = []\n",
    "# Choose a range of k values to test\n",
    "k_range = list(range(1, 21))\n",
    "# Iterate through the different k values\n",
    "for k in k_range:\n",
    "    # Instantiate new knn model with k nearest neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    # Fit knn model on scaled training data\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    # Use model to predict target variable on testing set\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    # Find the recall score\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    # Append recall score to list of recall scorees\n",
    "    k_scores.append(recall)\n",
    "\n",
    "# Find max recall score\n",
    "idx, val = max_value(k_scores)\n",
    "# Print max recall score and it corresponding k value\n",
    "print(idx + 1, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best k value is 19 with a recall score of 74%. We rerun the model using k=19 and check the f1 and accuracy metrics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.883275080458179 Test 0.24826789838337182\n",
      "Accuracy Score: Training 0.8683655947549173 Test 0.7131526768010575\n",
      "Recall Score: Training 0.9960974086793631 Test 0.7413793103448276\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 19)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "knn_train1 = knn.predict(X_train_scaled)\n",
    "print('F1 Score: Training', f1_score(y_train, knn_train1), 'Test', f1_score(y_test, y_pred))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, knn_train1), 'Test', accuracy_score(y_test, y_pred))\n",
    "print('Recall Score: Training', recall_score(y_train, knn_train1), 'Test', recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a highly increased test scores for recall, however the accuracy is much worse and the f1 score slight worse. The model is still highly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3022, 1227],\n",
       "       [  75,  215]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of error from this model is still predicting false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8476827201973194\n"
     ]
    }
   ],
   "source": [
    "scores_knn = cross_val_score(knn, X_train_scaled, y_train, cv=10)\n",
    "print(scores_knn.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle KNN model\n",
    "with open(\"../models/best_knn.pickle\", \"wb\") as best_knn:\n",
    "    pickle.dump(knn, best_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our base model we will instantiate a Decision Tree Classifier with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 0)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 Test 0.23693379790940766\n",
      "Accuracy Score: Training 1.0 Test 0.9035029742233972\n",
      "Recall Score: Training 1.0 Test 0.23448275862068965\n"
     ]
    }
   ],
   "source": [
    "dt_train = dt.predict(X_train)\n",
    "dt_test = dt.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train), 'Test', f1_score(y_test, dt_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train), 'Test', accuracy_score(y_test, dt_test))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train), 'Test', recall_score(y_test, dt_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model has a high accuracy score, but a low f1 and recall score. The model is also highly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4033,  216],\n",
       "       [ 222,   68]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the error is from predicting false negatives, while the false positives are also high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first tuning, we want to check a wide range for the parameters of max depth, max_features and the min_sample_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth' : range(1, 15, 1), 'max_features' : range(55, 75, 1), 'min_samples_split' : range(10, 20, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the grid search, we use the f1 scoring metric so that ideally both accuracy and recall scores will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2800 candidates, totalling 28000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:   41.9s\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:   54.0s\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9768 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11218 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 14418 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 16168 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 18018 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 19968 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 22018 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done 24168 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 26418 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 28000 out of 28000 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(1, 15),\n",
       "                         'max_features': range(55, 75),\n",
       "                         'min_samples_split': range(10, 20)},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtg = DecisionTreeClassifier(random_state = 0)\n",
    "grid_model = GridSearchCV(dtg, parameters, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "\n",
    "grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9242561036170699\n",
      "{'max_depth': 14, 'max_features': 62, 'min_samples_split': 17}\n",
      "DecisionTreeClassifier(max_depth=14, max_features=62, min_samples_split=17,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(grid_model.best_score_)\n",
    "print(grid_model.best_params_)\n",
    "print(grid_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 Test 0.23693379790940766\n",
      "Accuracy Score: Training 1.0 Test 0.9035029742233972\n",
      "Recall Score: Training 1.0 Test 0.23448275862068965\n"
     ]
    }
   ],
   "source": [
    "dt_train2 = dt.predict(X_train)\n",
    "dt_test2 = dt.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train2), 'Test', f1_score(y_test, dt_test2))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train2), 'Test', accuracy_score(y_test, dt_test2))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train2), 'Test', recall_score(y_test, dt_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still highly overfit. The test recall and f1 score are still much worse than the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4033,  216],\n",
       "       [ 222,   68]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model the majority of the error is still in predicting false negatives, hence the low recall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous model, the results showed that it was highly overfit and could not generalize well. By reducing the `max_depth`, it should reduce the overfitting. In the previous model the `min_samples_split` was at a high end of the range, so for model we increase the range. The `max_features` will test a lower range of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters2 = {'max_depth' : range(10, 13, 1), 'max_features' : range(35, 65, 1), 'min_samples_split' : range(14, 30, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1440 candidates, totalling 14400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4968 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6018 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7168 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 8418 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9768 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 11218 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 12768 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 14400 out of 14400 | elapsed:  6.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(10, 13),\n",
       "                         'max_features': range(35, 65),\n",
       "                         'min_samples_split': range(14, 30)},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtg2 = DecisionTreeClassifier(random_state = 0)\n",
    "dtg2_model = GridSearchCV(dtg2, parameters2, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "\n",
    "dtg2_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9151798193645881\n",
      "{'max_depth': 12, 'max_features': 52, 'min_samples_split': 15}\n",
      "DecisionTreeClassifier(max_depth=12, max_features=52, min_samples_split=15,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(dtg2_model.best_score_)\n",
    "print(dtg2_model.best_params_)\n",
    "print(dtg2_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.9134525605113841 Test 0.26426426426426425\n",
      "Accuracy Score: Training 0.9059475491726506 Test 0.7840934126459572\n",
      "Recall Score: Training 0.9926631283172026 Test 0.6068965517241379\n"
     ]
    }
   ],
   "source": [
    "dt_train3 = dtg2_model.predict(X_train)\n",
    "dt_test3 = dtg2_model.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train3), 'Test', f1_score(y_test, dt_test3))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train3), 'Test', accuracy_score(y_test, dt_test3))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train3), 'Test', recall_score(y_test, dt_test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has reduced overfitting. The accuracy test score has decreased, while the Recall test score has increased significantly. The F1 test score is still low meaning most of our error is predicting false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next model we will reduce the `max_depth` again to reduce any overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters3 = {'max_depth' : range(8, 9, 1), 'max_features' : range(35, 65, 1), 'min_samples_split' : range(14, 30, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 480 candidates, totalling 4800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2418 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 3168 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4800 out of 4800 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(8, 9),\n",
       "                         'max_features': range(35, 65),\n",
       "                         'min_samples_split': range(14, 30)},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtg3 = DecisionTreeClassifier(random_state = 0)\n",
    "dtg3_model = GridSearchCV(dtg3, parameters3, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "\n",
    "dtg3_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8665665288129925\n",
      "{'max_depth': 8, 'max_features': 42, 'min_samples_split': 20}\n",
      "DecisionTreeClassifier(max_depth=8, max_features=42, min_samples_split=20,\n",
      "                       random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(dtg3_model.best_score_)\n",
    "print(dtg3_model.best_params_)\n",
    "print(dtg3_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.8702843468468469 Test 0.2608173076923077\n",
      "Accuracy Score: Training 0.8561504839213238 Test 0.7290152015862524\n",
      "Recall Score: Training 0.9651108335935061 Test 0.7482758620689656\n"
     ]
    }
   ],
   "source": [
    "dt_train4 = dtg3_model.predict(X_train)\n",
    "dt_test4 = dtg3_model.predict(X_test)\n",
    "\n",
    "print('F1 Score: Training', f1_score(y_train, dt_train4), 'Test', f1_score(y_test, dt_test4))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, dt_train4), 'Test', accuracy_score(y_test, dt_test4))\n",
    "print('Recall Score: Training', recall_score(y_train, dt_train4), 'Test', recall_score(y_test, dt_test4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall test score has increased again, but is now above the accuracy score. The recall score is greater than it was without the population and population density features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Decision Tree Model\n",
    "with open(\"../models/best_decision_tree.pickle\", \"wb\") as best_decision_tree:\n",
    "    pickle.dump(dtg3_model.best_estimator_, best_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model we instantiate a Random Forest Classifier with a set `random_state` for reproducibility and kept everything else at their default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state = 0)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train = rfc.predict(X_train)\n",
    "rfc_test = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 1.0 0.28384279475982527\n",
      "Accuracy Score: Training 1.0 0.9277373870896674\n",
      "Recall Score: Training 1.0 0.22413793103448276\n"
     ]
    }
   ],
   "source": [
    "print('F1 Score: Training', f1_score(y_train, rfc_train), f1_score(y_test, rfc_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, rfc_train), accuracy_score(y_test, rfc_test))\n",
    "print('Recall Score: Training', recall_score(y_train, rfc_train), recall_score(y_test, rfc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous models, the base model was able to have perfect training scores, however from the f1 score and recall, the model is highly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4146,  103],\n",
       "       [ 225,   65]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, rfc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of error from this model occurs in the predictions of false negatives. We want to maximize the recall score, while not boosting the number of false positives too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gridSearch to reduce the overfitting of the model and increase the test scores. The chosen parameters to be tuned are `n_estimators`, `criterion`, `min_samples_split`, and `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = { \n",
    "    'n_estimators': [75, 100, 125],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': list(range(100, 102)),\n",
    "    'max_features': list(range(30, 40))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_grid = GridSearchCV(RandomForestClassifier(random_state = 0), param_grid1, cv=5, scoring='f1', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed: 12.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': [30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "                                          39],\n",
       "                         'min_samples_split': [100, 101],\n",
       "                         'n_estimators': [75, 100, 125]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9428839896424058\n",
      "{'criterion': 'entropy', 'max_features': 38, 'min_samples_split': 100, 'n_estimators': 125}\n",
      "RandomForestClassifier(criterion='entropy', max_features=38,\n",
      "                       min_samples_split=100, n_estimators=125, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(rfc_grid.best_score_)\n",
    "print(rfc_grid.best_params_)\n",
    "print(rfc_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the best paramters, the number of estimators is 125 and we can further tune this parameter by also checking 100 and 150. The max features is 38, which is at the high end of the range tested. The `min_samples_split` is at the low end of the range checked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.9734873191881211 0.3333333333333333\n",
      "Accuracy Score: Training 0.973033093974399 0.8889623265036352\n",
      "Recall Score: Training 0.990165469871995 0.43448275862068964\n"
     ]
    }
   ],
   "source": [
    "rfc_grid_train = rfc_grid.best_estimator_.predict(X_train)\n",
    "rfc_grid_test = rfc_grid.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, rfc_grid_train), f1_score(y_test, rfc_grid_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, rfc_grid_train), accuracy_score(y_test, rfc_grid_test))\n",
    "print('Recall Score: Training', recall_score(y_train, rfc_grid_train), recall_score(y_test, rfc_grid_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall test score has increased by more than 20%, while the accuracy test score has decreased by 4% and the F1 score has increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increase the `min_samples_split` to reduce the overfitting and check a new set of `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = { \n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': list(range(200, 202)),\n",
    "    'max_features': list(range(30, 40))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_grid2 = GridSearchCV(RandomForestClassifier(random_state = 0), param_grid2, cv=5, scoring='f1', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed: 14.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': [30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "                                          39],\n",
       "                         'min_samples_split': [200, 201],\n",
       "                         'n_estimators': [100, 125, 150]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_grid2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9049666579930428\n",
      "{'criterion': 'gini', 'max_features': 37, 'min_samples_split': 200, 'n_estimators': 100}\n",
      "RandomForestClassifier(max_features=37, min_samples_split=200, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(rfc_grid2.best_score_)\n",
    "print(rfc_grid2.best_params_)\n",
    "print(rfc_grid2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.9366017307765621 0.317937701396348\n",
      "Accuracy Score: Training 0.9350999063378083 0.8601013439083498\n",
      "Recall Score: Training 0.9587886356540744 0.5103448275862069\n"
     ]
    }
   ],
   "source": [
    "rfc_grid2_train = rfc_grid2.best_estimator_.predict(X_train)\n",
    "rfc_grid2_test = rfc_grid2.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, rfc_grid2_train), f1_score(y_test, rfc_grid2_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, rfc_grid2_train), accuracy_score(y_test, rfc_grid2_test))\n",
    "print('Recall Score: Training', recall_score(y_train, rfc_grid2_train), recall_score(y_test, rfc_grid2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall test score has increased by 8%, whle the accuracy and f1 test score has decreased by 2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep increasing the `min_samples_split` until the reduction in overfitting becomes negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid3 = { \n",
    "    'n_estimators': [75, 100, 125],\n",
    "    'criterion': ['gini'],\n",
    "    'min_samples_split': list(range(1400, 1402)),\n",
    "    'max_features': list(range(30, 40))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_grid3 = GridSearchCV(RandomForestClassifier(random_state = 0), param_grid3, cv=5, scoring='f1', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini'],\n",
       "                         'max_features': [30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "                                          39],\n",
       "                         'min_samples_split': [1400, 1401],\n",
       "                         'n_estimators': [75, 100, 125]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_grid3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7938577671824036\n",
      "{'criterion': 'gini', 'max_features': 36, 'min_samples_split': 1400, 'n_estimators': 125}\n",
      "RandomForestClassifier(max_features=36, min_samples_split=1400,\n",
      "                       n_estimators=125, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(rfc_grid3.best_score_)\n",
    "print(rfc_grid3.best_params_)\n",
    "print(rfc_grid3.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.8067085953878408 0.2879788639365918\n",
      "Accuracy Score: Training 0.8020995941305027 0.762502753910553\n",
      "Recall Score: Training 0.8259444270995941 0.7517241379310344\n"
     ]
    }
   ],
   "source": [
    "rfc_grid3_train = rfc_grid3.best_estimator_.predict(X_train)\n",
    "rfc_grid3_test = rfc_grid3.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, rfc_grid3_train), f1_score(y_test, rfc_grid3_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, rfc_grid3_train), accuracy_score(y_test, rfc_grid3_test))\n",
    "print('Recall Score: Training', recall_score(y_train, rfc_grid3_train), recall_score(y_test, rfc_grid3_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall test score increases to 75% and the accuracy decreases to 76%. The majority of error is from predicting false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check once more the `criterion` to see if the scores increase with the same `min_samples_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid4 = { \n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': list(range(1400, 1402)),\n",
    "    'max_features': list(range(30, 40))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_grid4 = GridSearchCV(RandomForestClassifier(random_state = 0), param_grid4, cv=5, scoring='f1', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': [30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "                                          39],\n",
       "                         'min_samples_split': [1400, 1401],\n",
       "                         'n_estimators': [100, 125, 150]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_grid4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7938577671824036\n",
      "{'criterion': 'gini', 'max_features': 36, 'min_samples_split': 1400, 'n_estimators': 125}\n",
      "RandomForestClassifier(max_features=36, min_samples_split=1400,\n",
      "                       n_estimators=125, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(rfc_grid4.best_score_)\n",
    "print(rfc_grid4.best_params_)\n",
    "print(rfc_grid4.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: Training 0.8067085953878408 0.2879788639365918\n",
      "Accuracy Score: Training 0.8020995941305027 0.762502753910553\n",
      "Recall Score: Training 0.8259444270995941 0.7517241379310344\n"
     ]
    }
   ],
   "source": [
    "rfc_grid4_train = rfc_grid4.best_estimator_.predict(X_train)\n",
    "rfc_grid4_test = rfc_grid4.best_estimator_.predict(X_test)\n",
    "print('F1 Score: Training', f1_score(y_train, rfc_grid4_train), f1_score(y_test, rfc_grid4_test))\n",
    "print('Accuracy Score: Training', accuracy_score(y_train, rfc_grid4_train), accuracy_score(y_test, rfc_grid4_test))\n",
    "print('Recall Score: Training', recall_score(y_train, rfc_grid4_train), recall_score(y_test, rfc_grid4_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores and best parameters remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Random Forest model\n",
    "with open(\"../models/best_random_forest.pickle\", \"wb\") as best_random_forest:\n",
    "    pickle.dump(rfc_grid4.best_estimator_, best_random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model, we instantiate a AdaBoost Classifier with a specific `random_state` for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(random_state=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_clf = AdaBoostClassifier(random_state = 0)\n",
    "adaboost_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8065875741492351\n",
      "Recall: 0.8630970964720575\n",
      "Test:\n",
      "Accuracy: 0.7495042961004627\n",
      "Recall: 0.7724137931034483\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, adaboost_train_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_train, adaboost_train_preds)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, adaboost_test_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_test, adaboost_test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model, has a high recall score at 77%, while accuracy is slighly lower at 75%. This means that the majority of errors are false positives as seen in previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Adaboost model\n",
    "with open(\"../models/best_adaboost.pickle\", \"wb\") as best_adaboost:\n",
    "    pickle.dump(adaboost_clf, best_adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters - GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use GridSearch to tune the parameters to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_adaboost1 = {\n",
    "    'n_estimators' : [50, 75, 100],\n",
    "    'learning_rate' : [0.5, .75, 1.0, 1.25, 1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=AdaBoostClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'n_estimators': [50, 75, 100]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_clf1 = AdaBoostClassifier(random_state = 0)\n",
    "adaboost_model1 = GridSearchCV(adaboost_clf1, para_adaboost1, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "adaboost_model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8243705571989788\n",
      "{'learning_rate': 1.25, 'n_estimators': 100}\n",
      "AdaBoostClassifier(learning_rate=1.25, n_estimators=100, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(adaboost_model1.best_score_)\n",
    "print(adaboost_model1.best_params_)\n",
    "print(adaboost_model1.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_train_preds1 = adaboost_model1.best_estimator_.predict(X_train)\n",
    "adaboost_test_preds1 = adaboost_model1.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8250468310958476\n",
      "Recall: 0.8713705900718077\n",
      "Test:\n",
      "Accuracy: 0.7596386869354483\n",
      "Recall: 0.7413793103448276\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, adaboost_train_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_train, adaboost_train_preds1)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, adaboost_test_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_test, adaboost_test_preds1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default values for `n_estimators` is 50. The increase to 100 has decreased the recall score with a slight increase to our accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3233, 1016],\n",
       "       [  75,  215]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, adaboost_test_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the majority of the error is due to the false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_adaboost2 = {\n",
    "    'n_estimators' : [100, 125, 150],\n",
    "    'learning_rate' : [0.5, .75, 1.0, 1.25, 1.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=AdaBoostClassifier(random_state=0), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'n_estimators': [100, 125, 150]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_clf2 = AdaBoostClassifier(random_state = 0)\n",
    "adaboost_model2 = GridSearchCV(adaboost_clf2, para_adaboost2, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "adaboost_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8354248138093929\n",
      "{'learning_rate': 1.25, 'n_estimators': 150}\n",
      "AdaBoostClassifier(learning_rate=1.25, n_estimators=150, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(adaboost_model2.best_score_)\n",
    "print(adaboost_model2.best_params_)\n",
    "print(adaboost_model2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_train_preds2 = adaboost_model2.best_estimator_.predict(X_train)\n",
    "adaboost_test_preds2 = adaboost_model2.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8407742741180143\n",
      "Recall: 0.8879175772713082\n",
      "Test:\n",
      "Accuracy: 0.7717558933685834\n",
      "Recall: 0.7034482758620689\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, adaboost_train_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_train, adaboost_train_preds2)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, adaboost_test_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_test, adaboost_test_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3299,  950],\n",
       "       [  86,  204]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, adaboost_test_preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model, we instantiate a GradientBoosting Classifier with a specific `random_state` for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_clf = GradientBoostingClassifier(random_state = 0)\n",
    "gbt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_preds = gbt_clf.predict(X_train)\n",
    "gbt_test_preds = gbt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8520917889478614\n",
      "Recall: 0.9171870121760849\n",
      "F1: 0.8611314670965851\n",
      "Test:\n",
      "Accuracy: 0.7706543291473893\n",
      "Recall: 0.7586206896551724\n",
      "F1: 0.2970965563808238\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, gbt_train_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_train, gbt_train_preds)))\n",
    "print('F1: {}'.format(f1_score(y_train, gbt_train_preds)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, gbt_test_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_test, gbt_test_preds)))\n",
    "print('F1: {}'.format(f1_score(y_test, gbt_test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3278,  971],\n",
       "       [  70,  220]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, gbt_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base score is similar to the tuned models from other types of models. In Gradient Boosting the accuracy is slightly higher than the recall score. There is some overfitting in the model and the majority of the error is in the prediction of false positives as seen in all previous models. The f1 score however, is slightly higher than the previous models.\n",
    "\n",
    "To increase the metric scores, GridSearchCV will be used to tune the parameters of gradient boosting and to validate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle GradientBoosting model\n",
    "with open(\"../models/best_gradientboosting.pickle\", \"wb\") as best_gradientboosting:\n",
    "    pickle.dump(gbt_clf, best_gradientboosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three parameters that will be tuned are `n_estimators`, `learning_rate` and `max_depth`. The default values for these are 100, 0.1, and 3 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_gbt1 = {\n",
    "    'n_estimators' : [75, 100, 125],\n",
    "    'learning_rate' : [0.1, 0.5, .75, 1.0, 1.25, 1.5],\n",
    "    'max_depth' : [2, 3, 4, 5]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 72 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 23.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GradientBoostingClassifier(random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'max_depth': [2, 3, 4, 5],\n",
       "                         'n_estimators': [75, 100, 125]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_clf1 = GradientBoostingClassifier(random_state = 0)\n",
    "gbt_model1 = GridSearchCV(gbt_clf1, para_gbt1, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "gbt_model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9723413212601215\n",
      "{'learning_rate': 1.0, 'max_depth': 5, 'n_estimators': 125}\n",
      "GradientBoostingClassifier(learning_rate=1.0, max_depth=5, n_estimators=125,\n",
      "                           random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(gbt_model1.best_score_)\n",
    "print(gbt_model1.best_params_)\n",
    "print(gbt_model1.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best parameters, the `max_depth` and `n_estimators` are both at the highest range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_preds1 = gbt_model1.best_estimator_.predict(X_train)\n",
    "gbt_test_preds1 = gbt_model1.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.9999609740867936\n",
      "Recall: 1.0\n",
      "F1: 0.999960975609756\n",
      "Test:\n",
      "Accuracy: 0.9004185944040538\n",
      "Recall: 0.28620689655172415\n",
      "F1: 0.2686084142394822\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, gbt_train_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_train, gbt_train_preds1)))\n",
    "print('F1: {}'.format(f1_score(y_train, gbt_train_preds1)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, gbt_test_preds1)))\n",
    "print('Recall: {}'.format(recall_score(y_test, gbt_test_preds1)))\n",
    "print('F1: {}'.format(f1_score(y_test, gbt_test_preds1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4004,  245],\n",
       "       [ 207,   83]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, gbt_test_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training score is much higher than default model, however the except for accuracy, the other metric scores are much lower. The model is highly overfit to the training data. The most likely culprit for this is the `max_depth` parameter. The higher the number of splits, the better the model will do on the training data, but it will also increase the overfitting of the model. The best parameter for the `max_depth` will usually be the max number in the range. So for the next tuning, we will decrease the possible `max_depth`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_gbt2 = {\n",
    "    'n_estimators' : [200, 250, 275],\n",
    "    'learning_rate' : [0.1, 0.5, .75, 1.0, 1.25, 1.5],\n",
    "    'max_depth' : [3]    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 12.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=GradientBoostingClassifier(random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 0.75, 1.0, 1.25, 1.5],\n",
       "                         'max_depth': [3], 'n_estimators': [200, 250, 275]},\n",
       "             scoring='recall', verbose=1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_clf2 = GradientBoostingClassifier(random_state = 0)\n",
    "gbt_model2 = GridSearchCV(gbt_clf2, para_gbt2, cv = 10, scoring = 'recall', verbose = 1, n_jobs = -1)\n",
    "gbt_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "{'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 275}\n",
      "GradientBoostingClassifier(learning_rate=0.5, n_estimators=275, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(gbt_model2.best_score_)\n",
    "print(gbt_model2.best_params_)\n",
    "print(gbt_model2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_train_preds2 = gbt_model2.best_estimator_.predict(X_train)\n",
    "gbt_test_preds2 = gbt_model2.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.9860287230721199\n",
      "Recall: 1.0\n",
      "F1: 0.9862212300823647\n",
      "Test:\n",
      "Accuracy: 0.8885217008151576\n",
      "Recall: 0.39655172413793105\n",
      "F1: 0.31250000000000006\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, gbt_train_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_train, gbt_train_preds2)))\n",
    "print('F1: {}'.format(f1_score(y_train, gbt_train_preds2)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, gbt_test_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_test, gbt_test_preds2)))\n",
    "print('F1: {}'.format(f1_score(y_test, gbt_test_preds2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the base model, we instantiate a XGBoost Classifier with a specific `random_state` for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier()"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(random_state = 0)\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_preds = xgb_clf.predict(X_train)\n",
    "xgb_test_preds = xgb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8383546674992195\n",
      "Recall: 0.9129722135497971\n",
      "F1: 0.8495787332945962\n",
      "Test:\n",
      "Accuracy: 0.751487111698612\n",
      "Recall: 0.7689655172413793\n",
      "F1: 0.2833545108005083\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, xgb_train_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_train, xgb_train_preds)))\n",
    "print('F1: {}'.format(f1_score(y_train, xgb_train_preds)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, xgb_test_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_test, xgb_test_preds)))\n",
    "print('F1: {}'.format(f1_score(y_test, xgb_test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model has a recall score of 76.8% and a accuracy score slightly lower at 75.1%. This along with the low F1 score means that the majority of the error is still in the predictions of false positives. We us GridSearchCV to increase the metric scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle XGBoosting model\n",
    "with open(\"../models/best_xgb.pickle\", \"wb\") as best_xgb:\n",
    "    pickle.dump(xgb_clf, best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Parameters - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_xgb = {\n",
    "    'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
    "    'max_depth': [6],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=XGBClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
       "                         'max_depth': [6], 'min_child_weight': [1, 2],\n",
       "                         'n_estimators': [100], 'subsample': [0.5, 0.7]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf2 = XGBClassifier(random_state = 0)\n",
    "xgb_model = GridSearchCV(xgb_clf2, para_xgb, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9790650069481475\n",
      "{'learning_rate': 1, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.7}\n",
      "XGBClassifier(learning_rate=1, max_depth=6, subsample=0.7)\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model.best_score_)\n",
    "print(xgb_model.best_params_)\n",
    "print(xgb_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_preds = xgb_model.predict(X_train)\n",
    "xgb_test_preds = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "Test:\n",
      "Accuracy: 0.9165014320334876\n",
      "Recall: 0.2655172413793103\n",
      "F1: 0.28893058161350843\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, xgb_train_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_train, xgb_train_preds)))\n",
    "print('F1: {}'.format(f1_score(y_train, xgb_train_preds)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, xgb_test_preds)))\n",
    "print('Recall: {}'.format(recall_score(y_test, xgb_test_preds)))\n",
    "print('F1: {}'.format(f1_score(y_test, xgb_test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4083,  166],\n",
       "       [ 213,   77]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, xgb_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is highly overfit. The training metrics show that it can predict the training observations perfectly, however the recall and f1 score drop a high percentage. the greatest error is due to false negatives; something that we want to minimize due to the problem being wildfires. The overfitting is likely from the `max_depth`. The default value is 3, and 6 was tested. The `max_depth` will be reduced to minimize overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_xgb2 = {\n",
    "    'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
    "    'max_depth': [4],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=XGBClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
       "                         'max_depth': [4], 'min_child_weight': [1, 2],\n",
       "                         'n_estimators': [100], 'subsample': [0.5, 0.7]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf3 = XGBClassifier(random_state = 0)\n",
    "xgb_model2 = GridSearchCV(xgb_clf3, para_xgb2, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "xgb_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9698452523780718\n",
      "{'learning_rate': 1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.7}\n",
      "XGBClassifier(learning_rate=1, max_depth=4, subsample=0.7)\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model2.best_score_)\n",
    "print(xgb_model2.best_params_)\n",
    "print(xgb_model2.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_preds2 = xgb_model2.predict(X_train)\n",
    "xgb_test_preds2 = xgb_model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.9940290352794255\n",
      "Recall: 1.0\n",
      "F1: 0.9940644760833302\n",
      "Test:\n",
      "Accuracy: 0.9026217228464419\n",
      "Recall: 0.31724137931034485\n",
      "F1: 0.29392971246006394\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, xgb_train_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_train, xgb_train_preds2)))\n",
    "print('F1: {}'.format(f1_score(y_train, xgb_train_preds2)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, xgb_test_preds2)))\n",
    "print('Recall: {}'.format(recall_score(y_test, xgb_test_preds2)))\n",
    "print('F1: {}'.format(f1_score(y_test, xgb_test_preds2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4005,  244],\n",
       "       [ 198,   92]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, xgb_test_preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of true positives have increased, true negatives have slightly decreased, false negatives have decreased, and false positives have increased. The majority of error has switched, and the false positive predictions have increased to more than the false negativse. The overfitting has also slightly decreased. In the next model, we will test the best model when the `max_depth` is 2, which is less than the default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_xgb3 = {\n",
    "    'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
    "    'max_depth': [2],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=XGBClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
       "                         'max_depth': [2], 'min_child_weight': [1, 2],\n",
       "                         'n_estimators': [100], 'subsample': [0.5, 0.7]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf4 = XGBClassifier(random_state = 0)\n",
    "xgb_model3 = GridSearchCV(xgb_clf4, para_xgb3, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "xgb_model3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075789383119526\n",
      "{'learning_rate': 1, 'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.7}\n",
      "XGBClassifier(learning_rate=1, max_depth=2, subsample=0.7)\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model3.best_score_)\n",
    "print(xgb_model3.best_params_)\n",
    "print(xgb_model3.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_preds3 = xgb_model3.predict(X_train)\n",
    "xgb_test_preds3 = xgb_model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.9123868248517015\n",
      "Recall: 0.9758039338120512\n",
      "F1: 0.9176116554735954\n",
      "Test:\n",
      "Accuracy: 0.8138356466181978\n",
      "Recall: 0.6206896551724138\n",
      "F1: 0.2987551867219917\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, xgb_train_preds3)))\n",
    "print('Recall: {}'.format(recall_score(y_train, xgb_train_preds3)))\n",
    "print('F1: {}'.format(f1_score(y_train, xgb_train_preds3)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, xgb_test_preds3)))\n",
    "print('Recall: {}'.format(recall_score(y_test, xgb_test_preds3)))\n",
    "print('F1: {}'.format(f1_score(y_test, xgb_test_preds3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3514,  735],\n",
       "       [ 110,  180]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, xgb_test_preds3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the number of false negatives have decreased, the number of false positives have increased a large number. The overfitting of the model has also decreased a large percentage in the recall and accuracy metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Models - GridSearchCV 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next model, we reduce the `n_estimators` to decrease the overfitting in recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_xgb4 = {\n",
    "    'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
    "    'max_depth': [2],\n",
    "    'min_child_weight': [2, 3],\n",
    "    'subsample': [0.5, 0.7],\n",
    "    'n_estimators': [75]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=XGBClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 1, 1.5, 2],\n",
       "                         'max_depth': [2], 'min_child_weight': [2, 3],\n",
       "                         'n_estimators': [75], 'subsample': [0.5, 0.7]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf5 = XGBClassifier(random_state = 0)\n",
    "xgb_model4 = GridSearchCV(xgb_clf5, para_xgb4, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "xgb_model4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8887028968474644\n",
      "{'learning_rate': 1, 'max_depth': 2, 'min_child_weight': 3, 'n_estimators': 75, 'subsample': 0.7}\n",
      "XGBClassifier(learning_rate=1, max_depth=2, min_child_weight=3, n_estimators=75,\n",
      "              subsample=0.7)\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model4.best_score_)\n",
    "print(xgb_model4.best_params_)\n",
    "print(xgb_model4.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_preds4 = xgb_model4.predict(X_train)\n",
    "xgb_test_preds4 = xgb_model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.8952544489541056\n",
      "Recall: 0.9566812363409304\n",
      "F1: 0.9013162732553865\n",
      "Test:\n",
      "Accuracy: 0.8048028200044063\n",
      "Recall: 0.6275862068965518\n",
      "F1: 0.2912\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, xgb_train_preds4)))\n",
    "print('Recall: {}'.format(recall_score(y_train, xgb_train_preds4)))\n",
    "print('F1: {}'.format(f1_score(y_train, xgb_train_preds4)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, xgb_test_preds4)))\n",
    "print('Recall: {}'.format(recall_score(y_test, xgb_test_preds4)))\n",
    "print('F1: {}'.format(f1_score(y_test, xgb_test_preds4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters - GridSearchCV 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_xgb5 = {\n",
    "    'learning_rate': [0.1, 0.5],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'subsample': [0.5],\n",
    "    'n_estimators': [75]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=XGBClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5], 'max_depth': [2, 3],\n",
       "                         'min_child_weight': [1, 2], 'n_estimators': [75],\n",
       "                         'subsample': [0.5]},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf6 = XGBClassifier(random_state = 0)\n",
    "xgb_model5 = GridSearchCV(xgb_clf6, para_xgb5, cv = 10, scoring = 'f1', verbose = 1, n_jobs = -1)\n",
    "xgb_model5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9172640789290369\n",
      "{'learning_rate': 0.5, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 75, 'subsample': 0.5}\n",
      "XGBClassifier(learning_rate=0.5, n_estimators=75, subsample=0.5)\n"
     ]
    }
   ],
   "source": [
    "print(xgb_model5.best_score_)\n",
    "print(xgb_model5.best_params_)\n",
    "print(xgb_model5.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_preds5 = xgb_model5.predict(X_train)\n",
    "xgb_test_preds5 = xgb_model5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Accuracy: 0.9203871370590072\n",
      "Recall: 0.9814236653137683\n",
      "F1: 0.9249668971605121\n",
      "Test:\n",
      "Accuracy: 0.822207534699273\n",
      "Recall: 0.6103448275862069\n",
      "F1: 0.3049095607235142\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, xgb_train_preds5)))\n",
    "print('Recall: {}'.format(recall_score(y_train, xgb_train_preds5)))\n",
    "print('F1: {}'.format(f1_score(y_train, xgb_train_preds5)))\n",
    "print('Test:')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, xgb_test_preds5)))\n",
    "print('Recall: {}'.format(recall_score(y_test, xgb_test_preds5)))\n",
    "print('F1: {}'.format(f1_score(y_test, xgb_test_preds5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import Recall, Precision, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25624, 143)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insantiate the StandardScaler()\n",
    "ss2 = StandardScaler()\n",
    "# Fit the feature training data\n",
    "ss2.fit(X_train2)\n",
    "\n",
    "# Transform both the training and test features\n",
    "X_train2_scaled = ss2.transform(X_train2)\n",
    "X_test2_scaled = ss2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "601/601 [==============================] - 1s 2ms/step - loss: 0.4290 - acc: 0.7971 - precision_9: 0.7655 - recall_9: 0.8547 - auc_9: 0.8737 - val_loss: 75438.5859 - val_acc: 0.5064 - val_precision_9: 0.5056 - val_recall_9: 1.0000 - val_auc_9: 0.5016\n",
      "Epoch 2/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3570 - acc: 0.8411 - precision_9: 0.7995 - recall_9: 0.9091 - auc_9: 0.9078 - val_loss: 68517.0000 - val_acc: 0.4655 - val_precision_9: 0.4737 - val_recall_9: 0.5281 - val_auc_9: 0.4649\n",
      "Epoch 3/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3253 - acc: 0.8582 - precision_9: 0.8144 - recall_9: 0.9266 - auc_9: 0.9215 - val_loss: 110326.0156 - val_acc: 0.4736 - val_precision_9: 0.4438 - val_recall_9: 0.1685 - val_auc_9: 0.4766\n",
      "Epoch 4/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2989 - acc: 0.8763 - precision_9: 0.8294 - recall_9: 0.9463 - auc_9: 0.9317 - val_loss: 157156.1562 - val_acc: 0.4769 - val_precision_9: 0.4352 - val_recall_9: 0.1215 - val_auc_9: 0.4804\n",
      "Epoch 5/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2774 - acc: 0.8868 - precision_9: 0.8418 - recall_9: 0.9517 - auc_9: 0.9403 - val_loss: 223004.2500 - val_acc: 0.4716 - val_precision_9: 0.4080 - val_recall_9: 0.1036 - val_auc_9: 0.4752\n",
      "Epoch 6/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2611 - acc: 0.8961 - precision_9: 0.8523 - recall_9: 0.9576 - auc_9: 0.9463 - val_loss: 390223.8750 - val_acc: 0.4877 - val_precision_9: 0.1129 - val_recall_9: 0.0022 - val_auc_9: 0.4924\n",
      "Epoch 7/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2436 - acc: 0.9059 - precision_9: 0.8628 - recall_9: 0.9646 - auc_9: 0.9530 - val_loss: 516575.5938 - val_acc: 0.4952 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00 - val_auc_9: 0.5000\n",
      "Epoch 8/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2300 - acc: 0.9109 - precision_9: 0.8688 - recall_9: 0.9672 - auc_9: 0.9578 - val_loss: 529663.5000 - val_acc: 0.4952 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00 - val_auc_9: 0.5000\n",
      "Epoch 9/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2147 - acc: 0.9189 - precision_9: 0.8777 - recall_9: 0.9728 - auc_9: 0.9629 - val_loss: 447675.0312 - val_acc: 0.4877 - val_precision_9: 0.1129 - val_recall_9: 0.0022 - val_auc_9: 0.4924\n",
      "Epoch 10/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2036 - acc: 0.9233 - precision_9: 0.8850 - recall_9: 0.9725 - auc_9: 0.9666 - val_loss: 538263.3125 - val_acc: 0.4877 - val_precision_9: 0.1129 - val_recall_9: 0.0022 - val_auc_9: 0.4924\n",
      "Epoch 11/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.1927 - acc: 0.9274 - precision_9: 0.8895 - recall_9: 0.9755 - auc_9: 0.9698 - val_loss: 801430.5000 - val_acc: 0.4952 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00 - val_auc_9: 0.5000\n",
      "Epoch 12/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.1799 - acc: 0.9324 - precision_9: 0.8971 - recall_9: 0.9762 - auc_9: 0.9739 - val_loss: 851829.8750 - val_acc: 0.4952 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00 - val_auc_9: 0.5000\n",
      "Epoch 13/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.1729 - acc: 0.9382 - precision_9: 0.9045 - recall_9: 0.9794 - auc_9: 0.9754 - val_loss: 827813.1250 - val_acc: 0.4952 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00 - val_auc_9: 0.5000\n",
      "Epoch 14/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.1651 - acc: 0.9406 - precision_9: 0.9090 - recall_9: 0.9789 - auc_9: 0.9775 - val_loss: 892838.9375 - val_acc: 0.4952 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00 - val_auc_9: 0.5000\n",
      "Epoch 15/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.1585 - acc: 0.9433 - precision_9: 0.9122 - recall_9: 0.9806 - auc_9: 0.9793 - val_loss: 1033505.4375 - val_acc: 0.4952 - val_precision_9: 0.0000e+00 - val_recall_9: 0.0000e+00 - val_auc_9: 0.5000\n",
      "Test loss: 0.47883307933807373\n",
      "Test accuracy: 0.8618638515472412\n",
      "Test precision: 0.22240526974201202\n",
      "Test recall: 0.4655172526836395\n",
      "Test AUC: 0.7832196354866028\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=143))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', Precision(), Recall(), AUC()])\n",
    "\n",
    "history_1 = model.fit(X_train2_scaled,\n",
    "                    y_train2,\n",
    "                    epochs=15,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# print metrics\n",
    "score = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test precision:', score[2])\n",
    "print('Test recall:', score[3])\n",
    "print('Test AUC:', score[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is highly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = history_1.history\n",
    "loss_values = hist['loss']\n",
    "val_loss_values = hist['val_loss']\n",
    "acc_values = hist['acc'] \n",
    "val_acc_values = hist['val_acc']\n",
    "\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(epochs, loss_values, 'g.', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(epochs, acc_values, 'r.', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "601/601 [==============================] - 1s 2ms/step - loss: 0.4622 - acc: 0.7808 - precision_11: 0.7523 - recall_11: 0.8352 - auc_11: 0.8534 - val_loss: 91101.6406 - val_acc: 0.5048 - val_precision_11: 0.5048 - val_recall_11: 1.0000 - val_auc_11: 0.5000\n",
      "Epoch 2/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.4008 - acc: 0.8166 - precision_11: 0.7805 - recall_11: 0.8792 - auc_11: 0.8869 - val_loss: 61577.9219 - val_acc: 0.5258 - val_precision_11: 0.5159 - val_recall_11: 0.9814 - val_auc_11: 0.5213\n",
      "Epoch 3/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3729 - acc: 0.8315 - precision_11: 0.7890 - recall_11: 0.9035 - auc_11: 0.8988 - val_loss: 42470.1602 - val_acc: 0.5223 - val_precision_11: 0.5190 - val_recall_11: 0.7344 - val_auc_11: 0.5203\n",
      "Epoch 4/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3516 - acc: 0.8443 - precision_11: 0.7973 - recall_11: 0.9221 - auc_11: 0.9081 - val_loss: 49892.8516 - val_acc: 0.5172 - val_precision_11: 0.5120 - val_recall_11: 0.9307 - val_auc_11: 0.5131\n",
      "Epoch 5/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3347 - acc: 0.8524 - precision_11: 0.8046 - recall_11: 0.9295 - auc_11: 0.9156 - val_loss: 53492.0977 - val_acc: 0.4849 - val_precision_11: 0.4856 - val_recall_11: 0.3442 - val_auc_11: 0.4862\n",
      "Epoch 6/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3220 - acc: 0.8603 - precision_11: 0.8103 - recall_11: 0.9397 - auc_11: 0.9194 - val_loss: 51161.9609 - val_acc: 0.5150 - val_precision_11: 0.5145 - val_recall_11: 0.6970 - val_auc_11: 0.5132\n",
      "Epoch 7/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3104 - acc: 0.8671 - precision_11: 0.8180 - recall_11: 0.9432 - auc_11: 0.9249 - val_loss: 70019.0391 - val_acc: 0.4908 - val_precision_11: 0.4958 - val_recall_11: 0.5053 - val_auc_11: 0.4906\n",
      "Epoch 8/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.3020 - acc: 0.8718 - precision_11: 0.8223 - recall_11: 0.9476 - auc_11: 0.9282 - val_loss: 76690.5391 - val_acc: 0.4836 - val_precision_11: 0.4857 - val_recall_11: 0.3878 - val_auc_11: 0.4845\n",
      "Epoch 9/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2936 - acc: 0.8766 - precision_11: 0.8270 - recall_11: 0.9515 - auc_11: 0.9315 - val_loss: 74864.6562 - val_acc: 0.5151 - val_precision_11: 0.5263 - val_recall_11: 0.3961 - val_auc_11: 0.5163\n",
      "Epoch 10/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2855 - acc: 0.8830 - precision_11: 0.8332 - recall_11: 0.9569 - auc_11: 0.9346 - val_loss: 69143.6953 - val_acc: 0.5126 - val_precision_11: 0.5228 - val_recall_11: 0.3964 - val_auc_11: 0.5138\n",
      "Epoch 11/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2785 - acc: 0.8860 - precision_11: 0.8366 - recall_11: 0.9584 - auc_11: 0.9387 - val_loss: 154201.9688 - val_acc: 0.4867 - val_precision_11: 0.4703 - val_recall_11: 0.1320 - val_auc_11: 0.4902\n",
      "Epoch 12/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2696 - acc: 0.8911 - precision_11: 0.8431 - recall_11: 0.9602 - auc_11: 0.9414 - val_loss: 129938.2656 - val_acc: 0.4978 - val_precision_11: 0.5085 - val_recall_11: 0.1580 - val_auc_11: 0.5011\n",
      "Epoch 13/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2641 - acc: 0.8930 - precision_11: 0.8452 - recall_11: 0.9614 - auc_11: 0.9440 - val_loss: 256654.9688 - val_acc: 0.4941 - val_precision_11: 0.4957 - val_recall_11: 0.1262 - val_auc_11: 0.4977\n",
      "Epoch 14/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2577 - acc: 0.8973 - precision_11: 0.8501 - recall_11: 0.9639 - auc_11: 0.9468 - val_loss: 204969.8750 - val_acc: 0.4978 - val_precision_11: 0.5203 - val_recall_11: 0.0674 - val_auc_11: 0.5020\n",
      "Epoch 15/15\n",
      "601/601 [==============================] - 1s 1ms/step - loss: 0.2527 - acc: 0.8982 - precision_11: 0.8521 - recall_11: 0.9628 - auc_11: 0.9486 - val_loss: 225533.5781 - val_acc: 0.5070 - val_precision_11: 0.6152 - val_recall_11: 0.0628 - val_auc_11: 0.5114\n",
      "Test loss: 0.4732692241668701\n",
      "Test accuracy: 0.7986340522766113\n",
      "Test precision: 0.18924303352832794\n",
      "Test recall: 0.6551724076271057\n",
      "Test AUC: 0.8086211681365967\n"
     ]
    }
   ],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(Dense(64, activation='relu', input_dim=143))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', Precision(), Recall(), AUC()])\n",
    "\n",
    "history_2 = model2.fit(X_train2_scaled,\n",
    "                       y_train2,\n",
    "                       epochs=15,\n",
    "                       validation_data=(X_val, y_val))\n",
    "\n",
    "# print metrics\n",
    "score = model2.evaluate(X_test2_scaled, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test precision:', score[2])\n",
    "print('Test recall:', score[3])\n",
    "print('Test AUC:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist2 = history_2.history\n",
    "loss_values = hist2['loss']\n",
    "val_loss_values = hist2['val_loss']\n",
    "acc_values = hist2['acc'] \n",
    "val_acc_values = hist2['val_acc']\n",
    "\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(epochs, loss_values, 'g.', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(epochs, acc_values, 'r.', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "151/151 [==============================] - 1s 4ms/step - loss: 0.5915 - acc: 0.6809 - precision_31: 0.6746 - recall_31: 0.6949 - auc_31: 0.7450 - val_loss: 77009.0391 - val_acc: 0.5165 - val_precision_31: 0.5108 - val_recall_31: 1.0000 - val_auc_31: 0.5118\n",
      "Epoch 2/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.4785 - acc: 0.7777 - precision_31: 0.7397 - recall_31: 0.8549 - auc_31: 0.8447 - val_loss: 84074.9766 - val_acc: 0.5165 - val_precision_31: 0.5108 - val_recall_31: 1.0000 - val_auc_31: 0.5118\n",
      "Epoch 3/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.4535 - acc: 0.7909 - precision_31: 0.7463 - recall_31: 0.8794 - auc_31: 0.8574 - val_loss: 63703.4922 - val_acc: 0.5165 - val_precision_31: 0.5108 - val_recall_31: 1.0000 - val_auc_31: 0.5118\n",
      "Epoch 4/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.4347 - acc: 0.7998 - precision_31: 0.7530 - recall_31: 0.8904 - auc_31: 0.8682 - val_loss: 65150.3555 - val_acc: 0.5206 - val_precision_31: 0.5131 - val_recall_31: 0.9895 - val_auc_31: 0.5160\n",
      "Epoch 5/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.4189 - acc: 0.8091 - precision_31: 0.7610 - recall_31: 0.8995 - auc_31: 0.8769 - val_loss: 36847.3086 - val_acc: 0.5206 - val_precision_31: 0.5131 - val_recall_31: 0.9895 - val_auc_31: 0.5160\n",
      "Epoch 6/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.4047 - acc: 0.8146 - precision_31: 0.7656 - recall_31: 0.9052 - auc_31: 0.8829 - val_loss: 36741.0742 - val_acc: 0.5272 - val_precision_31: 0.5169 - val_recall_31: 0.9678 - val_auc_31: 0.5229\n",
      "Epoch 7/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3923 - acc: 0.8192 - precision_31: 0.7709 - recall_31: 0.9068 - auc_31: 0.8883 - val_loss: 51708.9297 - val_acc: 0.5251 - val_precision_31: 0.5155 - val_recall_31: 0.9895 - val_auc_31: 0.5206\n",
      "Epoch 8/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3798 - acc: 0.8289 - precision_31: 0.7744 - recall_31: 0.9267 - auc_31: 0.8914 - val_loss: 40171.3438 - val_acc: 0.5239 - val_precision_31: 0.5148 - val_recall_31: 0.9895 - val_auc_31: 0.5193\n",
      "Epoch 9/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3722 - acc: 0.8346 - precision_31: 0.7788 - recall_31: 0.9333 - auc_31: 0.8960 - val_loss: 28983.0566 - val_acc: 0.5476 - val_precision_31: 0.5295 - val_recall_31: 0.9320 - val_auc_31: 0.5439\n",
      "Epoch 10/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3592 - acc: 0.8395 - precision_31: 0.7843 - recall_31: 0.9352 - auc_31: 0.9006 - val_loss: 21341.5898 - val_acc: 0.5510 - val_precision_31: 0.5390 - val_recall_31: 0.7644 - val_auc_31: 0.5490\n",
      "Epoch 11/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3537 - acc: 0.8444 - precision_31: 0.7851 - recall_31: 0.9471 - auc_31: 0.9025 - val_loss: 14589.7725 - val_acc: 0.5534 - val_precision_31: 0.5535 - val_recall_31: 0.5962 - val_auc_31: 0.5532\n",
      "Epoch 12/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3445 - acc: 0.8486 - precision_31: 0.7909 - recall_31: 0.9465 - auc_31: 0.9072 - val_loss: 14391.7588 - val_acc: 0.5498 - val_precision_31: 0.5536 - val_recall_31: 0.5591 - val_auc_31: 0.5497\n",
      "Epoch 13/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3394 - acc: 0.8518 - precision_31: 0.7943 - recall_31: 0.9481 - auc_31: 0.9086 - val_loss: 23247.0918 - val_acc: 0.5259 - val_precision_31: 0.5376 - val_recall_31: 0.4360 - val_auc_31: 0.5268\n",
      "Epoch 14/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3338 - acc: 0.8546 - precision_31: 0.7958 - recall_31: 0.9526 - auc_31: 0.9110 - val_loss: 40144.6602 - val_acc: 0.4825 - val_precision_31: 0.4711 - val_recall_31: 0.2038 - val_auc_31: 0.4852\n",
      "Epoch 15/15\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.3286 - acc: 0.8590 - precision_31: 0.8007 - recall_31: 0.9549 - auc_31: 0.9135 - val_loss: 63717.3906 - val_acc: 0.4817 - val_precision_31: 0.3223 - val_recall_31: 0.0241 - val_auc_31: 0.4862\n",
      "Test loss: 0.4914436340332031\n",
      "Test accuracy: 0.728354275226593\n",
      "Test precision: 0.16772374510765076\n",
      "Test recall: 0.8206896781921387\n",
      "Test AUC: 0.8385076522827148\n"
     ]
    }
   ],
   "source": [
    "model3 = models.Sequential()\n",
    "\n",
    "model3.add(Dense(64, activation='relu', input_dim=143))\n",
    "model3.add(Dropout(0.4))\n",
    "\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "\n",
    "model3.add(Dense(32, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', Precision(), Recall(), AUC()])\n",
    "\n",
    "history_3 = model3.fit(X_train2_scaled,\n",
    "                       y_train2,\n",
    "                       epochs=15,\n",
    "                       batch_size = 128,\n",
    "                       validation_data=(X_val, y_val))\n",
    "\n",
    "# print metrics\n",
    "score = model3.evaluate(X_test2_scaled, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test precision:', score[2])\n",
    "print('Test recall:', score[3])\n",
    "print('Test AUC:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist3 = history_3.history\n",
    "loss_values = hist3['loss']\n",
    "val_loss_values = hist3['val_loss']\n",
    "acc_values = hist3['acc'] \n",
    "val_acc_values = hist3['val_acc']\n",
    "\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(epochs, loss_values, 'g.', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(epochs, acc_values, 'r.', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
